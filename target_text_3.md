在这里，我想和大家探讨一个改进的思路。在我们的第一步中，比如说，我们参考彭建怡他们的工作，做了数据分析，找到了最佳提示词，并立即将其写入了脚本。是的，这个脚本是批量化的，意味着它可以自动地生成相应的统计结果，比如统计出过百个项。这个过程就是像处理一个Excel文件一样，大家都理解这个逻辑吗？这个逻辑是非常关键的。许多人在实践中犯错误，偏要一次性完成所有的工作，却往往达不到理想效果。也许他们可以编写批量脚本，但最后生成的效果并不好。 

举一个例子，比如静怡需要处理大量的数据，可能有三百个Excel文件需要处理。传统的做法是检查这三百个文件中的错误，这无疑是非常浪费时间的。但是，如果你现在调用我之前强调的代码提示器，是不是问题就得到了解决？所以说，这一套逻辑是非常重要的。这次讲座，我希望你们能理解API和大型的代码提示器的重要性。

现阶段，我们的欧洲AI团队正在承受巨大压力，因为广大用户对他们的工作效率并不满意。从这周开始，他们会将大量的资源，包括人力、实践经验，都投入到提高社区的服务能力中。所以，我们可能会有更多的能力，例如我们可以直接使用命令提示行，API调用，甚至是相关插件。 

目前，我们已经解决了最优提示词的问题，编写了批量化脚本，以提高我们的工作效率。接下来的第三步是什么呢？ 当我们发现我们的提示词非常出色，而且我们的批量化脚本也写得很好，接下来，我们应该与这个活跃的团队合作，将其转化为一个小产品。现在，我们已经购买了一百到三百个域名，这些域名的特点都是以432开头。我们有一个域名是4384.com，准备为大家提供一百到三百个可用的域名。因此，最好的提示词解决了某个领域的具体问题，最好的脚本以及方便使用的应用程序或者桌面程序，可以被封装起来，最终形成一个非常完美的产品。

所以说，这其实是一种最佳工作流程，大家可以再思考一下，在这三种不同的常见情况中，如何应用这个工作流程。
本次讲座主要解答了以下问题：知识工作者最核心的问题在于怎样解决不同工作场景下的具体问题。我们根据具体工作场景和团队规模，可以将它们划分为以下三种类型：

第一类是针对小团队，尤其是以个体工作为主的场景。例如，如果有一个项目需要由一个主导者与十人左右的团队完成，我们需要解决的问题就是如何有效地将工作任务和方式传达给每一个团队成员。这种情况下，分享工作流程，或者提供最优的操作提示词是一个很合理的做法。但是在更大的团队，例如几万人的大组织里，这种方式就不太合适了。这一类场景的技术和产品能力要求相对较低，用户只需要理解基本操作步骤，如复制粘贴或者查看账单等。因此对非技术人员，如客服等，更适合提供这样类型的界面。

第二类则是针对技术能力较高的团队或个体。这一类场景下，主要通过量化脚本来提升工作效率，因此我们需要向研发团队这样的技术人员提供更高级的操作界面。

第三类则是针对的不仅是内部团队，还有可能包括外部的收费用户。在这类场景下，我们可能需要向外部用户提供某些服务并进行小额付费。

因此，总结起来，这三个类型的场景是我们在AI时代最应该注意的不同工作模式。

接下来我将以一个具体的前端界面为例，给大家演示一些实际操作技巧，这些技巧会帮助大家更好地理解和适应上述不同的工作场景。在这一部分，我注意到有些同学可能对某些专业术语不太熟悉，我会尽量给出详细解释。

让我们以LO平台为例，第一步，我们需要登录进去。登录之后，在界面左上角，我们可以做一些个性化的设置。第一个就是语言设置，我们可以将其改为简体中文，适应大部分用户的习惯。第二个设置是自动生成标题。我们还可以选择去掉预览气泡，这样可以使界面更为简洁。最后一个重要的功能是云端数据同步，通过开启这个功能，我们可以轻松实现多设备之间的数据同步。以上这些都是LO平台的一些基本操作，相信通过这些设置，大家对如何适应不同工作场景有了更清晰的理解。
这是一项大数据应用，利用云端保存我们的一键使用记录。我在这里最推荐的是一个由外国开发者开发的服务。我需要特意提醒大家，这一切并非由杨总团队开发，所有这些产品都是国外开发者贡献的，与我们团队并无任何关系。我们只是使用者而已，你们需要明确这一点，确保不会产生任何误会。

这个产品功能强大，下面我会详细讲解如何进行配置。首先打开指定的网站，你会看到一篇测试文章，这是losh car云端数据库的一个产品，它在大模型时代中是一项主流服务。看到网址后，请务必记录下来，它是接下来配置过程的关键。

这个数据库提供了三个主要服务：云端支持、losh core数据库以及其他尚未明确的功能。

接下来是注册步骤，点击指定位置进行注册，然后登录系统。登录完毕后，你会看到一个新界面，我在这里已经为大家创建了一个演示用的数据库。

创建完数据库后，最为关键的是两个密钥，这是配置数据库连接的必须条件。你需要将这两个密钥填入指定位置，然后点击检查可用性并确定。

最后，我要重申一点，这个云端信息同步的能力，是这项服务最值得推荐的一部分。我们可以实时将使用的数据同步到云端数据库，这样无论何时何地我们都可以方便地获取自己需要的信息。

我们现在就来测试一下，首先我会创建一条新的记录，然后在数据库中搜索该记录，以此来演示云端数据库的实时信息同步功能。
在这次讲座中，我想让大家了解到如何在某个特定的软件中进行数据同步。首先，你需要找到并点击这个同步按钮。当你完成这个操作之后，你会发现时间已经变成了十一点二十五分二十七秒，这表示数据同步成功。

接着，我将在下文中介绍两种可以查看同步数据的方法。第一种方式是进行在线查看，你只需要点一下数据浏览器就可以了。第二种方式则需要用到命令提示行，这一般是程序开发者使用的方法。如果你并不擅长编程，那么你可以将数据保存到本地。这个过程其实非常简单，你只需要找到一个本地数据库的客户端，就可以将这些数据保存在自己的电脑上。我之前常用的是一个叫NAV的客户端，你也可以去找一下。当你找到一个合适的客户端后，将网址输入其中，运行程序，程序便会自动将这些数据同步到你的电脑本地。

在这里，我只是简单的介绍了一些基础功能，还有一些更高级的功能这次就不多提了。接下来，我想告诉大家云端的一种配置方法。这种同步数据的方式你们应该都很熟悉了，这个内容在"肉腿的一个课程"中我曾反复讲解过。以前的所学内容小节现在就能发挥作用了。一般在国内，大家常用的同步工具是坚果云，只需要在坚果云上进行简单的配置就可以了。关于这个内容我讲解过很多次，本次我就不再展开了。

每种配置格式都有其自己的优缺点。比如数据库模式的优点在于它可以使用数据库查询语言，使得查询变得更为方便。而如坚果云这样的微部署或VDIV配置的格式，其优点在于它可以将数据直接生成为一份交程文件，这对于非编程者来说，则更像是一份程序文件，但如果你没有编程技能，查询数据可能会变得复杂。大家理解我所说的意思就好。

我刚刚介绍了数据同步，这是非常核心的一个功能。下面我想介绍另一个同样重要的功能，那就是代理。当你在某个特定的场景中需要用到这个功能时，它的作用可以发挥得相当显著。
设想你在一个无法使用网络代理（翻墙）的环境中，同时，你的团队成员们也无法翻墙；你需要向他们介绍某个功能。此时，我们需要搭建一个代理服务器，大家通过代理服务器来使用这个功能。你只需要启用代理（代理服务器），并根据具体的情况，填入相应的选项即可。但是，假设你是在像杨老师这样的团队环境中，大家都会科学上网，这时代理服务器的功能就显得多余了；它通常只在为学生提供服务的时候使用，所以一般情况下，用得并不多。

接下来，我们谈谈文件相关的功能。数据的导入导出功能是非常重要的，该功能反映了保护用户隐私的需求。由于我们所处理的项目往往不能联网，所以假如你在一台电脑上使用浏览器存储了一些数据，但是如果你更换电脑，之前的数据就会丢失。但如果你使用了数据导出功能，将数据储存下来，那么在新电脑上只需要再导入数据即可。然而，有些功能一般需要被禁用，比如自定义提示词功能。

自定义提示词列表是一个非常关键的功能，对于用户来说，非常重要。这个功能我将稍后详细讲述。其次，我们要讨论访问密码。比如LOA PM密钥和访问密码，这是两种不同的访问方式。一般来说，我们更多地使用API密钥，它的使用限制较少，并无像其他方法那样每三小时只能使用三十条之类的限制，但费用稍高。例如，我在上个月就花费了约500多元人民币，然而，在八月份我却花费了600多元人民币。不过，其成本会逐渐降低，比如我在九月份其实使用的更多，但花费的并不多。

接下来，我们讨论自定义模型名。这部分内容是与前面讲述过的产品密切相关的，比如Local AI等产品。假设我们本地有一些高隐私的信息或文档，这一部分就填入到模型名里。而我建议大家选择使用GPT-4模型。我们在给出提示时，一定要选择最优的模型，以及最佳的提示词语。千万不要因小失大，选择太便宜的模型，你可能只是节省了一点成本，但可能导致你的工作效率大打折扣。
就像你请一个小学生帮忙干活，虽然不用花钱，但他完成活动后你可能需要花费大量时间来修正他的工作。因此，我主张使用最优质的模型来完成任务。大体上讲，我们通常会选择GPT-4作为首选模型。

在之前的完整的GPT讲解课程中，我已经对所有相关的参数进行了详细解析，今天我不再重复。但是，我还是要强调一下，这其中有一个被称为"话题新鲜度"的参数对你的模型选择极为重要。以前在完整的GPT课程中我曾提到过这一点。

例如，如果你主要工作内容是处理严肃的、例如法律文本之类的内容，那么你可能需要把话题新鲜度这个指标稍微调低一些，但是如果你的工作内容是写小说、笑话或者诗歌这样更具创新性的文本，那么你则需要将这个指标调高。我在三月份的课程中已经详细解释过这些。

我想再提醒大家一个常被忽略的功能，就是"历史摘要"。如果你主要工作内容是翻译，那么我建议你最好不要使用这个功能，因为在进行翻译工作时，历史摘要的效果并不理想。

下面我想讲的是另外两个核心概念，它们分别是"面具"和"提示词"。让我们先来讲解一下"提示词"，这个更容易理解。

提示词分为内置的和自定义的两种类型，未来我们可能会将GPT-4中的数千个提示词全部内置于系统之中。

假如我们要添加一个新的提示词，我将给你展示一个如何添加的指导示例。

首先，你需要给这个提示词起一个方便自己记忆的名字，比如我们可以叫它"中文写作大师"。之后，你需要为其定一个最优化的提示词模型。

对于每一个提示词，我们都有约定的输入和输出格式，这是一个很容易被忽视，但却极为关键的部分。举例来讲，我给大家做了一个示范，这是我经常用的两种提示词。

然后，我将对上述内容进行演示和展示。回到主界面，我将打开某个功能，进行现场演示。
实时笔录是一项强大的功能，在实际应用中，我们可以把它视作一个翔实的记录工具。当我们在与其他人进行会议时，例如采用腾讯会议等工具，我们可以打开实时笔录的功能，这意味着，所有的讨论和发言都将被精确地记录下来。

具体来说，当我们需要导出记录时，我们可以选择将它导出为纯文本。假设我们找到一个特定的会议记录，实时笔录功能就可以帮助我们将这次会议的所有讨论详细地记录下来。如果需要，我们甚至可以选择其中的一段，进一步进行整理。

此外，我们还可以通过实时笔录的功能来了解讨论内容的核心要点，这样，无论是进行进一步的分析，还是进行总结，我们都能轻松地找到我们需要的信息。

对于中文写作来说，之前我们需要抄写和粘贴的步骤，现在都可以通过书面语小助手来实现。这可能似乎是一个小细节，但实际上却非常重要。同时，由于现在我们无需进行重复的复制和粘贴操作，我们也可以节省大量的时间。

作为讲师，我注意到有些学生对这些操作可能比较不熟悉，我最初认为，这些都是助教能教给大家的东西，但后来我发现这并不尽然。这并不是说助教没有尽全力，相反，他们教了很久，可能是因为这些知识点有点复杂，所以并没有完全教会大家。

因此，我决定来给大家做示范，给大家详细地说明这个过程。该过程相当于一次学习过程，我们最终将经历讨论、理解、实践和合并各个步骤的完整路径。

我尽量将提示信息优化到一种状态，那就是无法再进行优化。这时，我们可以感觉到这个工具的效果非常好，能够帮助我们更好地完成各种文本工作。

例如，尽管括号看起来可能微不足道，但实际上，它们在中文写作中非常重要，因为它们有助于我们清晰地组织结构，进行有些步骤。

文本工作可能包括各种各样的任务，比如说，校对翻译，整理口语稿，或者改写成更有创意的稿件。对于改写稿件这一步，我们可以对参数进行调整，要求它在开头使用设问的句子，在结尾处用金句进行小结。这是一种有效的写作技巧，可以帮助我们生产出更有创造力的文本。

将这些提示词联合使用，最终会形成一个工作流，就像是一副面具，能够增强我们的效率和效果。这个工作流将使我们能够更好地处理文本工作，例如前面提到的任务，都可以尝试约束在大约10个提示值以内。

我们所构建的模型，默认使用的资源主要是GPT-3.5的，因为在大多数情况下，这种模型既能提供良好的效果，又能较好地节省资源。

总的来说，无论是进行翻译工作，还是整理口头语，在实现过程中我们不可避免地会遇到许多难题。但是，通过良好的策略和工具，我们可以找到优化问题的途径，让这些工作变得更加简单而有效。
让我现在为大家示范一个具体操作。这里我们会涉及到一个被称作"自定义模型面具"的概念。在这个自定义模型面具的设置界面里，我们可以看到一个“提示词”选项。提示词就是我们稍后将会讲解的那一部分。首先，来看看提示词包含了些什么内容。我们可以看到，提示词时代表角色的名称。

然后，我们会看到一个选项，问我们要用什么模型。再之后，我们会看到一个关于话题新鲜度的设置。这个话题新鲜度，我们当前设置的是一般般。

假设我们这时候要创建一个新的自定义模型面具，要它表现得更好一些，我们该如何操作呢？我们可以按照以下步骤进行。首先，找到并点击“克隆预审”的按钮。完成克隆后，这个新的面具就会出现在我们的面具列表中。然后我们为这个新的面具取一个新名字，就像小红书写手2023版这样。

接下来我们来调整这个模型的新鲜度设置。我们将它调大一点，但是22.0似乎太高了，我们调整为1.0应该就足够了。然后，还有一个选项是是否需要记下这个操作的摘要，但这个随你决定。完成这些修改之后，我们可以继续优化这个提示词。

通常，我们会设置三个系统用户，大部分时间使用第二个系统用户。接下来，我们点击确认按钮，完成这一步骤。

接下来，我将展示我们该如何使用这个新建的模型面具。这个功能是非常重要的，因为一个模型面具能支持多个提示词。我们单击模型面具选项，展开新的聊天窗口。你可以选择清除聊天窗口，或者重新打开一个新的聊天窗口，但我发现后者的效果通常更好。然后，在所有面具的选择列表中找到并选择刚才创建的小红书写手的模型面具。

用这个新的面具模型，如果我们提出一个请求，比如，“请推荐一本打动你的书？”然后看到生成的答案，比方说是，“推荐阅读杨志平的书《新知》”。这本书其实是很不错的。事实上，这个功能可以帮助我们在闲暇时间获得有意义的时间度过方式，它还可以帮助我们的工作。

实际上，我们对这个提示词还可以继续进行优化。比如说，我们可以在请求中附带一本书的图书简介，这样生成的结果会更准确。这个小技巧在实践中是非常有用的。当然为了快速演示，我在这里没有附带图书简介。

当我们完成所有的这些步骤后，你应该已经对这个"自定义模型面具"有了一些基本的理解。并且，你能知道，每一次生成的结果其实都可以进行进一步的优化。这就是我们今天关于模型面具的讲解，希望对大家有所启发。
作为计算机专家和科学作家，我想向您全面地介绍两种不同的人工智能框架，以及它们如何在不同的课程中应用。首先，这两种框架包括了"阿卡"框架和"科"框架。理解这两个框架，并了解它们如何操作，将有助于我们更好地巧妙运用提示词。

在我们的完整的GPT（生成预训练转换器）课程中，使用的核心框架是"阿卡"框架。同样，我们的AI辩论课程中也使用了另一种框架，这就是“科”框架。这两种框架对应的课程即将上线，欢迎感兴趣的同学拭目以待。

大部分优秀的提示词的设计和应用，都可以在这两个框架中找到。除此之外，我们还会积极收集并整理大家在实践中发现的优秀提示词。例如，微博上一位名为"宝约XP"的微软工程师，他不时会分享一些非常好的提示词。而在GPT help社区，也可以找到许多出色的提示词。这样，我们就能更全面地掌握和应用各种高效工具和策略。

当我们完成学习材料的整理和编写后，下一步就是要分享这些信息。此时，我们可以选择将信息导出为图片、JSON(JavaScript 对象表示法)或TXT(文本)格式。通常，选用TXT格式更为便捷。在选择导出格式后，我们通常会取消默认选项，然后选择"全部"选项，即导出全部信息。然后可以查看预览，并下载文件。

一旦文件下载完成，我们就可以将其分享给他人。此外，还可以将这些文件分享到"Sai"社区，不过这个功能在过去可能存在一些问题，我们期望它能尽快得到改善。

这两种提示词框架和分享功能，可以帮助我们更高效地利用GPT技术。在了解了这几点之后，你将有一个全新的理解关于这些工具如何运作，以及如何将它们应用到实际的场景中去。希望这个介绍能帮助大家进一步深入理解人工智能和大数据语言模型，从而使我们能更好地使用这些工具，取得更多的进步和成就。
让我们继续进行下去。刚才我们的讨论停在了第二点，现在让我们回过头来再仔细看看：它是关于提示词工程和四川"恰"的知识。我们有时候在各类学习分享的平台上，能够看到别人写的一些非常好的提示词，这些词汇我们也可以参照收藏。比如说，可以找一些我们认为他们写的比较好、有用的提示词收藏过来，进行推广和应用。另外，我们也可以在网络上一些资源丰富的网站进行学习和查阅。在这里我特别想提醒大家，虽然这个网站并不是我们开发的，可能有些人根本不知道是谁开发的，但是它依然有价值。只是需要注意，不要因为对网站的开发者不清，而对合法使用产生顾虑。全面理解并适当应用，以提升我们的学习效果。我的介绍到这里暂告一段落。

在这个网站上，不管是谁开发的，我们都可以找到大量的提示词。比如说，我可以选择“咨询师”这个词汇，并且把它复制下来使用。这种复制粘贴的过程实际上就是一种衔接关系的构建。在未来，会有更多的提示词直接内置在里面，至于是如何内置的，逻辑非常清晰，大家应该都能明白。

接下来我想说的第二个关键点是关于批量化脚本的。但是因为这个脚本中有我的个人API，出于安全考虑，我不能直接给大家看到。这里我只能给大家演示一下它的运行流程。

这份脚本原本是我们团队一起讨论和使用的，目前在其中仍然存有我的个人密钥。我只能告诉你们它是这样运行的：我们先完成对输入输出的预设，然后再运行这个脚本。我发现似乎屏幕共享中有些信息没有完全显示出来，让我重新共享一下。

现在，大家可以看到我的屏幕了，我想给大家示意的是，比如说我们需要对整本书或者其他长文档进行检查，我们就可以通过这种方式进行。我们预先准备好需要检查的文档，然后再调用相应的脚本。既然其中存在我的个人密钥，我在此处就不方便深入展示，但是大家应该能够理解整个过程到底是如何运行的。接下来，我们就可以正式调用这个脚本进行操作了。
在我们对现有脚本进行调研后，你们可能会发现一个问题，那就是不管你如何处理，可能都没能深入到问题的核心。现在这个问题有所改观，我们可以进行下一步的探讨。这部分我们无需纠结，大家应当明白这是什么问题。它其实界定了API的功能，这些API会直接产生一些结果，这些结果其实就是更定制化的输出。

可能是网络状态突然变差，导致大家在利用这些API时可能会遇到一些问题。如果大家对API如何调用，如何编写存在疑问，我强烈推荐你们参加我们的AI编程课程。广告时间结束，我们再回到我们的讨论。

感觉现在应该会有内容可以呈现出来，尽管不知道为何一瞬间好像产生了什么问题。现在的网络环境显然对我们的讨论产生了干扰，但我们不会因此影响主线的探讨。那我们停止等待网络恢复，接着进行下一个环节，并且会有时间给大家提问和交流。

下一个环节我大致给大家展示一下，我们如何将先前的工作部署为一个产品。我们这篇测试文档就是这样处理的，你们只需将目标文本输入即可。

我们来观察一下，当前的提示词并不完善，我们需要换一个更能引领我们的提示词。所以我们将输入端的提示词换成刚才的那条。这让我们认识到，提示词是非常关键的一环。这应该让你们越来越明白程序运行原理。

如果看完我们的演示，你们发现有错误未检出，或修正部分无何变化，可能是我们的提示词设置还存在待优化的部分。一旦我们更换了这个提示词，我们将能获取到更多的反馈信息。

但大家需要注意的是，尽管看起来我们做了许多改变，但核心关键点的改变却非常微小。大家注意观察一下，我们进行异议部分的处理，认知的改变来源于我们对大模型领域的理解，这就是一个理由。

这样一来，我们得到的结果将会呈现非常好的效果。所以，我备有一个推进计划或者说是脚本，我让大家看一下这个脚本的输出效果，你们打开查看一下。这是我提前准备的脚本，你看，实际上是将我在知心球上的一段话进行了一些工作处理，你们发现这段话可以进行很多的编辑工作。

所以说，借助这里的工具，你们可以进行大量的工作，并从中获取到一些思维方向和策略。
例如，假定你是一本书，书名含有十三个字。是否可以这样进行定制，即编辑可以按需定制文档呢？我想大家应该清楚了。所以，现在大家正在聆听这场研讨会，理解我所说的内容了吧？

很好，今天的预演结束之后，我发现部分内容可能存在一些问题，诸如翻墙问题导致的网络问题，我估计有些人可能理解起来会有困难。但是，我们今天先不要深究这些细节，我们将在演示结束后再回头看这些问题。大家需要先理解我现在讲的内容。

所以，我们真正关注的，对我们实际应用的最重要的一点，就是找出最优的提示词。为了找到这个最优的提示词，我们可能需要花费大量的时间进行测试。测试完成后，第二步，我们要做的事情就是提高我们的工作效率。我们把提示词嵌入到AI调用的脚本当中。

然后，第三步，我们想将这个工具提供给更多的人，我们会添加授权、美化界面、并设置收费机制，以此向用户提供服务。因此，这就是整个优化工作流程。过去的六个月里，我们一直在努力理解如何更高效地进行工作，就像攀登八万英尺的高峰一样。

好的，今天的讲座我已经讲完了。现在，也许是因为我喝了七杯酒导致我稍微有一些忘记了这一点，但是我希望大家能清楚理解今天的讲座内容。
