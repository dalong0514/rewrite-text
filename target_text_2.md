首先，无疑对于翻译结果，大家会有各自的观点。你可以自行携带这一套指导方案，尝试用你专业领域的语料进行翻译以便校正其中的部分内容，看看如何将其调整以满足你的需求。让我更正一下一个偏差。同时，宝玉老师后来提出了一项新的修改建议。他指出我们前面执行了五步，虽然看起来逻辑很顺，但实际效果却似乎并非如此明显。那是否有可能进行进一步的简化呢？

他后来发现了一个简化的方法。这个方法就是先让一个英文老师进行直译，然后有两个中文老师进行不同风格的意译。最后，校长将英文的直译和两位中文老师的意译做一个整合，三者结合起来得出一个结果。最后得到的结果与之前的那个含有回忆过程的效果看起来差不多。

对于这个方法我进行了简单地测试，实际上，从我现有的英语和中文水平来看，我认为这两者的效果都相当不错。然而，我只进行了几次试验，并未进行完整严谨的测试，这只是我个人的感受，它的结果符合我们当前的感知。

在这个过程中，实际上我们只是做了简单的翻译工作，但我们发现了涉及到许多不同的技巧。例如，我们将全文作为上下文，仅将一部分直接转译。然后，我们用UGB进行了多重翻译，并由人进行选择。在翻译的过程中，我们进行了追问。可能会有客户满意度跟踪这样的方式进行二步翻译，期间也产生了多步操作，引入了新的角色。最后，我们通过回忆进行校验，等等。当我们在一个具体的细分领域里，如果我们想通过提示语，模型，和提示来更好地调用模型的能力，实际上会出现许多独特的技巧。

但是这些技巧并不是说，你只要掬一把浑水泼上去，模型就能得到好的结果。因为漫天飞舞的都是类似的说法，像是你只要按照一定的方式提问，就能得到结果。实际上，你只有将模型与专业领域做出精准的融合，你才能得到一个好的结果。而且只有在这个融合的过程中，我们才能依赖我们自己的专业能力去作出相应的判断，进一步调整这项技术。最后，我们就能得到一个效果好，成本合理的提示方案。
我曾深思过，如果AI能全程管理图书的翻译流程，可能会呈现出怎样的景象？根据我个人现阶段的理解，最初步和基础的辅助工作，如AI生成不同的译稿，然后由人工基于这些译稿进行翻译修订，这种方法完全可行。这其实与我以及宝玉老师所采用的方法有许多共通之处。我们在尝试这种方法的过程中，会发现当我们的中文表达不足的时候，我们会主动参与修正错误或者做些必要的补充。

对于那些我们人工操作的部分，我把它们标为蓝色；而对于AI可以进行的工作，我则用黄色来标注。然后，在这一系列过程中，当我们进行到编辑老师的校对环节时，你会发现，在某些环节上，AI可以更好地发挥其功能。

总的来说，我认为，当前在一个完整的业务流程中引入AI作为生产工具时，我们实际上需要引入一种流程思维。我们需要将整个业务流程分解为许多部分，并确定哪些部分适合AI处理，哪些部分需要人为深度介入。然后，在使用过程中，我们逐步引入相关的AI能力。

当我们提到AI时，我们并不只是局限于一个特定的AI。就算是基于GBG的模型，我们也认为AI具有许多不同的能力。我们通过设定一些提示语来把AI的能力局限下来，使其能适配到我们特殊的流程中。

所以，我想分享一个看法，即在将AI作为生产工具使用时，它需要与实际的工作流程充分融合，而并不是追求“端到端”的自动化过程。比如，有人会说：“我让AI直接为我写一个脚本，或者我指定一些设定，然后让AI为我写一个穿越时空的故事。” 但实际上，这并不可能实现，因为这个过程中需要很多人为的介入。

关于这个论点，有一个著名的设计理论，即在迭代的过程中，人应该主导这个迭代过程。我这里画了一个形象的图说明，我们将一个业务分为多个流程，有的流程中AI的参与较少，有的流程中AI的参与就相对较多。像翻译这个业务流程，最后一步是人工进行的校对环节。其实在现今，AI可以为此环节贡献80%的工作量。

但是也有一些环节，比如最后的定稿环节，AI可能不能百分百靠谱，最多可以帮你快速预览一遍，但定稿还是需要你的主编，甚至作者本人来完成。所以，这是我对在将AI作为生产工具使用时的首要启示。接下来，我们将继续讨论如何使用AI来改造我们的工作机器。
在刚开始的讲座中，我首先提到了在基于深度学习的逻辑框架上，我们应如何去理解和利用机器AI。我们需要将整个过程定下一个明确的目标，然后用这个目标来指导深度学习机器的行为，并由此得到一个结果。在得到这个结果之后，我们将其与设定的目标进行对比，这可以使我们清楚地看到应该如何优化我们的机器AI。

在实现这个过程的时候，我们主要依赖三个部分：其一是我们自身，因为人是机器AI的最主要用户和调试者；其二是我们执行的工作流程，也就是常说的标准作业程序（或简称为SOP）；其三是我们正在使用的各种各样的相关技术工具。如果我们想用人工智能（AI）来改造自己的机器，我们应该怎么做呢？

我通常认为，无论我们是在使用任何一种技术工具，还是作为程序员引入新的库，或者引入一个新的云服务，这些操作都可以分为四个步骤。但在众多的技术书籍中，一本本的“从入门到精通”似乎变成了“从入门到放弃”，我们会发现，实际上想要真正精通一项技术，往往非常困难，我们可能在操作过程中就会选择放弃。

那么问题来了，我们如何有效的使用这个技术工具呢？最有效的方法，我认为是先去理解它的基本原理和工作方式。这点实现其实并不难，因为这个技术工具毕竟是有人创造出来的，我们这些灵活的用户总是能够理解并挖掘出它的潜力。然后，我们需要评估这个工具的优点与不足，以及确定它是否适用于我们的具体环境。

而，什么时候我们最容易放弃一个工具呢？通常是在我们实际并不需要它，或者说用不用这个工具对我们来说没有明显区别的时候。但是，如果这个工具的某个功能我们是必须要用的，那么我们很可能会坚持下来，甚至可能会熟能生巧，然后在熟练和精通之间进步。在这种情况下，我们就不太可能会从入门到放弃，只有当我们必须使用某个工具的某个功能时，我们才可能不会放弃它。

因此，我认为在引入新的技术工具时，我们需要找到该工具上的一个“单点”——即那个能让我们真正产生效果的点。一旦我们发现这个“单点”的效果非常好，我们就可以尝试着把这个“单点”的规模大大放大，这样我们最终就可以获得更多的效果。

在实际操作中，如果一个公司内部的员工发现了这样一个“单点”，并且这个“单点”带来的效果非常好，那么公司其他的员工也有可能会用上这个方法，这对于整个公司来说都会带来很大的效益。因此，无论我们过去的几年中如何使用技术工具，我都认为这四个步骤是非常关键的。

那么，当AI大语言模型出现之后，我们也同样需要遵循这四个步骤。无论是在调试机器AI，还是在引入新的技术工具，这四个步骤都会始终如一的指导我们前行。
今年上半年，许多人踏出了对于人工智能技术学习与理解的第一步，旨在探寻其中的奥秘原理。有一种观点，短时间内，无论是专业人士还是公众，似乎都已经基本了解了这个技术。其中，有一个重要的原理就是转化器（Transformer）模型的运作方式，它是很多人学习理解过程中的重要步骤。部分人会进一步了解每一层层次的功能与工作原理，还有的人则会扩展深入到各种不同的模型，如语言模型、图像模型，以及各种先进的技术如CTRLT等。

我们经历了理论的了解后，自然就会实际应用测试。在这个过程，拥有交互界面的API接口为我们提供了巨大的便利。我们可以借助它进行各种图像生成的试用，对自己设计的模型进行训练。而当我们遇到困难或者问题时，我们会搜索相关的技巧或者指导，这些都会大大提升我们的研究进度。

我们作为技术人员，日常工作中需要使用更高级、更复杂的工具。不过，我自己曾进入了一个陷阱，那就是对开源模型的微调。我试图通过微调来提升模型的效果，但是最后发现，相比于闭源模型，开源模型的效果反而不如人意。于是我又回到闭源模型的研究之中。

在使用这些模型的过程中，我们发现了一个问题，那就是这些模型并不能理解我们的专业知识和新兴知识。特别是我们这些专业领域的知识，于是我们采取了解索增强生成的方式。我们输入大量的资料库，研究如何最有效的处理这些资料库，尽快准确地提取我们想要的内容。这是一个充满挑战的过程，我们也曾经走过许多弯路。

然而，当我们在对模型进行某种程度的更改之后，我们需要进行测试，验证我们的改变是否提升了模型的性能，是否达到了我们的期待。这个过程需要我们投入大量的时间去了解模型性能的评估方法，无论对于我们改进后的模型，还是如IAG和n untime之后的模型。仅有在评估后，我们才能安全地在生产环节中使用这些模型。

总的来说，这些都是一种理解的过程。在这之后，我们还需要对技术本身进行全面地评估，这是一个很多人容易忽视，但非常重要的阶段。我们需要对技术决策进行远瞻预判，例如，我们预判GPT-5将要到来，或者预计不久的将来会有很多与GPT-4相近的中国本土模型落地应用。

值得注意的是，当我们在进行应用开发时，如GBG在中国的应用实践中，我们已经遇到了一定的困难。如果我们想要开发一些更具野心的应用，我们可能需要放弃已经不能满足需求的GBT-4模型，而转向国内经过验证的模型。这就需要我们评估与跟踪这些国内模型的发展，以预判它们何时能够达到相对理想的性能水准。
自然语言处理中的技巧实在是琳琅满目。举例来说，我们或许无法直接采用GPT-4模型来得出最终的结果，但是我们完全可以将微调过的模型，或者是国内某模型经过微调后，和GPT-4进行混合使用，这样便能够优化出期望的效果。然后在通过GPT-4对这个混合模型进行校准，校准完之后再正式投入使用。

当然，在此过程中，我们需要对当前深度学习模型的发展态势有较好的认识，尤其是需要了解这些大语言模型的局限性，以及可能潜藏的风险。例如，我在今年四月所参与的一个项目，在上线前的最后几天，我们发现其在安全问题上处理得并不理想，安全异常不是只有那些违背广义价值观的问题，因为这样的问题，GPT-4已经为我们预设好了一套处理方案。然而我们那时所碰到的问题，主要是与中国特有的社会环境有关的问题，这些问题的解决缺乏成熟的措施或者解决方案。

这些问题必须解决，否则产品无法投入使用。如果你在自己的企业内部使用，可能只涉及到几十个人，或几百个人，问题可以尽量被控制。但是一旦你打算将其推广给不特定的公众，这些问题必须得到妥善的处理。在这个项目进行过程中，我们感到深深的压力和风险，这个所谓的风险就像一场意想不到的惊吓。

当然，既然有了对风险的觉知，便能够找到解决的办法。事实上，只要你意识到且面对风险，找到解决方案并非难事。我们也需要从更广的视角来看待这个问题，比如这个模型可能会对社会或者产品带来何种影响。我们必须时刻提防，避免我们的产品被那些有意使用这些模型恶意使用的人所利用。

在这个 использовGPT模型来模型来解决实际问题的过程当中，我开始感觉到我们需要深入了解和评估这个模型。回顾了一下我自己和别人向GPT模型提问的方式，我认为在使用这种大型语言模型的时候，我们需要遵循一些基本原则。这些原则应当以个人的视角为出发点。

我们必须明白，尽管看起来GPT模型可以理解和生成语言，并且具有一定的推理能力。但是如果我们从更基础的层次来理解，这个模型的基本运作原理，是通过大量的语料学习和调整参数，然后在运行时通过预测下一个词，以构建出连贯的回答。这一过程中，模型的能力得到了泛化，表现出来的结果看起来很像人的智能。然而，如果我们回归到根本，实际上模型在做的还是预测下一个词。

当我们从这个角度理解和对待模型时，我们就能够解决很多我们在长期使用模型过程中遇到的问题。比如一开始我们往往会忽视模型的错误，认为模型只不过是在做预测而已，可能会出错，我们需要人来做判断。然而当我们抛弃这种轻视的态度，我们真正地理解并用心对待模型时，我们就可能在对人工智能这个领域有更深的把握和理解。
在他的讲解过程中，语言显得非常专业，并且非常具有权威性。对于一些高深且复杂的知识，他常常能深入地讲解，十分冗长。并不是专业人士可能无法理解，而他却能如数家珍地说出这些知识，让你感觉他随口一吐就是一篇论文。在这样的情况下，普通人很有可能会毫无保留地相信他。然而，当我们向他们揭示，他们正在使用的实际上只是一个大语言模型，它其实并没有他们想象的那么智慧，而只是根据已有的数据信息来预测下一个词语的产出，他们对大语言模型的过度信任就会得到一定程度的修正。因此，在推广这类AI产品的时候，有必要向普通用户解释，他们不能过于轻信大语言模型所给出的答案。另外，大语言模型会创造许多的幻觉，而这些幻觉并不一定是模型本身的问题。

如果你深入理解大语言模型的工作原理，你会明白它是如何通过大规模的数据学习，根据给定的一段文本进行延续性的生成。如今，我们将这个概念扩展至问答系统中，你只需输入一个问题，它便能根据学习的知识给出一个合适的回答，这其实也是一种反向的延续性生成。然而，问题的提出者可能会无意间将大语言模型的回答引导至错误的路径，这并不是模型的错，也许是我们自己的问题。虽然我会从常规的角度去解释这个问题，但实际上我更注重我们作为一个提问者对问题的思考。

以一次实验为例，我曾经问过大语言模型一个问题：“请您解释一下杜甫的窗前明月光是什么？”这其中包含了两个误导性的陷阱，首先，并没有一首叫“窗前明月光”的诗，其次，“窗前明月光”其实是“床前明月光”误写。然而，当这样的问题出现后，大语言模型会继续按照我们的问题进行解答并即兴创作，比如说，“这首诗表达了诗人的某种心情。”在某些情况下，模型无法识别问题中的误导，但如果模型具有高级的理解和判断能力，它就可以发现我们提出的问题存在错误，并纠正这些错误。因此，我们经常会使用这样带有误导性的问题去测试大语言模型的表现。

我还会使用类似的方法进行另一项实验。我曾经问过一个问题：“为什么鲁迅和周树人不能够互相争吵？”一款国内的大语言模型立刻回答说：“他们都是民国时期的知名知识分子，为什么他们不能和平相处？”这让我们对模型的智能程度感到困惑。然而，随着时间的推移，到了七八月份，我们发现许多模型对于这样经常被用来测试的问题，已经有了比较好的改进并给出了正确的回答。因此，向大语言模型提问时，我们需要了解它存在的幻觉问题。

第三，当我们向AI提问时，包括宝玉老师早些时候的提问，我们实际上都是采用一种结构化的提示语。
该讲座主要内容是介绍了一种由海外工程师编写的技术教程中使用的解决问题的框架，该框架将问题解决的过程分为四个步骤：指令（Instruction）、上下文（Context）、数据（Data）和输出（Output），以此形成了ICDO模型。更具体地说，当我们用这个模型来和聊天机器人交流，例如让其执行某项任务时，我们就按照这个顺序给出指示。例如，“你帮我做......”，然后机器人就会理解我们的意图，按照指示去执行操作。接下来的讲座会进一步详细解析这个模型。

另一个值得注意的模型是GBG三模型，它从对应的学术论文中提炼而来。此模型把语言模型视作一种学习器，能够从我们的指导和提示中学习，并在处理大量新的信息时对这些指导产生反应。比如，我们可以在提示中添加足够多的信息给聊天机器人，让它学习并理解这些信息，在接下来立即对我们提出的问题作出回答。这样的回答通常能够达到我们的期望，因为聊天机器人的回答主要是依靠理解和推理，而不是去其庞大的信息库中寻找答案。这样的方法也克服了由于同一问题可能有不同的答案，机器人无法确定哪个答案更适合的问题。在我们提供了信息后，聊天机器人可以针对当前给定的信息更好地进行回答。

我们发现引导模型进行链式思考是很有效的，这在许多讨论中都得到了证实。我从“快速思考与慢速思考”的概念中得到启发，认为让模型执行链式思考其实就是让它运用两种系统：快速思考（即直觉）和慢速思考（即理性推理）。当我们引导模型进行链式思考时，我们要求模型按照一定的顺序，步步为营地逐步推理出结果，而不是直觉性地一次性给出答案。这也解释了为什么链式思考能引发进一步的思考，因为它是以步骤为单位，逐步深入思考。

然而，有时不是我们要求模型一步一步思考，然后让它全部执行。更常见的情况是，我们在外部将一个大任务分解成许多小任务，再让模型去执行。执行这种方式的主要原因在于，虽然今天的模型可能能完成一项大任务，但在完成后，我们无法确定其结果是否符合我们的预期，也无法知道其在整个过程中是如何工作的。因此，通过将大任务分解成小任务，能让我们更具信心地指导模型的工作。
确实，当我们面对终局结果时，往往并无足够的信心去预判其是好还是不好。不过，情况会在我们对任务进行分解后发生变化。我们 可以逐步完成任务，通过每一步的执行，我们都能对它的结果进行合理的评判——它的结果是好还是不好。在每一步步骤结束时，我们能对整个过程产生信心，进而对最终的结果产生信心。在执行过程中，我们也鼓励相关人员介入其中，以期通过他们的参与、优化，使最终结果更加理想。

同样，当我们把一个复杂任务分解为简单的步骤后，那么在每一步的执行过程中，我们都可以利用AI模型进行优化。这样，在优化我们的步骤后，每一步的结果可以更进一步地优化，使得整体的结果更优秀。显然，这种方式要比我们编写一个提示语句，给出一个模型，然后让AI模型直接给出一个推理结果的方式要好很多。

接下来进入原则之六的内容，即我们要将复杂的任务拆解为简单的任务。例如，如果我们提出的问题比较短，那我们的问题本身就比较简单。但是当我们提出一个很长的问题时，那我们就会涉及到很多部分。在处理这些部分时，我们需要用不同的方式，使得语言模型能够很好地理解它是什么。如，代码可以用反引号标示，长段的文章建议用三个英文引号等等。在举例说明的时候，我们用什么样的方式来进行编号，都没有特定的规式，只需要采用一种相对固定的方式，以便让模型更好地理解。找寻和借鉴各种经验，通过尝试，最终找出一个自己对现阶段满意的方法，这就需要每个人自己去做。

最后提到第八个原则，也许是最重要的一点。我们常说AI就像机长的副驾驶，但我们仍应坐在主驾驶的位置。坐在主驾驶位置的我们，肩负着四个角色的职责。首先，我们需要明确一点：我做的这件事情的目标是什么，或者更深入一些，我做这件事情的目的是什么？只有人类才会去追问这个问题，机器是不能做到的。第二，我们需要了解AI在哪，了解它的状态、位置。第三，我们要有判别和鉴赏的能力。在判断方面，我把它细分为两部分，一个是直觉判别力，另一个是理性判别力。
首先，我们谈到当前人工智能(AI)的能力，它已经能够参与到大量的事务中，如你们大家都熟知的，甚至可以利用AI进行股票投资，帮助决策。然而，请注意，虽然AI可以提供建议和决策支持，但对于任何结果，无论是利益还是损失，最终承受者始终是我们自己，并非AI。答案的风险和收益仍然由我们来承担。

然后，我们来探讨如何设计有效的提示语。提示语有很多种模板，我知道工程师们之前使用过一套叫做ICDO的模型，它的设计理念我认为是十分合理的。ICDU就是最初别人写出来的简写，我们只是对其重新命名。不过此后，我们其实还进行了一些进一步的优化和改善。我们认为在指令部分，我们可以进行更详细的划分。首先，我们定义了角色，你是什么角色，要承担什么职责？接下来，我们给出任务目标，你今天的任务是什么？并规定了一些规则，例如，你在中国环境下，甚至是全球范围内，都不能讨论任何有关政治、星座或娱乐的议题等等。设置了这些规则后，AI就能在这些规则的指导下进行操作。

接下来，我们给出了更多的上下文。我们认为至少应该有三类上下文。第一类，假设今天我要求你运用某种特定的商业模型，例如，我要求你使用这样的商业模型进行分析，那我就需要把这个模型的相关知识告诉你。同样假设大家中有不少人是从事心理学相关工作的。假如我需要你用心理学模型进行分析，那我就会再对这个心理学模型进行一次详述，以帮助AI更好地完成任务。第二类，我们会详细叙述完整的操作步骤，一步一步讲清楚该如何进行，并要求AI将整个执行过程都记录下来，这样AI的表现会更好。第三类就是少样本学习，我们可以在数据里放一些少量的样本，帮助AI进行更好的学习，从而获取更好的结果。

最后我们来谈谈ICD模型。这里，I就是我们输入的数据，C代表上下文，D就是我们对输出的一些要求。以翻译为例，我们应明确哪一段文本需要翻译，这就是输入数据；在上下文部分，我们可以提供许多有用的背景信息；然后，我们需要设定对输出的要求，指导AI如何进行输出。例如，我们希望AI的输出可以放在markdown文件里，使我们能方便地复制，或者我们会有一些特定的格式需求等等。

按照以上的方式组织提示语，我们可以获得相对理想的效果。这种做法在之前，可能只有程序员才会去用。但如今，我们让它成为了一个更加通用的工具。
后来，我们引入了“自定义指令”（customer instruction）的概念。这个概念表示，在界面的设置中，我们可以把相关内容放入自定义指令里，从而获得优良的效果。

我们从此得到了一项重要的启发，即在今天使用大型语言模型时，我们其实处于一种“人机共舞”的状况，也就是人和AI共同合作的状态。在这种合作中，AI的角色主要是辅助性质的。我这里用三个相关的词来描述其角色：助教、助手和顾问。

首先，AI可以被理解为是一个热情的“助教”。他极其热情地获知我们需要的知识，然而他所提供的信息有一定概率会出错，至少与教授相比，他出错的概率会相对较大。我们在对待助教的态度上，不会轻信其每一项建议，但我们又相信他是聪明而且热情的。

其次，AI可以被视为一个“助手”。他并不是负责一个部门的负责人，因为如果是部门负责人，他必须为整个部门的运行承担责任。但作为一个助手，他只负责帮助完成任务，然后由我们检查他的工作效果。

最后，AI也可以被视为一个“顾问”。顾问与高级公司管理人员的主要区别在于，管理人员需要承担责任，而顾问则不必。尽管顾问会提供许多建议，但如果最后结果不理想，责任并不在他。即使是大型的咨询公司，他们只是提供咨询，收取咨询费，如果提议实施后公司业绩下滑，还是公司自己承担责任。顾问可以提供许多聪明的想法，但最后承担责任的还是我们自己。

因此，我们认为，AI的角色类似于助教、助手和顾问。那么，人应该做什么呢？我认为，尽管AI现在能够做很多事情，我们人类应该去做更难的事情，即机器无法完成的事情，或者说大部分人无法做好的事情。这就是我们需要做的“更大的作品”。

例如，AI在处理小事情上表现得相当优秀，但在理解和完成大事情上，他有些力不从心。因此，我们需要让自己去做更大的事情，像写一本书这样的事情。我们知道，写一本书需要投入巨大的精力，例如，进行大量的采访，跟许多人交谈，阅读和研究许多学术论文，实施大量实验，进行大量的文本调研，进行深度的讨论。这些事情都不可能由AI来完成。

除此之外，我们也需要做品味更高的事情，因为机器没有品味，他的品味最好也只能达到普通大众的水准，或者只能达到互联网上一些普通人的平均水准。而作为一名有追求的人，我们应该致力于达到更高的品味水准。

最后，我们需要做的是为我们的客户提供有价值的事情。在这个过程中，我们需要去理解客户的需求，然后用我们的产品去满足他们的需求。
在研究和实践中，我发现我们确实需要以一种更优化的方式来提高自身的表现，尤其在进行知识生产类工作的时候。在操作过程中，我发现存在几种不同的情况。

首先，有时我能完成一个项目，但是却无法感受到成就感，即不论我产生了何种成果，产出都显得不够，不满意。此时，我认为我应着重于将自己能够做好、并对本身极为满意的作品尽可能地做出来。然而，仅此还不够，我们需要再深入一步：我们所做出的产品，应该不仅仅出于自我需求，而更应当能满足他人实际的需求。

然而，我们如何判断他人实际的需求呢？一种有效的方式就是看这个人是否愿意为此付出些什么。付出的形式不一定是实打实的金钱，也可能是时间、注意力等等。例如，一个人会说，我可以找时间与你面谈，讨论这个问题。他对这个问题的诚意是完全不同的，因此，明辨我们所做的东西，究竟是否真的被其他人所需，是非常重要的。

在这个过程中，你会发现有时我们无法完成一项工作，有时又觉得我们的表现并不理想。但是，对于那些连人工智能都无法完成的工作或产品，我们是无法将其转交给AI来处理的。期间，你可能会察觉到一个有趣的框架，它可以帮助我们思考：实际上，有许多事情，我们必须亲自去做。

然而，随着时间的推移，我们也会逐渐理解一点：我们不应去做AI可以并且可以做得很好的事情。只要AI能够出色地完成，我们就应该将其交给AI，让AI尽可能地去做。而我们则应该专注于那些更加困难、大规模、并且对最终的用户客户具有实质价值的工作。

这之后，我想进一步探讨第三个观点，这也是个使我深感痛苦的启发。因为我们在长期的工作中，一直强调产品的精确性和准确性，否则它就无法投入使用。这就像在工作中，我们常常纠结于一定要把这个结果做得非常准确。然而，大语言模型却不同，当我们以准确性为评估标准时，往往只能给它打四十分或五十分。

即使我们付出了许多的努力，让它的得分提升到六十分及格，甚至在后期达到了七十分，但是我们还是不能让它达到满分。现在，我们已经意识到，其实大语言模型达到七十五分，基本上对于一些浅层次的使用已经足够了。

与此同时，我们还发现了一个有趣的事实：我们与AI合作的最好方式，可能就是享受它的随机特性。也就是说，我们要接受这一点：AI并非总是完美的，而且在一些情况下，它可能表现得颇有随机性。
我们首先通过使用"Control that"对图像进行草图设计，无论是我们怎样引领其进行绘制，或者我们训练模型让它了解我们期待的效果，我们发现，它总是无法完全满足我们的期待。但如果我们稍微放宽一些要求，比如当我们希望设计一些配图或者插图，此时，如果我们仅仅给出一些创意的提示，让模型未受约束地发挥其创造性，然后再根据模型输出的结果进行相应的调整，我们会发现其实这样的结果还是不错的。

我个人觉得，可能在未来的一到两年的时间里，我们还需要在探索如何有效地引导机器进行创造性工作。例如，前不久我在设计一本书的插图时，并未寻求外部设计师的帮助，而是我们自己进行了绘画。在尝试和修改的过程中，我发现这种方法其实很有用。当然，在此过程中，我查看了很多不同的设计，逐一筛选，最后选出了一个不错的设计并进行了相应的调整。在这个过程中，我们会发现，充分利用模型的随机性，最终可以生成一系列有趣的设计，并达到很好的效果。

因此，我认为在使用AI时，我们需要充分利用其内在的随机性。接下来我们再展开讲述大语言模型在教育方面的应用。

在教育领域，我们看到了很多大语言模型的应用例子。尽管这些模型还未在K-12教育中全面应用，但已经有许多相关的讨论和尝试。这个过程中我们开始了解到，大语言模型有很多可能的应用场景。比如在一本名为"GBT入门"的书中，作者用了一个例子说明我们可以让GBT进行文章的撰写。

此处的撰写，并不是简单的文字输出，我们可以有策略的向它提问，从而让其输出不同风格的文章。最后我们可以看到，同一内容，大学生、研究生、四年级小学生、初二学生、AP英语学习者等不同群体的写作结果将有怎样的区别。这只是一种理想的应用场景，然而如果我们将这种策略应用到实际的教学场景中，教师可以让AI模仿一种特定的风格撰写文章，或者生成不同版本的样稿，之后教师可以根据这些样稿进行修改和指导。

此外，教师还可以让AI生成一些错误的文字，让学生们对其进行修改，从而提升他们的学习效果。因此，我们可以利用AI扮演不同的角色，展现出不同的风格。我们还可以让AI做一对一的导师，这种方式可以大幅度提升学生的学习成效。
经过查阅若干相关资料后，我相信大语言模型（例如GBT）对于教育的影响力。虽然有人将其视为过时的说法，但在我看来，其理论是经过反复验证，并且从常识角度来看，也是合理的。

我们可以从教学模式的角度来理解这个问题。设想一个班级，哪怕只有二十个学生，教师也需要面临分配注意力的困难。然而，在一对一的教学环境中，教师能够就学生的问题进行有针对性的指导，这无疑可以极大地提高学生的学习效率。在过去，我们可能需要借助家教来实现一对一教学的模式，而这往往会付出较高的成本。

然而当今，基于深度学习的大语言模型，例如GBT，可以通过智能提示语来实现更加精准的指导。比如说我们可以让GBT适应每一个学生的学习水平，并对其进行个性化的辅导。自然，这也可以用于辅助学习。

想象我们有一个教师，其手下有三十个学生。这位教师可以将每个学生的作文输入到语言模型中，让模型给出修改建议。这些建议将针对每个学生的特性，因此每个学生获得的都是高度个性化的改进建议。用这种方式，教师、家长，甚至我们作为成人，都可以在学习过程中得到语言模型的有效帮助。

在教育领域，我看到的一种具有启发性的应用示例是翻转教学。大家可能听说过课堂翻转，这次翻转的是角色——让AI模拟学生，让真实的学生扮演教师的角色。我非常欣赏这个想法，因为它巧妙地避免了AI的一个主要问题：不可避免的错误答案。

如果我们将AI的答案视为正确的，介绍给学生，那么学生可能会学到错误的知识。然而，如果我们反过来，让AI扮演学生，让真人学生扮演教师，那么就算AI的回答错误，学生也有能力发现并纠正。找出彼人的错误，实质上是一个很好的学习过程。这样，通过改正AI的错误，真人学生不仅提升了自己的理解，同时也避免了接受错误答案的风险。

这种巧妙的方式，既利用了学习的原理，又避免了AI的缺陷。我们还可以进一步优化这个教学模式，例如，我们可以在学生找出并纠正AI错误后，提供正确的答案，以便学生能够更好地理解判断。在这个过程中，学生的学习过程也能够连续地进行。

在我最近的一本书中，我画了一个2x2的图，以便更直观地展示这一理论。
在我观察过的材料中，有一些具有高完成度和正确性，这类资料基本上呈现出教科书的特点。然而，这种资料虽然内容正确，但并不全面地覆盖一个领域。例如，许多人采用学术论文作为学习材料，但这些论文其实只是个别领域发展进程中的一部分，材料本身的内容仅一块一块地展现出特定专业的知识点，它们是非常合适且重要的参考资料。

此外，一些学生的作业也可以作为参考。某些作业可能看起来已经完成了，但实际上可能存在很多错误。此外，我在研究的话题有一部分可能还没有完成，因此与自己关联的问题模型都还是“未完成的”，可能存在着错误。正因为如此，这些其实是非常好的学习材料。

目前，AI的发展为我们提供了大量全新的学习材料。例如，沃顿商学院的教授伊森·蒙克和他的妻子（从他们的姓名看，我认为他们应该是夫妻关系），就是一对这样积极进行研究的学者。她在沃顿的一家与教育相关的中小学教育机构担任教学总监。在过去一年中，他们发表了多篇论文，其中一篇探讨了如何将AI用于教学的五种策略。虽然这不是一篇严谨的论文，但他们以快速精练的方式做出了一些总结。

我认为他们提出的这五种策略都十分出色，非常适合学校的教师用以辅助AI教学。可以借助AI解释各种知识，提供实例进行说明，这样可以帮助学生更好地理解概念。AI还可以生成详细解释，让书本的内容更为充实；也可以替教师产出一些不重要的考试题目，这称作“低风险测试和诊断”。因为期末考试对于学生的影响大，意味着高风险，如果考试成绩不好，则可能得到低分。但是平时需要进行大量的低风险测试，这样即使学生答错了也无关紧要。也因为过去这样的题目太少，所以他们提出可以用AI来为我们生成这样的测试题目。

同时，他们也建议让学生们进行“翻转课堂”，让学生们试着教授一个话题。过去，老师需要花很多精力去详细审阅每一个学生的作业，而现在在AI的辅助下，教师可以先用AI工具看一遍学生的作品，然后在AI给出的建议基础之上，给出对学生的最终评语和建议。
让我们进一步讨论如何利用计算机技术和深度学习，将老师的教学压力减至最低，同时还可以让他们有更多的时间去关注学生们具体且深入的问题。在这方面，状元教育的一种重要理念值得我们借鉴，即在新单元的考试中，把上一个单元或前几个单元的相关内容放进来，以插播或交错的形式出现。这样的布置将有助于学生更好的掌握整体知识。

传统教科书通常按单元来组织，很难将相关内容融入进来。但利用大语言模型，我们可以将这种教学理念告诉模型，让模型为我们组织相关的知识和测试题，然后调整后放入教学内容。通过这种方式，老师可以更好地将分散练习的方法应用到实际教学中去。

对于小学生，可能还需要别人的辅助使用，或者只能提出简单的问题。但对于成年学生，比如商学院的学生，他们就可以自己设计各种新的提示，新的提示语角色。

在这里，我想谈谈微软的提示语仓库。他们已经将一些实例录入进去，其中主要的部分是放在角色那里。但是，角色部分并不只是简单地定义一个角色，而是需要复杂的分析和考虑：他是什么角色？现在扮演的是什么？他的特征是什么？遇到的问题是什么？最后该如何解决？因此，这套提示语在我看来，具有非常大的借鉴意义。

在我稍后分享的链接中，你们可以找到相关的提示语。这个仓库除了教育领域的内容之外，还有许多其他领域的提示语，都值得借鉴和学习。

在此，我想提及一个启示：当我们使用AI进行工作生产时，应该首先想到的一个人可能已经有六个月前就提出了这个观点。这可能对我们的工作有所启发，帮助我们从不同角度审视问题，提出新的解决思路。
