是的，接下来，我要向大家介绍泛方科技的招聘需求。泛方科技是中国排名前四的量化基金之一。这意味着该公司的资金实力比起一般的IT公司将更为强大。泛方科技的资金来源是股市，他们甚至拥有价值一万块的GPU资源。

泛方科技有一个招聘岗位非常吸引人，尽管他们没有明确指出。这个职位非常适合我们这里的一部分同学，特别是那些对各个行业知识储备丰富、有阅读《阳光下的透视镜》等书的人。泛方科技希望这个职位的员工能为他们收集不同行业中的优质语料。

现在我要提醒大家的是，这个职位是在泛方科技的杭州总部招聘的，他们在北京还有一家分公司。很多人往往容易忽略这样的岗位，所以，我想告诉大家，千万不要低估你在AI时代的可能性，否则会错过大量机会。

接下来，我要讲的是竞品。竞品太多了，每天都有新的产品出现。所以我们要做的是跟踪最新的AI资讯。其中，国内的AI资讯，大家只需要关注“36氪”和“虎嗅”就可以了。这两个平台都是国内做得最好的IT资讯平台。

然而，这两个平台之间有个很大的差别。“36氪”比较注重信息的速度，而不太强调价值观。而“虎嗅”恰好相反，它非常注重价值观，以及与读者的共情，会讲许多深度的故事和行业八卦。不过，过多的共情有时会浪费我们的时间。

其中，“36氪”的资讯源主要是采用大模型处理。我们一般会按照发布时间顺序去跟踪。至于国外的IT资讯，我们一般通过“GitHub”这个平台去跟踪。因为目前，世界上大部分的大语言模型都托管在GitHub上。

现在，我将演示如何在这些平台上跟踪最新资讯，并提供一些注意事项和技巧。“36氪”的网站和APP都是好的选择，我们可以根据自己的喜好来选择使用。
首先，我要说明的是，定义一个关键词就能概括所谓的"大模型"。你可以将其看作是具备了所有顶尖信息技术技能的巨星。然而，我要提醒大家，不要被一些吸引眼球的标题所诱惑，无谓地将注意力投向那些会极度浪费你时间的信息。实际上，你只需通过下载相关应用并设定关键词就能满足需求。

在我们的研究中，我们并不重视那些稍显滞后的国内资讯。我们真正应关注的重点在于如何获取帮助以及其他重要事务。那么该如何获取帮助呢？你们可以看一下。

这里有一个关键的小技巧。我们通过GPP搜索得出的信息量通常很大。例如，我记得在五月份的总结中，我似乎找到了两万条信息。但现在，如你们所见，我们有了十万个项目。目前的情况是，在这个领域，我们必须理清真正有价值的东西是什么。真正有价值的不是信息量，而是开发者的品位，他们的品位远胜于我们。因为他们具备识别那些无关紧要的信息的能力，这就像识别出了某些人像王小轩这样，可能只是购买了一些马甲账号而已，这对我们毫无意义。唯有开发者的眼光才能说明实质性问题。

举个例子，我们在评价一本书时，它的销量对我们毫无意义。但是，如果一本书被列为学术界核心青年参考文献，那就十分有意义。在程序开发领域，同样的参考文献就是这些项目，大家明白了吗？因此，我们可以尝试使用这个小技巧：选择一下某个参数，例如，50到100个focus，或者大于500个focal。大于500个focal的项目，基本都是顶级项目。你们看到了吗？这样一来，项目数量就直接减少到了107个。因此，你只需要深入理解这107个项目就足够了，不要被那十万个项目误导。

其中的十万个项目，有很多是垃圾信息，是营销手段，是忽悠人的。甚至很多还是一些大公司的工程师编造出来，以忽悠老板，作为绩效考核的项目，我们将其称为“绩效评估项目”。
过一段时间后，这些项目总有一天无人维护，理解我的意思吗？只有那些顶级开发者们围绕着风口关注的项目，比如项目规模在五百以上的这种，我们才会去积极引用和关注。当我看到一个项目可以有效提升我的工作效率时，我便会选择调用它。这种有益的共享项目，就具有了极高的意义。你们都明白了吗？

好的话，那我推荐大家可以稍微了解一下，也就是对这些项目产生兴趣，并将其标记下来。标记下来之后，就可以明确我们关注的重点了。此外，还有一种稍微重要一点的方法，就是咨询师的建议和指导。

另外要强调的是，我们还可以通过甄选，比如借助左侧一些工具，像是角辩语言等等，来筛选更多的有用项目。一大部分项目是针对GS和TS这样的前端项目。例如，如果你想要改善自己使用GPT等这类人机交互区的效率。只需要单独研究这个领域就够了。我们看一下，所有这些趣味项目，都是以人机交互为基础的。你们理解了吗？

但在实际情况下，我们提及我们的生产效率，仅仅依靠人机交互是远远不够的。我们需要深入到一些底层问题上去。这些底层问题，往往是和沟语言和Rush语言等有关系。比如，我们可以运用沟语言进行筛选。

大家可以多关注一下这类项目，尤其是我强烈推荐的一些项目。例如这一个，由Open i事务所发布的项目。虽然该项目一般，但曾经也产生过一定的影响。其主要是集结了当前十几个大型模型API，并将它们打包到一个地方，容易调用和使用。

这个项目的主要思路是什么呢？它的思路是一种脱离我们常规思维的方式。有的学生看了这个项目后，往往会立刻想去实行。然而，这个项目本质上只解决了一个问题，即将十几个大型模型集成在一起。

这时候，大家要对开源界的项目有一个清晰的认识：并不是所有的项目都完美地提供应用方案，而是专注于某个小领域，并在这个领域中做到最极致。
本篇的核心话题围绕着大模型、人机加速、开源项目以及运用开源项目的相关问题展开。通过对所谓“一锅炖”的解决方案进行探索，我们寻找由多个大型模型联手解决问题的可能性。在这个过程中，一个重要的考虑因素是如何以更用户友好的方式利用人机协作以进行加速。

关于开源的讨论，在目前的计算机科技界已经非常丰富了。开源的理念并不仅仅是在商业软件的购买和使用上提供替代方案。反而，开源方案旨在通过解决特定的需求和问题。因此，开源项目不是全能解决方案，而是为了解决某个特定需求而生的工具。当一个开源项目美妙地解决一个问题，然后另一个开源项目又美妙地解决另一个问题时，这两个开源项目就会在相互影响之间变得更有影响力，并可能进行一些协作，这就是开源世界的生态。

因此，我们需要清楚明白，在使用开源项目时，我们往往需要多个项目的联合作用，而非仅依赖一个项目来解决问题。例如，如果你查看某项技术的相关项目，你就会发现这个技术目前支持的项目非常多，包括国内外的开源项目，甚至还包括第三方代理等等，而且部署也特别简单，能支持各种各样的方式。

这就导致我们可以看到这些项目的互相关联。三个具有特定功能的项目可以形成一个独特，新颖的群体。理论上讲，在这个过程中，所有问题都能得以解决。归根结底，通过大量密集的努力，开源界已经基本解决了我们能想到的所有需求。

再次强调，使用开源项目不仅仅是使用单个项目，而是需要利用一系列的项目进行组合，才能解决我们的问题。比如部署到第三方平台，最推荐的是第一个平台，这个平台是目前做的最好的，服务器放在新加坡，使用一种我们不怎么说的科学算法。

随后的内容会更多的涉及到这个大的思路，以及第三方平台的利用等题目。同时，对于某个国外电视节目的探究，其中一个重点是看电视哪几个栏目，趋势这个栏目吸引了很多的注意力。此外，还会发现它分为三个核心的表格：模型、数据企业和空间。在这三个表格中，模型这个表格是最重要的。

总的来说，本讲座致力于揭示大模型和开源项目在解决问题方面的无限可能性，以及如何有效地利用这些工具以满足我们的需求。只有正确理解和应用这些开源工具，我们才能完全利用这些工具的潜力，以实现我们的目标。
在这个表格中，我们会发现，你所看到的都是最新的报道。例如，最近跃居首位的GPT模型被誉为"强大"，所有这些最新的模型和发展趋势都在这里一一呈现，可以清晰地在这个清单上看到，因此你没有必要花费大量时间去查询。通常来说，模型的参数越大，得到更多关注，被标注和下载的人数也越多的模型，其效果往往更好。这个模型现在有一个特殊的功能，即可以在线测试，直接测试这个模型。大家可以直接在这个界面进行测试，点击测试按钮即可。这就相当于在我们判断一个模型是否更能满足自己的需求时，我们可以先在这里进行测试。这是模型社区的优秀之处。在国内，阿里巴巴的模型社区被模仿，但因为国内的竞争非常激烈，阿里的模型社区没有得到其他大厂的认同。因此，国内的模型社区对我们来说意义不大，阿里的模型社区基本上都是自家的大模型，很少有新的模型，比如来自百度的大模型。所以我们只需要关注这个模型社区就足够了。国内的小公司也将模型授权在这里，因此，只需要看这一个模型社区就够了。

但是，对我们来说，除了那些专注于算法的同学关心模型之外，我们绝大多数的同学其实更关心的是数据集。杨教授基本上将大量的数据集都下载了下来。数据集解决了什么问题？例如，在数学中，我们需要看一下这个数据集解决了哪些问题？有了提示词之后，我们就能想到，大模型最能提高我们的生产力就在于数据集的部分。我会在后面讲述为什么数据集如此重要。因为作为知识的生产者，我们自己都积累了大量的领域数据。但是如果这些数据按照什么样的格式，什么类型的级别进行整理，你可能并不知道。但是当我们下载这些数据，并按照预定的格式，将其整合在一起，我们就可以迅速掌握其用法。

例如，在微调大模型的时候，我们需要准备大量的输入数据，但是这些输入数据如何编写才是最合理的呢？这些细节你可能并不清楚。但是只要我们有了有效的数据集，我们就可以根据这些数据来微调我们的大模型，从而提高我们的生产力。
然而，当你浏览这种大量人关注的数据集，你会迅速理解他们的编写方式。我希望大家能明白我的意思。例如，我主要下载了几个数据集，我是否之前跟大家提过这一点？我当时把这个网站的所有数据集全都下载，随后还有其他关联数据也一并下载，最后将这些数据集合并在一起，形成了一个包含二百五十万条提示词的大数据集。在去重后，这个数据集依然拥有二百五十万条数据，这就是我们最终构建了Outapp框架基础。我记得是在三月份，由于主要是在观看电视，现在的数据集比这个更多，我还没有完全看完这些数据集。比如某个数据集，我还没来得及看，感觉好像是刚上传的。看看这类数据集，你就知道这个提示词就是它了。所以这对我们的工作是非常有帮助的。你需要能够判断哪些提示写得好，哪些提示写得不好，这是我们每个人都需要拥有的判断能力。

好的，大家理解我现在说的吗？那时我还讲说，下载这个数据集其实非常简单，非常简常，就像下面这样。你看，下载数据集和使用get help的方法完全一样，我来给大家演示一下怎么下载这份数据的，它和get help的使用方法是一模一样的。你看这个网址，其中一个就是get，您只需点击这个按钮，就能将这个数据集下载下来，所以这就是它的逻辑，这个逻辑其实是完全一样的。好了，很简单，我们就不等它慢慢下载了，因为可能需要相应的权限。所以我们大概已经明白了。

好，那么我们接着讨论回到这个主题，返回到这个领域，其实这部分相对简单，你可以将其理解为在大模型领域，最好的开源组织就可以了，它比我们臆测的要好得多。可以说全球最好的公司，最好的组织，基本上都在这个领域。你会看到，这个就是我们刚刚讨论过的，做的最好的，就是做图片的这几个。我们再来看看这几个，这个是当前信息的主流。所以我们一查看就清楚了，大家就会明白，这是做的最好的AI公司，最好的商业实践。其实没有必要耗费太多时间，因为他们的项目就都集中在这个领域。好了，我们继续向前发展。
首先，这个网站具有一项核心功能：它的博客。至于我的头像，由于某种原因，当我在十个月前的19点尝试更改时却无法实现。在此，我特别提醒大家注意这一点，那些大型公司，如这三家巨头，他们的博客写得都非常棒。他们已经为我们解决了大量技术问题，我们一定要把这些技术文档仔细查阅。这些公司解决了我们的大量问题，并提供了更加简洁的实现某些任务的方法，例如如何做脱敏处理，如何进行模型评估等问题，他们实际上已经帮助我们解决了许多问题，我们没必要从零开始。这是我特别想提醒大家的。

现在，接下来我们再看看“最佳生态”的问题。当前，Open AI的发展速度非常快。除了Open AI，他们还在努力建设自己的生态系统，但是他们的生态系统建设得并不是很好，可以说他们一直在追赶竞争对手。我预计再过三个月他们在生态构建方面才会有所起色。现在，大量的开发者并没有使用Open AI提供的服务。在生态搭建方面，开源社区做得更好。

当前，生态态势的变化有三个典型趋势：一个是我们刚才讲的可能是围绕应用的相关领域；第二个是把Open AI的API能力迁移到本地；第三个是在军事、数据安全这些重要领域出现了完全本地化、保障隐私的大型模型。这三个项目可以提高应用性，我们后续会反复讨论。针对前两个趋势的讨论，即Open AI的API能力，其中最核心的就是代码解释器。代码解释器可以处理比较长的文本和更多的数据。然而，Open AI的代码解释器存在一个问题，就是无法和本地的工作流程，以及我们的项目相应地进行配合。
杨老师极度推荐的是这个项目，它近期是最受瞩目的一个项目。仔细看看这个项目，你会发现其涨势迅猛，近两三个月更是崭露头角。这个项目实则相对简单，一旦我们安装好之后，就获得了一种本地化的能力。你可以清晰地看到，它具备这种本地能力。

项目默认使用的是GPT-4模型，当然，我们也可以选择使用其他大型模型，本地化的能力依然保持。项目的应用方式还有许多，如果你对此感兴趣，可以继续进行深度研究。我强烈推荐这个项目，因为它的理念甚至比许多代理项目还要正确。

在这里，我要指出一个问题，就是过多的代理项目实际上将大量能力封装进了黑箱项目，这类操作对于我们人类来说相当无法控制。我们不知道这个代理最终会带来何种后果，对于我们知识工作者的生产习惯来说，其关联关系并不大。

然而，就像代码解释器，这种能力可能是最强大的，没有任何能力可以超越代码解释器的这种能力。它能够将大量本地工作流整合到一起。所以，这个项目受到广泛关注的原因也在这里，对此大家应该已经非常清楚了。

好的，接下来，我要为大家介绍第二个项目。像我刚才提到的，这两个项目都需要满足本地隐私法或者军事安全法律，所以是没有办法将大量的文本上传到GPT的服务器的。这两个项目都是在解决这个问题。

第二个项目是以facebook的开源项目为基础的，是第二代开源项目。这个项目的支持度要更高，其发展速度也更快。我在这里要特别提醒大家，这些项目其实都在解决同一个问题：如何在本地，如苹果笔记本上，运行一个本地化的模型。
在这次讲座中，我们主要讨论了在进行机器学习和深度学习开发时，如何利用开源项目和工具，通过充分发挥其现有功能，以创新的方式解决实际问题。

假设我们正在利用本地数据资源，例如自己的博客或自己创建的语料库，进行一次自我表达的实践。在这个环境中，我们有多种方式来完成前端的展示。对此，我们可以与之前讲述的几个项目相结合。我们这次的开源界之旅，实际上是一个创新的过程。我们所做的，并非从零开始，也并非全然开发新的项目，而是利用现有的开源项目，通过巧妙地组合与搭配，解决各种不同的问题。

例如，我们可以将四散在各处的开源项目汇集起来，通过前端技术进行有机的组合。其中一个项目，是一个开源电视中的模块，这个模块的主要功能是进一步增强本地AI的效果。此外，还有类似Facebook的项目，也可以借助其功能，实现更为逼真的接口效果。还有一个非常关键的项目，其主要功能是解决授权问题。

授权问题是很重要的一类问题。在AP的功能介绍中，我已经向大家提到过。其主要是为了确保我们的工作流程更为安全和有效。这就好像是我们所构建的一个最佳工作模型。在这个模型中，我们将一系列的项目和技术有机地组合在一起，形成了一套功能齐全，重视用户隐私的开发模型。

在这套模型中，有些项目的功能是解决如军事法务、政府工作等需遵守严格保密政策的文档不允许上传到互联网的问题。因此，这套模型能够在不需要与互联网进行任何交互的情况下，在局域网环境中完整地运行一整套的工作流，其中包括AI处理、代理服务器、授权管理、前端展示等等。

你们应该已经听懂了，这就是我们所推荐的，最佳的工作流程和生态模型。其实，与其在其它地方搜寻解决方案，不如将精力集中在这套工作流程上，因为这才是最重要的部分。

现在的程序开发界被大量新项目所涌现，这些项目或许充满了活力，但也充满了挑战。因此，经过大约6个月的摸索与实践，我们已经对开源界有了更为深入的了解。在过程中，有许多项目因为各种原因而被此次潮流所淘汰。或许，最初的开发者将项目遗弃；或者，项目本身设计不够成熟，无法满足市场需求；而有些项目，则由于它们逻辑简单明了，代码质量高，用户界面精致，以及配备了完整的技术文档，因此能够在竞争中脱颖而出。

对此，我想强调的一点是，成功的项目并不一定来自于知名公司或著名人士，而更多的是依赖于实际效果和应用价值。希望大家明白这一点。
在前几个月，我们一直致力于某个特定项目，如今，其发展格局几乎已经明朗化，从三月十五日以来，已经过去了半年的时间。我们对这些项目的理解和认识已经相当深刻，至少对于最理想的项目组合已经有了明确的把握。

要谈论这些项目，不得不提到"Lock AI"，这并不只是一个特定的大语言模型，而是一个平台，它可以对接各种不同的大语言模型，甚至包括那些由中国提供的如"汉字谱"等模型，或者是由斯坦福调适的模型等等。不过要注意，这种模型的运行必须依赖于本地设施，因此不同的设置可能会带来不同的效果。

在这段时间，我想和大家分享的内容，显然会与六个月之前有所不同，可能会更详尽，也可能观点会有所不同，假如我讲话速度过快，那么请随时提醒，我可适当放慢。

下面，我想对一个关键问题进行探讨，那就是关于计价工作流的问题，怎样才能构建出一个最佳的工作流程呢？

首先，我想强调的是"提示词工程"。我们必须明确，无论是Open AI的官方团队，还是我们的团队，对大语言模型的潜力挖掘都还不够充分。不同的提示词，可能会引导出完全不同的结果。所以，我们第一步，就应该着手解决明白提示词工程，为特定的需求进行充分的测试。

我们不能一开始就困扰于如何选择最强的GP4模型，或者是乱七八糟的大模型。相反，我们的出发点应该是，如何根据特定需求调试出最理想的提示词。然后再考虑如何在其他大模型上产生优异的效果。

因此，我们一定要把重点放在"提示词工程"上，并要明白，即使是Open AI的开发者，也没有唯一正确的写作方式。实际上，提示词的拟定，甚至可以看作一种随机组合。同样的一段表达，稍作修改，可能就会得到完全不同的结果和质量。这便是我在这部分特别要强调对大家的提醒。

尽管如此，今天的分享，我并不打算过多的讲述关于"提示词工程"的内容。

讲座中，主题围绕了GPT和我们的AI系统的提示词的相关影响，然而这并非是本次讲座的主旨。我们今天的焦点更多地放在如何将各个流程进行有效的组合。关于提示词，比如我们所使用的四下掐提示词，我可以在这里可以为大家做个演示。这个提示词是我们团队的成员在使用的，是四赛香火血型的分享。好的，接下来，当提示词满足了我们对某个设备的需求后，演示得十分精彩，我们会进行下一步操作。

那么接下来我们要做什么呢？在GPT的协助下，我们的第二步将会尝试去编写批量化的脚本。这个批量化脚本的作用就是去突破人机交互质量的局限。举例来说，我们知道ONI系统在处理庞大的电子书文件时，比如说一本十万字的书，会遇到一些问题。这种系统设定的局限，如果我们使用批量化脚本来处理，那么这将会非常合理且高效。

同样的情况也适用于大量的文档处理。假设我们有三百个文件，每个文件你都要上传给ONI代码提示器，然后让它帮你查找数据错误，这将会是非常耗时的过程。因此，使用这种批量化脚本进行处理会节省大量的时间。这点对于任何数据分析人员来说都非常清楚。
