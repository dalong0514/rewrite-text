接下来，让我来给大家介绍第二个模型，它名为“核心视觉语言模型”（Core VLM）。这个模型主要对标的是GPT-4。举个例子来说，当你向模型提示时，你可以这样描述：“下面这个菜是什么？”它会迅速响应：“这是麻婆豆腐。”顺带一提，麻婆豆腐是非常受欢迎的四川菜之一。此外，这个模型能够对图像进行深入理解。

该系统也推出了一个称为COGVRM的能力集，它以紫色边框表现出其能力的广泛性。这个集合可谓功不可没，可以形容为“八边形战士”，拥有多达13种强大的能力。例如，在一篇论文中，为了比较模型的性能，研究人员给出了同一张图片，并询问：“这张图上有多少棵树？有多少座房子？”然后Core VLM 回答：“有三座。”而如果向它提问，它可能会回答：“有四座。”这是因为它注意到图片中的一角，实际上背后藏着一座房子。尽管GPT-4没有蒙对这一点，但这并不是重点。我的个人观点是，这个例子不需要太过纠结。毕竟，那个角可能并不是房子，也有可能是其他物体，比如人的帽子等等。因为这是一幅卡通图，细节并不是完全清晰的。尽管如此，这表明模型对图像的理解能力可能超过了其他同类产品。

不仅如此，Core VLM还能解释为什么它会这样判断。如果只露出一角，它仍能判定出图片中有四座房子。此外，这个模型还能进行一些更为复杂的任务。比如，如果你给它一张图，并要求它描述场景，它能够准确地告诉你这是什么样的一个场景。基本上，它就像是在进行“看图说话”的活动，这项能力是非常强大的。

甚至，它能通过观察图像来完成作业任务。例如，如果你问它：“两块苹果加上一块苹果等于多少？”然后给它看一张图片，它能告诉你答案是三。对于养育过小孩的人来说，你会知道教小孩进行简单计算时，通常会用到这类有趣的图片。

Core VLM的能力并非止步于此。它也能对图像进行一些数字化处理，并告诉你衣服的坐标位置。这相当于说，它具备目标检测的功能，可以在所需区域准确标注目标。总的来说，这个多模态的能力展现了它的强大潜能。
在这场讲座中，我们探讨了具有多模态能力的一种机制。简而言之，多模态能力是一种非常强大的能力。通过观察该机制的效果，我们可以获得对其原理的初步了解。虽然这个原理并不像我们想象中那样复杂，但也不是毫无难度。

以图文匹配为例，我们可以发现一张图片与其描述成对出现。每一张图片对应着它的描述，形成了图文对。在这个过程中，文本这一侧的每个标记（token）可以被转换为一个向量。这些文本向量序列，通过VIT encoder——一种使用Transformer模型处理图像的专门模型来将图像也转换成向量序列。关于VIT模型的详细信息，我们之前在B站上有所讲解，如果大家感兴趣，可以联系助理老师通过微信获取视频链接。

在得到图像的向量序列后，我们通常会使用一个叫做LMP的处理步骤来实现向量序列的维度转换，从而使其与文本的向量序列保持一致。然后，将这两个向量序列拼接在一起，形成一个非常长的向量序列。

基于这个长序列，我们采用的处理方法非常巧妙。类似于GPT，它通过输入图像来预测文本。例如，输入一张图片后，系统将预测出A，随后连同图片和A一起用来预测下一个词——如Fox，然后再连同图片、A和Fox一起来预测下一个字母或词。与GPT模型一样，这是一个逐词预测的过程。

总而言之，输入包括文本序列和图像序列，在将文本与图像的序列串联起来之后，这些序列分别作为Q、K、V参与到多头注意力（multi-head attention）机制中。这个过程中，注意力权重实际上是在图像信息和文本信息之间进行加权融合。因此，综合后得到的向量序列既包含了图像信息，也包含了文本信息，实现了两者的有效结合。
在本次讲座中，我们主要讨论了Transformer架构在处理多模态任务中的应用，尤其是在图像和文本信息融合方面的创新。首先，让我们来回顾基础的Transformer模型，它基于自注意力（self-attention）机制构建，通过全联接前馈网络（Feedforward Neural Networks, FFN）进行信息处理。Transformer模型优秀的并行计算能力使其成为近年来自然语言处理领域的一个突破。

我们的研究中，通过对独立的文本和图像内容采用分开的Transformer模型进行处理，每个模型内部都包含了若干个自注意力层和FFN层。在此基础上，两者的输出通过特定的方式进行了融合。这种融合策略实际上是借鉴了Transformer Decoder的架构，并应用了一种技巧来合并不同源头的信息，即图像信息与文本信息的结合。

我们所讲的合并策略非常直观。简单来说，它通过将图像信息和文本信息的尾部进行串联，实现了信息的整合和预测。例如，当输入一张图片时，模型就会预测与之相应的文本描述。在准备训练样本时，我们采用了端到端的方法，即将一整串文字逐个进行预测。这样，输入模型的不仅是图像，模型还会逐字符地输出预测结果。

除了分析Transformer在图生文（从图像生成文本）任务中的应用，本次讲座还探讨了文生图（从文本生成图像）的模型。这类模型在艺术创作等领域应用广泛。例如，OpenAI和支普（可能是一个特定的研究机构或公司名字）分别推出了各自的模型，其中包括OpenAI出品的类似CoreWave的模型。

我们还介绍了OpenAI的模型背后的原理。举例来说，当输入一段描述“一只可爱的小猫”的文本时，模型会将此文本转换成一系列的token，进而变为向量序列。这与前面提到的clip模型有所不同。clip模型是直接将文本或图像转换成相应的序列，而在我们讨论的这个模型中，并不是这样的处理方式。这里的模型具有特点：首先对图像进行编码处理，将图像离散化为token。例如，它可以将图像分成4x4个区块，每个区块与一个token相对应。

讲座的内容包罗万象，详细讨论了Transformer模型及其在多模态任务中的应用实践。我们期待着这些先进的模型带来的技术革新，以及在未来应用中的无限可能。
在将语音演讲稿整理成书面语时，我们的目标是清晰、准确而且流畅地转述原讲话内容。以下是对提供的语音稿进行整理的文本：

首先，我们将图像分割为多个标记（token），并为每个标记寻找相应的向量，将其组合为一个系列的向量。接着，我们利用这些向量序列进行预测。例如，输入一张可爱小猫的头像，我们需要单独预测这幅图像中各个部分的标记。这个预测过程借鉴了JPG原理对图像标记进行推断。预测完毕之后，整个图像的所有部分都完成了预测，我们便可以通过解码器将这些图像标记重新组合，恢复出原始图像；这一步骤是通过decode的方式实现的。

这种方法选择不使用CLIP的原因在于，虽然CLIP能够产生高质量的向量序列，但是它并不适合逆向生成图像。而采用标记化的方法，我们可以较为容易地逆向重构出图像。这也是OpenAI采用此方案的理由。

在讲到质子谱的方案时，其实与OpenAI的方案大致相似。具体来说，在处理一张图像时，我们首先也需要进行标记化，这里使用的是BIT模型。进行标记化之后，我们开始预测图像序列。不过，质谱的方法有所不同，它允许预测过程中能够“见到”某些先前的元素。例如，在预测S5标记时，它能够引入之前标记的信息，这使得预测的视角与前述方法不同。在这里，视线是完全线性的，我们只能参考前面的标记来进行预测。

至于BID模型，它需要离线训练。假设我们有一个图像，总共被编码为812个部分。我们用一个编码模型（例如，4x4的区块）来识别图像中的每一块，并将这些区块与特定的编码对应起来。通过编码模型对图像进行编码之后，我们的目标是能够将这些编码恢复成原始图像。因此，我们会计算编码和原图之间的差异，即loss，并在编码然后解码的过程中最小化这个差异。当编码器训练完毕后，我们可以用它来对图像进行编码。然后，我们希望能够使用解码器从这些编码中恢复图像。这就是DIT模型所要完成的任务。它所需要的编码器和解码器都是进行了离线训练的。

通过上述转化，我们确保了讲座内容被详实、准确且符合中文书面语表达习惯的方式呈现。
经过精心训练的模型均是通过离线训练达到其高效能的。在当前的应用中，模型只是执行已被训练的功能而已。关于“文生图”（text-to-image）和“图生文”（image-to-text）的转换，我们已经做了大体的讲解。接下来，另一个引人注目的功能是编程代码的生成，这是一个强烈的刚性需求。这方面的模型确实能写出长达8192个字符的代码序列，并且在各大主流编程语言中都有显著的性能提升。

让我们从编程语言的使用比例来细看。Python大约占比26%，C++则为28%。目前，编写C和C++的人可能比较少，而Python和Java的使用者则相对较多。模型的工作机制其实非常简单：比如，在Python中，你写了一个如“for i in range(……)”的循环语句，模型就会将这句话视为一个语言序列，进而直接预测下一个可能出现的token。

这一机制其实是基于GPT架构的。换句话说，模型完全采用了GPT的结构。但在预测时，模型进行了一个创新——在某一位置添加了一个“捷径”，将当前位置与历史信息Dn连接起来，预测下一位置的N+1的token。此处我之前的表述有所纰漏：正确的是，模型在某一点引入一个变量，通过捷径连接，以预测随后的词。

这个新引入的自由变量是指，在一个序列中，比如GPT的序列结构中，token 1预测token 2，token 2预测token 3以此类推，而在模型code1的结构中，每一个位置都有特定的编码来参与预测。在预测token 2的时候，一个特定的编码被放置在那个位置，然后用来预测token 2。之后，位置编码会移动，为预测下一个token而放置。这样的结构意味着每个位置都有一个位置编码，这个编码帮助模型准确地进行预测。

当然，这涉及的是编程模型构建中非常精细的细节。简而言之，这类模型基本上采用的是GPT的架构，你可以把它看作是GPT的一个变种。这种模型还可以进行代码间的翻译，例如，你给它一个Java代码，模型就能够直接生成对应的其他语言版本。

总之，我们讨论了一个高效的模型在编程领域的应用，不仅仅覆盖了代码生成，在多种编程语言间的翻译上也展现了模型的强大能力。这个模型依据丰富的编码和创新的结构，提高了代码预测的准确性，极大地拓展了编程语言处理模型的应用范围。
在本次讲座中，我们讨论了如何将一种语言翻译成Python代码，这个过程实际上属于机器翻译。这与GPT（生成预训练变换器）的工作过程类似，在这里，我们将一个输入转换成一种不同的语言代码。今晚，我们对各种模型做了一个概览性的介绍，主要聚焦在GLM（通用语言模型）系列，概述了它们的功能和基本原理。由于详细解析每个模型需要大量时间，本次我们仅提供了粗略的轮廓。在后续的课程中，我们将逐一细致地探讨每个模型的详细信息。

我们讨论了GBT（梯度提升树）的一些特别之处，指出GBT并非开源，同时存在安全问题。在这方面，我们可以比较国产技术与外来技术，如比亚迪与特斯拉。国产技术可能并不逊色于外来技术，在开源和生态支持方面更表现出色，特别是对国产GPU的支持，我认为这将成为未来在实际应用中的强大优势。随着算法性能和效果的逐步提高，即便存在微小的差距，也不会对实际工作产生太大影响。重要的是对国产GPU的支持，它将是成功应用落地的关键因素。

此外，我们也提到了对“小型化部署”的支持。拥有1.5B和3B模型的小型化部署在很多场景中至关重要。如果没有这类部署能力，很多端设备如安防设备将不能使用，例如在无法联网或在隐私需要保护的情况下。小型化部署可以在这些场景中扮演重要角色，特别是对于1.5B和3B这样的模型，它们的价值和应用潜力是巨大的。

在今天的课程中，我们还提及了自己正在开发的一些课程，包括多模态课程大纲。这个大纲将涵盖各种多模态主题，例如图生文、文生图，以及步态扩散学习等领域的内容。

本次的内容概述到此，我们期待在后续课程中为您呈现更多精彩的讲解和深入的模型分析。
关于我们即将介绍的两个训练营的课程大纲，对于有兴趣深入了解的同学，可以通过添加注意老师的微信来获取更多信息。首先，我们有一个专注于多模态研究的训练营。在这里，我们的重点是结合视觉与其他模态的训练方法。另一个是专门用于训练语言处理大模型的训练营。这个训练营更侧重于文本与语言方面的研究。

需要提醒大家的是，要参与这两个课程，可能需要具备一些基础的记忆和学习能力。也许有些同学会担心，自己对于记忆学习一无所知，不知该如何是好。针对这种情况，我们准备了一门适合零基础学习者的入门课程。无论您是否具备编程知识，都可以放心报名参加这个课程。这是一门针对初学者设计的全面课程，报名后会包含之前提到的大模型和多模态内容，方便您系统地学习并掌握前沿技术。目前，这个课程的报价也非常有优惠。

另外，如果您对我们过去举办的公开课，包括今天的讲座或者video processing、PVT等话题感兴趣，不妨加上我们助理老师的微信。通过私信，我们会将相关资料和信息发给您。如果不清楚助理老师的微信号码，您可以浏览我的聊天记录，或者关注我的B站账号并查看私信，以获取微信号。之后，您只需添加此微信号，就可以获得所需资料。如果对我们的课程感兴趣，也可以通过这种方式联系我们的助理老师。

好的，今天的讲座就介绍到这里，感谢大家的聆听，现在我们的课程结束，同学们可以下课了。
