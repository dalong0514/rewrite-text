尊敬的听众，今天我将向您阐述一个主题：XGMVSXGBT。选择这一主题的原因在于我个人深信，在国内能与国际知名的OpenAI XGBT或Google这样的企业在人工智能领域一决高下的，仅有少数几家公司。在我看来，首当其冲的就是百度。百度在广告方面的效果固然显著，但更值得关注的是其扎实的技术实力。另一家值得一提的是专注于XGM技术的清华智普。智普在业界有着引人瞩目的特色：它所推出的产品几乎在各个方面都与OpenAI的产品形成了直接对标。这意味着，不管OpenAI推出了何种新技术，智普几乎都能够推出具有相似功能的产品。因此，智普或与之相关的JYM这类模型，在我看来，潜力巨大，并且非常值得我们持续关注。无论是研究大型模型还是其他相关技术，这都是非常值得我们投入时间去深入了解和学习的领域。

接着，我们来探讨当前主流的大型模型。它们主要分为三个流派：一是以BERT为代表的编码器（encoder）模型，二是以GPT系列为代表的解码器（decoder）模型，以及三是结合了编码器和解码器的模型。

BERT以及与之类似的模型采取了编码器架构。我们看到，截至2021年，通过这一技术路线获得的成果已经不再增长，原因是这条路线可能已经接近尽头，或者至少在短期内难以作出突破。这是因为适用场景有限，技术改进空间受到限制。

而另一线——以GPT-4为代表的仅包含解码器的模型，却显示出极强的活力。在这一路线上，不仅有GPT-4，还包括了XGPT等多个模型。

今天，我希望重点探讨的是采用了编码器-解码器结构的GLM模型。我们将聚焦于它的构造和性能，以及与其他模型的比较分析。实际上，这不仅仅是两家公司产品的比较，更是两种不同技术线的较量。虽然二者在技术架构上有所差别，这种差异可能没有大家想象的那样巨大，但事实上，差异确实存在。以OpenAI为例，它发布了GP3，也被称为“达芬奇”。之后则是迭代版本达芬奇1、达芬奇2，以及最新的InstructGPT，在去年的十一月份又带来了新的视角和能力。

在这场技术的长跑中，每个参与者都不断地推动技术的极限，追求卓越的表现。各家公司的发展路径虽不尽相同，但他们之间的激烈竞争，无疑推动了整个人工智能领域的进步。因此，无论是行业内的专业人士还是其他领域的观察者，我们都应当持续关注这些前沿技术的发展和它们之间的竞争。这不仅能帮助我们把握未来技术的脉络，也可能为我们带来前所未有的机遇。
自去年以来，接近一年的时间，我们见证了InstructGPT的问世。今天，我想与大家分享一下智能生成模型（即智普）的发展时间线。智普团队从2018年开始研发自己的大型模型，虽然与GPT-1相比晚了两年，但从那时起，它便开始快速发展。不少人可能好奇，这家公司为何能在技术起步上有所迟缓，却能迅速成长。事实上，智普公司最初是在2015年或2016年成立（具体年份我记不太清楚），当时他们专注于制作质谱图。是的，他们的起点是知识图谱，正因为如此，公司积累了大量的数据。这是因为构建知识图谱需要爬取大批数据，因此，他们手头的数据量非常可观。有了这样的数据作为支撑，公司便迎来了飞速的成长。

去年8月，智普实现了令人印象深刻的突破，他们发布了价值达130币的应用，并且推出了一个专为VS Code设计的插件，这大大方便了程序员编写代码。事实上，这些成就的背后，是来自清华大学知识工程实验室的团队成员们的辛勤工作。

接着，我们注意到智普推出了GLM-6B模型。我想知道，有多少人已经体验过这个模型。不久前，他们刚刚发布了3.0版本。我亲自尝试后发现，这个新版本确实进行了一些改进，使用起来十分顺畅。这标志着GLM团队在整个大模型领域的一次显著进步。

今天，我们的主要议题是对比不同的大型模型。我们将具体分析Decoder only模型和Encoder-Decoder模型之间的区别。其中的一个模型可以进行量化，也引入了快速Transformer技术，加速了处理速度。关于横杠标记的部分，并不是说这个模型不能量化，而是因为OpenAI公司逐渐闭合其资源，变得不那么透明。他们很少发布论文，也许多技术不再开源，所以我们拿不到更多的技术细节。

值得一提的是，清华大学开发的模型是完全开源的，对实际工作来说支持帮助很大。与此同时，JRM模型有一个显著优点，那就是它支持国产GPU。这对我们来说意味着更大的便捷性和灵活性。
在目前的情形下，国有企业普遍倡导使用国产GPU。实际上，尽管有人可能更倾向于使用进口产品，但如A100型号的显卡已经较难购买。我们现在能够买到的显卡型号大多是A10、A6000或者V100。甚至抢手的4090型号也限制销售，这种情况有些让人无法接受。考虑到这一点，支持国产GPU无疑是一大亮点，并且这将成为未来的重要发展方向。

在训练方法方面，经典的训练方法，例如大规模XGBoost，包括了四个阶段：预训练、大规模训练、奖励模型与强化学习。XGBoost能够顺畅地完成这四个步骤。相比之下，GELM虽然宣称完成了所有步骤，但某些后续环节存在疑问。经过分析发现，它可能并没有完全达到预期。而且，后两个阶段——奖励模型与强化学习——目前存在质疑，有观点认为这些部分可能并非必要，只是过度充足而已，并且执行这两步骤需要耗费大量的人力成本。如果将其视为选项，那么我们或许可以认为这是一个变数。

接下来，我们再来看一下G2M的产品线。据我之前和大家的介绍，G2M的产品线与XGBoost在很多方面是全面对标的。当前，我们将针对这些产品的特点，向大家详细解读GLM相关的内容，并进行整体概述。

关于我们的课程，它更注重理论方面，而不是只教授实际操作。市面上许多课程避开了理论教学，只是简单地指导如何使用特定的工具和技术。我认为，对于具备大学基础教育，英语水平达到四级的人来说，这些课程的价值并不大，因为他们可以通过自己的努力去理解和掌握这些知识。比起那些课程，我们的课程更注重深度和理论，可能自学起来比较困难，相关资料也不容易查找。但请放心，我们会继续探索这些难点。

我们继续关注G2M的发展。从3月14日XGM1.0发布开始，接着在6月发布了2.0版本，然后到了10月27日——也就是前两周——发布了更新。在这两次发布中，G2M推出了两个新模型。一个是多摩泰模型，另一个是专注于长篇文本与对话的智能体。其中，配备6位元和17位元的版本也随之推出。同时，引入的“agent”概念是G2M的一个亮点。我们首先需了解，从1.0到2.0的转变带来了哪些进化。首先，增强了对长序列支持的能力，这是一项重要的技术升级。
在整理这段讲座语音稿为书面语时，我会首先整理语法，使之更符合书面语的标准，同时保留所有细节和原文的意思。以下是根据您的要求修改后的文本：

首先我们讨论了持有8K序列的主题，并且提到了快速的transformer。这种快速的Transformer能够有效提升处理速度。我们通常认为，在大型模型领域中，Transformer相对平静，它是一个较为平稳的技术点。因此，通过对其进行一定优化，我们便可以在一定程度上加快其运算速度。

我们还观察了其不同版本的模型，它们分别对应着不同的计算币数，包括6币、12币、32币、66币至130币不等。例如，一家名为360的公司，推出了TALK 130B模型。大家可能对TALK 130B这个名称比较熟悉，这是中国在大型模型领域内的一项杰作。

中国的大型模型可以被大致分为三大派系。首先是清华的GM（大模型）团队，其次是百度，第三则是由多家公司共同参与的。在这些公司中，拉玛地区的套客占据了极大部分。你们可能会提出一个问题，为什么国内的开发者较少使用这样的套客方法。原因是很简单的，主要是因为他们担心法律诉讼的风险。以拉玛为例，简单来说，就是他们赌Facebook不会跨越国界来起诉他们。而GM3模型，与GM2相比，其测试分数在各个榜单上均显著提高。

至于阿里巴巴，我们可以认为它也采用了套客的方法，虽然这只是我的个人推测。我记得我在B站上谈到过套客，而我并未明说是谁。结果发现很多人猜测是训飞，并且有人甚至联系训飞的法务部门说要起诉我。这件事我觉得挺有意思的。我曾经说过训飞的法务部门给我发送了律师函，但最终，我等了两周也没有收到。

在选择大型模型时，我希望同学们不要过于迷信某些榜单。并不是所有的榜单都具备绝对的权威，有时候这些榜单与实际应用场景存在明显的差异。有时，一些机构会针对榜单的标准对自己的模型进行特定优化，这并不意味着该模型在所有情况下都同样优秀。以某川为例，他们就是采用了这样的优化策略。

对于这些大型模型的一些特点，我们可以概览如下：这些模型的优势之一是它们基本都带有免费商业授权。需要注意的是，目前3B和1.5B这两个版本尚未开源。我们也无法获得这两个版本，这主要是因为它们用途较为特殊。它们体积较小，主要用于在智能手机端进行部署。华为的模型则被称为盘古，具有其独特的应用场景和功能。

通过这次整理，我们对大型模型领域的纷繁格局有了清晰的了解，同时也提醒大家在实践中要注重模型的实际应用价值，而非仅仅追求榜单上的排名。
在讨论到盘古大模型的时候，我们不得不承认它的开源程度并不高，普通用户难以接触和使用。这是因为盘古更多地服务于内部的各个产品线，而其生态系统相对封闭。看似小巧的盘古，让人不易与之深入接触。我的亲身体验也是如此，我未能直接使用它，故不宜妄加评价。

为了技术的分享和交流，接下来讲述的是1.5B和3B模型。这两种模型在性能上接近6B的水准，意味着它们可以用更少的参数量，即一半甚至四分之一，达到相近的效果。特别值得一提的是，这些模型支持国产芯片，并可以应用于笔记本、手机、汽车等多种设备上，此外，还能在CPU上无损效果地进行推理。这显示出它们在计算效率上的优势。

在当前的发展态势中，如果依然只是单纯追求模型效果的提升似乎已经到了瓶颈期，不易有显著的进展。因此，现在越来越多的研究开始专注于模型在各种现实场景中的实际应用，以及垂直领域的具体效果。JRM3在这方面做出了贡献，并进行了相关领域的研究。

说到minimax，我必须坦白，我没有使用过这个模型，因此不便作出评价。但我可以分享我亲自参与过的工作。

继续讨论，G2M模型因其庞大的训练数据、充分的训练步骤和合理的训练策略而显得非常强大。这三个因素，经常被人提及，似乎已经成为一种通用的表述方式。从携带码和知识角度来看，G2M模型也有显著的提升。接下来，讲述的内容会包括agent功能，这将逐一展示。

另外，G2M模型的另一个优势在于其全面的开源策略。它开放的内容更多，且基本上允许免费商业使用。这对于那些从事学术研究或者个人兴趣开发的人来说是十分有利的。我已下载了3.0版本，并会在之后的演示中向大家呈现。有兴趣深入了解的学员可扫描助理讲师的微信二维码，下载资源。

3.0版本的特色在于它不再是一个纯粹的语言模型。它通过调用各种工具，能够处理包括输入提示词在内的多种操作。然后，向模型提出问题，并得到模型的帮助和答复。
在探讨现代技术如何辅助我们获取信息的领域中，存在一些高效的工具，这些工具不仅能优化我们的信息检索过程，还能提高我们对信息的理解与应用能力。我有幸了解并使用过一些这样的工具，在此与大家分享。

首先，股票价格查询工具是我们常见的应用。它可以快速准确地提供股票市场的即时数据。例如，当我们想要知道特定股票在特定日期的收盘价格时，如11月6日某股票的收盘价是23.16元，这个工具能够迅速给出答案。

此外，文本转语音工具亦是一项重要的技术成果，尤其是应用了大模型的解决方案的这类工具。它不仅能将文本信息转化为语音，而且能理解并回应用户的查询。也就是说，在你向大模型提出需求后，例如查询股票价格，它能自动识别你的需求并调用外部的股票接口来返回查询结果。而这一过程并非单一的语言输出，它真正涵盖了与外部接口的实际通信和数据检索步骤。

来看一个场景应用，当我们对大模型说：“帮我查今天北京的天气怎么样”，并向它提示我们可用的工具——一个能获取实时天气信息的程序时，大模型首先需要了解工具的特征。这就需要我们明确告诉模型，我们所用的工具不仅能获得当前位置的天气，还能提供气温单位，是摄氏度还是华氏度。在此基础上，模型运行后便能给出答案。

接下来，有必要解释一下“Agent”的概念。简而言之，“Agent”是在大模型外围附加了一系列工具，使得大模型与这些工具联动，共同形成了一个能进行更多复杂交互的系统。“大模型+工具=Agent”——这个公式既简单又直观。当提及Agent时，可以想象大模型作为大脑，而工具则是它的手脚，共同作用以完成更复杂、具体的任务。

此外，还有一次我在B站上进行的有关Agent的直播，当时的内容覆盖了Agent的详细解释与应用。如果你有兴趣，可以通过加我微信的方式来获取这段直播的视频链接，以便更深入了解。

最后，我们不得不提的是G2M这套模型及其强大的功能——借助搜索引擎来提供相关答案，并给出引用来源，使得信息查询不仅有据可查，还能确保信息的准确性和可靠性。这意味着即使我们询问大模型某个如“今天的新闻有哪些”这样的问题，它不仅能理解我们的查询意图，还能通过相关的数据源给出实质性答案。

总之，随着科技的进步，这些工具不断地提升我们与信息交互的效率，让我们迅速获取、理解以及应用知识。这些技术正以前所未有的速度改变我们获取和处理信息的方式。
在这次讲座中，我们探讨了一个实际案例，即某人在训练完自己的模型之后，对特定日期11月16日的具体事件一无所知。我们来看看，模型已经上线运行了一段时间，那么当问及它11月16号发生的事件，它将如何作答。这个过程其实涉及三个步骤，我会重点跟大家分享每一步的含义。

首先，第一步相对简单。当我们向模型提问时，它会首先调用搜索引擎，这里以谷歌搜索为例，去获得相关的信息。得到的结果是若干个网页。由于网页内容较长，模型进一步将这些内容切分成几个片段。

接着，在第二步中，模型运用一个称为“contraver”的模块计算问题内容与这些文本片段之间的相似度。它会对所有段落进行打分，并选出得分较高的，这些段落作为引用或参考内容。这里，如果搜索引擎足够精准强大，这一步骤即使省略也不会有太大问题。

然后，在第三步，即‘提示学习’阶段，模型不仅简单地提取这些参考信息，还会在多个参考之间做比较和提炼。模型会将这些信息当作一个案例进行分析，反复学习，并结合实际问题进行模拟。比方说，如果我给出参考文献一和参考文献二，同时定义我的问题和预期答案，模型会在此基础上进行引用和整合，向大模型呈现需要解决的问题。此举其实是在训练模型，让它学会如何找到真实世界问题的准确答案。

最后，模型将所有这些参考资料整理、汇总，并根据模型对这些资料的处理和解析，给出一个最终答案。这个汇总步骤是必要的，因为面对一个问题，可能存在多角度的新闻报道或信息。模型需要将这些信息综合起来，以提供全面而准确的回答。

以上，就是整个过程的详细说明。我们通过这个案例，可以看到智能模型如何通过搜索、分析和学习，处理和回答现实世界中的问题。这一系列的步骤不仅展示了人工智能处理知识和信息的能力，也体现了我们如何能够让技术更好地服务于实际需求。
在讲述现代计算模型的构建与优化过程中，我认为各位应该注意到其复杂性与系统性。首先，我想强调的是，对计算模型的建构，既可以从多角度进行细节描述，也可以从一个整体角度来概括。在收集并整合了丰富的数据信息之后，我们得到的可能是一个非常大型且功能强大的模型。然而，这个模型未必能完美符合我们人类的需求和偏好。那么，面对这种情形，我们应当如何处理呢？

接下来，我会解释一个统计合理偏好的方法——评分体系。简言之，这个评分模型首先会通过收集高质量的用户反馈来定义有效数据。例如，任何一个获得三次或以上点赞的回答，都将被认定为一个有效的反馈答案。借助这些筛选出来的、高度有效的答案，我们能够更好地理解用户喜好。再如，假设我们有三个不同的答案，分别是A1、A2和A3。每个答案都会根据其获得的点赞数进行排名，从而形成一个相对的优先级顺序，比如A1大于A2，A2大于A3，以及A1大于A3。随后，我们利用这个打分模型来指导大型模型的学习，在模型中对各个答案打分，从而使其分数反映出我们人类的偏好。然而，这种打分操作可能会产生巨大差异。

也就是说，我们的目标是让大型模型在对A1的打分上尽可能高，而对于A2则尽可能低，这样模仿出人类评价的不同立场。这里的要点是，这种打分的依据来源于实际的人类标注。

很好，让我们把目光转向这个流程的总结。尽管整个过程看起来可能有些复杂，但其实其核心逻辑还是相当直接的。首先，它经历了网络搜索答案的过程，并通过筛选精炼了信息，这一步骤主要是提高搜寻答案的关联度。其次，运用大型模型将搜索结果整合汇总并进行输出。此处可能会有多个复合的结果。第三步则是运用评分模型计算每个结果的得分，并最终返回得分最高的答案。这样一来，我们的模型就能尽可能贴合人类的认知模式。

作为讲解的延伸，这个模型十分类似与一个我们所熟知的概念——Lanchain，但其实施要更为复杂。综述了整个WebGLM的策略，我们可以看到其基于一个高效的思路引擎，从而能够补充和丰富知识。此外，当谈到WebGLM与WebGPT的对比时，我们了解到一个仅有10B参数的模型便能够达到另一个有175B参数模型的效果，这无疑展示了它的强大能力。虽然与人类的水平相比，还存在一定的差距，但是仅凭10B的参数就能接近或者超越175B参数的表现，确实令人印象深刻。
