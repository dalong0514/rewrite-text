### 直译

随着数字媒体格式的演进，无损音频编码格式 FLAC 和图像格式 JPEG 已成为行业标杆，它们的优化对于存储效率和传输速率都是至关重要的。例如，FLAC （Free Lossless Audio Codec）能够在不损失音质的情况下减小音频文件大小，而 JPEG （Joint Photographic Experts Group）则以其高效压缩机制在图片质量和文件大小之间取得了良好的平衡。这些技术的发展受益于多年的研究和持续的技术革新，源自不断发展的算法和强大的数据处理能力，如今已被广泛集成到从我们的智能手机到专业媒体编辑套件中。

在这一背景下，公司如 Microsoft、Amazon 和 OpenAI 正在投入资源进行深度学习和 AI 技术的研究，希望通过开创性的算法来改进这些现有的标准。例如，通过大量实验和迭代，OpenAI 最近展示了他们的 Transformer 模型在音频和图像编码方面的突破，这些成果很可能位于即将发布的论文 [20] 中。

社交媒体上，行业领袖们经常分享他们对技术进展的见解，就像 Sam Altman（@sama）和 Satya Nadella（@satyanadella）这样的人物。通过他们的 Tweets，我们不仅能够获得即时的行业动态，还能洞察到他们对未来技术发展趋势的预期。

**Figure 1: **某项关键技术的发展轨迹分析图

**Table 1: **各企业AI研发投资回报率比较

### 意译

在数字世界里，我们经常提到 FLAC 和 JPEG 这样的格式。FLAC （全称 Free Lossless Audio Codec，自由无损音频编码）让音乐爱好者无需牺牲音质即可享受较小文件体积的音乐，而 JPEG （全称 Joint Photographic Experts Group，联合图像专家组）则帮助我们在保证图像质量的同时减小图片的存储空间。这些格式背后，是多年科研成果的结晶以及不断进步的技术革新，现在它们已经成为我们手机和专业媒体工具中不可或缺的部分。

在此背景之下，像 Microsoft、Amazon 和 OpenAI 这样的科技巨头们也在推动人工智能技术的突破。OpenAI 最近甚至利用他们的 Transformer 模型，在音频和图片编码上取得了革命性的进展，而这样的研究成果有望在不久的将来发布的论文 [20] 中呈现给世人。

而在社交媒体上，技术界的佼佼者如 Sam Altman（@sama）和 Satya Nadella（@satyanadella）也会积极分享他们对于科技潮流的洞察。通过他们的推文，我们不仅可以第一时间了解行业的最新动态，还能预见他们对技术未来发展的展望。

**图 1: **展示了一个关键技术如何一步步走向成熟的分析图

**表 1: **列出了各大公司在 AI 研究方面的投资与回报的对比
### 直译
{
### 05. 神经网络

那么，我们通常用于识别图像等任务的模型是如何工作的呢？目前使用最广泛且最成功的方式是采用神经网络。神经网络在20世纪40年代发明，其形态与今天的运用惊人地接近。神经网络可以被认为是对大脑工作方式的简单理想化。

在人类大脑中，大约有1000亿个神经元（神经细胞），每一个都能产生电脉冲，可能达到每秒一千次。神经元互相以复杂的网络连接，每个神经元具有树枝状分支，让它能向数千个其他神经元传递电信号。大致来说，任何特定神经元在特定时刻是否产生电脉冲，取决于它从其他神经元接收到的脉冲——不同连接以不同的"权重"贡献。

当我们"看到一个图像"的时候，实际发生的是图像中的光子落在我们眼睛后面的("光感受")细胞上时，它们在神经细胞中产生电信号。这些神经细胞与其他神经细胞相连，最终信号会通过一系列神经元的层次。在这一过程中，我们"识别"了图像，并最终形成了"看到了一个2"的思想（并且可能最终会像大声说出"二"这个词一样做些事情）。

前一节中的"黑箱"函数是这样一个神经网络的"数学化"版本。它恰好有11层（尽管只有4个"核心层"）：

这个神经网络并没有特别的"理论推导"；它只是在1998年时，作为一个工程项目构建起来，并发现有效。当然，这与我们描述我们的大脑通过生物进化过程产生的方式没有太大区别。

好的，但是这样的神经网络是如何"识别事物"的呢？关键在于吸引子的概念。假设我们有手写的数字1和2的图像：

我们想要所有的1都"被吸引到一个地方"，所有的2都"被吸引到另一个地方"。或者换句话说，如果一个图像在某种程度上"更接近于1"而不是2，我们希望它最终到达"1的地方"，反之亦然。

为了简单类比，假设我们在平面上有特定位置，用点表示（在现实中，它们可能是咖啡店的位置）。然后我们可能想象，从平面上的任何一点出发，我们总是想最终到达最近的点（即，我们总是去最近的咖啡店）。我们可以通过在平面上划分区域（"吸引子盆地"）并由理想化的"分水岭"分隔来表示这一点：

我们可以认为这实现了一种"识别任务"，其中我们不是在做类似于识别给定图像"看起来最像什么数字"的事——而是我们很直接地了解到，一个给定点最接近哪个点。（这里展示的"Voronoi图"设置分离了2D欧几里得空间中的点；数字识别任务可以被认为是在做类似的事情——但在由每个图像中所有像素的灰度值构成的784维空间中。）

那么我们如何让一个神经网络"执行识别任务"呢？考虑如下非常简单的情况：

我们的目标是获得一个输入，对应一个位置{x, y}——然后"识别"它为最接近的三个点中的哪一个。或者换句话说，我们想要神经网络计算一个关于{x, y}的函数，如下：

那么我们如何用神经网络来做到这一点呢？归根结底，神经网络是一个连接的理想化"神经元"的集合——通常以层的形式排列——一个简单的例子是：

每个"神经元"实际上都是设置来评估一个简单的数值函数。要"使用"网络，我们只需要在顶部输入数字（例如我们的坐标x和y），然后让每层的神经元"评估它们的函数"并通过网络向前传递结果——最终在底部产生最终结果：

在传统（生物启发式）设置中，每个神经元实际上都有一定数量的"输入连接"来自上一层的神经元，每个连接都被分配了某个"权重"（可以是正数或负数）。给定神经元的值是通过乘以"前神经元"的值以它们相应的权重，然后将这些加起来并加上常数——最后施加一个"阈值化"（或"激活"）函数。用数学术语来说，如果一个神经元有输入x = {x1, x2 ...}，那么我们计算f[w . x + b]，其中权重w和常数b通常对网络中的每个神经元选择不同；函数f通常是相同的。

计算w . x + b只是矩阵乘法和加法的问题。"激活函数"f引入了非线性（最终导致非平凡的行为）。常用的激活函数有多种；这里我们只使用斜坡(或ReLU)：

对于每一个我们希望神经网络执行的任务（或者，等效地，每一个我们希望它评估的整体函数），我们会有不同的权重选择。（而且，正如我们后面将讨论的，这些权重通常是通过"训练"神经网络从我们想要的输出示例中进行机器学习而得到的。）

归根结底，每个神经网络只对应某个整体数学函数——尽管把它写出来可能很乱。对于上面的例子，它会是：

ChatGPT的神经网络也只是对应像这样的数学函数——但实际上拥有数十亿个项。

但让我们回到单个神经元。下面是有两个输入（代表坐标x和y）的一个神经元可以使用各种不同的权重和常数（并使用斜坡作为激活函数）计算的一些函数示例：

但那更大网络上面的怎么样呢？好吧，这是它计算的内容：

它不完全"正确"，但接近我们上面展示的"最近点"函数。

我们来看看其他的神经网络发生了什么。在每种情况下，正如我们稍后将解释的，我们使用机器学习来找到最佳的权重选择。然后在这里我们展示带有这些权重的神经网络计算了什么：

更大的网络通常更擅长于逼近我们所追求的函数。而且在"各自吸引子盆地的中央"，我们通常能得到我们想要的确切答案。但是在边界——神经网络"很难做出决定"的地方——事情可能会更混乱。

在这种简单的数学风格的"识别任务"中，"正确答案"是很明显的。但是在识别手写数字的问题中，答案并不那么清晰。如果有人写的"2"很糟糕，看起来像一个"7"，等等怎么办？不过，我们还是可以问一个神经网络是如何区分数字的——这给我们提供了一个线索：

我们能否"从数学上"说出网络是如何做出区分的？实际上不能。它只是"做神经网络该做的事"。但事实证明，这通常似乎与我们人类所做的区分相当吻合。

我们来看一个更复杂的例子。假设我们有一些猫和狗的图像。而我们有一个被训练来区分它们的神经网络。以下是它在一些例子中可能的做法：

现在更不清楚什么是"正确答案"了。如果是穿猫衣服的狗呢？等等。无论给予什么样的输入，神经网络都会生成一个答案，并且在某种程度上与人类的看法相对一致。正如我在上面说的，这不是我们能够"从第一原理推导出"的。它只是经验性地被发现为真，至少在某些领域。但这是神经网络有用的关键原因：它们以某种"类似于人"的方式来做事。

给自己看一张猫的图片，并问自己"那为什么是猫？"。也许你会开始说"嗯，我看到它有尖耳朵等。"但是不太容易解释你是如何识别出这张图片是猫的。这只是你的大脑不知怎的算出来的。但是对于大脑来说，没有办法（至少现在还没有）"深入内部"去了解它是如何算出来的。那么对于（人工的）神经网络呢？好吧，很容易看到每个"神经元"在出现猫的图片时都做了什么。但即便要进行基本可视化通常也是非常困难的。

最终我们用于上面"最近点"问题的网络中有17个神经元。用于识别手写数字的网络中有2190个。而在用于识别猫和狗的网络中有60,650个。通常，要可视化相当于60,650维空间的内容是非常困难的。但因为这是一个设置来处理图像的网络，它的许多神经元层组织成阵列，就像它正在查看的像素阵列一样。

如果我们拿一个典型的猫的图像

猫

那么我们可以用一系列衍生图像来表示第一层的神经元状态——其中许多我们可以很容易地解释为类似"没有背景的猫"或"猫的轮廓"这样的事物：

到了第10层，很难解释发生了什么：

但总体上我们可能会说，神经网络正在"挑选出某些特征"（也许尖耳朵就在其中），并使用这些来确定图像是什么。但这些特征是否是我们有名称的——像"尖耳朵"这样？大多数情况下不是。

我们的大脑是否使用相似的特征？大多数情况下我们不知道。但值得注意的是，类似我们在这里展示的网络的前几层似乎会挑选出图像的某些方面（例如对象的边缘），这些方面似乎与我们知道的大脑中视觉处理的第一层挑选出的类似。

但假设我们想要一个关于神经网络中的"猫识别理论"。我们可以说："看，这个特定的网络就是这样做的"——立即就给了我们一些关于"这个问题有多困难"的感觉（例如，可能需要多少神经元或层）。但至少就目前而言，我们没有办法"给出叙述性描述"关于网络在做什么。也许那是因为它真的是计算上不可约简的，并且没有一般性的方法来找出它在做什么，除了通过显式追踪每一步。又或者，我们只是还没有"搞清楚科学"，并且识别出"自然法则"来概括正在发生的事情。

当我们谈到用ChatGPT生成语言时，我们将遇到同样的问题。而且同样不清楚是否有办法"概括其所做的事情"。但是语言（和我们对它的经验）的丰富性和细节可能会让我们比图像领域走得更远。

### 06. 机器学习和神经网络的训练

我们到目前为止讨论的都是"已经知道"如何执行特定任务的神经网络。但使神经网络如此有用的（大脑里也可能是如此）是，它们不仅可以在原理上执行各种任务，而且还可以通过"从示例中逐步训练"来执行这些任务。

当我们制作一个神经网络来区分猫和狗时，我们不必实际编写一个程序来（例如）显式地找到胡须；相反，我们只需要展示大量的猫和狗的例子，然后让网络从这些例子中"机器学习"如何区分它们。

重点在于训练有素的网络能够从它所展示的特定例子中"泛化"。正如我们在上面看到的，并不仅仅是网络识别了曾经展示过的一个猫的图片特定的像素模式；相反，神经网络不知怎么地设法基于我们认为的某种"通用的猫性"来区分图像。

那么神经网络训练究竟是如何工作的呢？本质上，我们始终尝试寻找使得神经网络成功复现我们给定示例的权重。然后我们依靠神经网络以"合理的"方式来"插值"（或"泛化"）"在这些示例之间"。

让我们来看一个比上面最近点问题还要简单的问题。我们只是尝试训练一个神经网络来学习函数：

对于这个任务，我们将需要一个具有一个输入和一个输出的网络，如：

但该使用怎样的权重等呢？拥有任何可能的权重集，神经网络都会计算某个函数。例如，这是它用几组随机选择的权重计
