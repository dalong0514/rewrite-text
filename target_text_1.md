在微调模型实战及经验分享的讲座中，我们详细探讨了构建大型模型的三个核心要素：算法、数据和算力。今天的分享重点围绕这三个领域展开，我将向各位报告我们在每个领域的实践经验以及未来的规划。如果您也有兴趣构建自己的大模型，那么这三个要素是您不得不考虑的关键点。

首先，我们来看一下初音大模型的整体架构。在这个架构图中，我们从左侧开始，可以看到训练数据包括了结构化文档问答队列以及非结构化文档。结构化文档主要涵盖了我们常见的问答格式数据，而非结构化文档则是包含大量领域知识描述的文本，比如规章制度和各类通知等。

通过构建一个专业的领域知识库，并利用Lanchan和Pomod这些工程技术，我们将模型量化，最终推动模型的在线部署。在线部署过程中，我们特意加入了一套自主研发的内容审核系统，确保输出的内容是合规且合法的。

除了利用Lanchan技术之外，我们还对模型进行了深入的训练。这一部分的训练数据被分为两类：一类是非结构化文档数据，用于训练基础模型，比如GLM类型的模型；另一类则是结构化问答数据，用来训练具备对话能力的XGLM类型模型。

在模型训练方案的选择上，我们实验了多种方法，包括Lara自适应器、强化学习以及部分层的微调。经过一系列的尝试与比较，我们发现Lara自适应器是最为理想的选择，因为它不仅学习成本低，而且易于上手。

在模型选型方面，尽管市面上存在众多不同的模型，我们最终确定采用的是XJ2模型。我们发现，模型的参数量并非越大越好，这是我们在实践中遇到的一个难题。事实上，有时参数量过大反而会降低模型效果。

为了优化线上服务，我们对模型进行了量化处理。量化模型的目的是最大限度地提高机器资源的利用率，从而降低运营成本。这就是我们初音大模型整体系统架构的概述。

接下来，我想分享的是XGBT如此火爆的原因。实际上，其他大型模型如BERT、T5和BAT等也算是大模型，它们的参数量同样不小。然而，这类模型往往只在自然语言处理（NLP）领域引起轰动，外界了解得并不多。其主要原因是这些模型的学习和应用成本非常高。首先，您需要具备编程技能，还需要了解NLP和机器学习，这些都是使用这些模型的前提。而提示工程技术的出现，相当于大幅度降低了这些成本，它通过引入提示，极大地降低了使用难度。
作为一位作家，我在整理此次讲座的语音稿并将其转化为正式书面语时，将严格遵守以下标准：首先，要确保保留所有的细节；其次，力求保留原文的语义和语感；再次，修改所有错别字和病句，符合中文语法规范；最后，尽可能详尽地呈现内容，不留下任何遗漏。

在本次讲座中，我们探讨的是一种示词方法，并且介绍了如何利用这种方法让模型完成各种任务。我相信在座的各位都有过类似的实践经验。你会发现它的学习成本是相当低的。这就像以往要开手动挡汽车一样，我们需要对汽车的内部构造等等有相对深入的了解。而如果使用“示词方法”，比如Promoter，就仿佛我们换成了一辆特斯拉，我们几乎不用关心太多的细节，一踩油门，汽车就能带给我们非常流畅的驾驶体验。然而，即便大模型的Promoter起步简单，它自身也存在一些劣势。其一，是上限有限。若不对其进行训练，使用它所能取得的效果是非常有限的，且若要达到更佳效果，难度会显著增加。其次，它存在模型适配问题。这是指当你更换模型时，可能必须重新适配提示词，从而相对减弱了其通用性。

还有两个劣势，是从更宏观的角度来考虑的。例如，对于初创公司来说，如果仅依赖于这种技术，投资者可能会认为这项技术较为肤浅，缺乏深度。此外，基础研究人员可能认为Promoter不够深入，更多的是易用性，而不是技术难度。

在此，我举了三个具体的例子。第一个例子中，我们可以提供一个句子的开始部分，模型就可以自动完成后面的内容。我在其中加入一个提示词，“描述天气情况”，模型进而理解我的语义，并据此描述天气。Case 2进一步展示了当我提及“上海的晴天”，模型如何根据这一信息进行描述。而Case 3其实是对"上海的晴天"的再次描述，进一步丰富了信息。

通过这三个案例，我们可以看到今年大模型的一个重大变化：我们不先讨论其输出是对是错，关键在于它的答复不会跑题，这是大模型今年的一大突破。相比以往容易出现答非所问的问题，现在的模型至少能够正确理解我们提出的问题。

我们的提示工程，如果应用于自然语言处理（NLP）任务，事实上就是覆盖了文本摘要、信息抽取、问答、分类、对话、代码生成、推理等七大NLP任务。我们可以利用提示工程来指导大模型进行回答。

此外，我们还可以玩转一些高级的提示词。例如，如果我对模型提出：“巴黎是法国的首都，那里有哪些著名景点？”接着问起纽约的景点，模型在回答了问题之后，当我提出北京时，它将类推出“北京是中国的首都，并列出了北京的几个著名景点”。这显示了模型不仅学习了给出的信息，而且掌握了它们之间的关联。同样的，我们也可以利用模型处理一些逻辑问题。通过这些例子，我们能够看到提示工程在操作大模型时的强大能力和潜在价值。
在本次讲座中，我们初步涉及了一个有趣的问题：如果煮一个鸡蛋需要两分钟，那么煮五个鸡蛋需要多长时间？这个问题其实考验了我们对自洽性的认识，也就是逻辑的一致性和排除自我矛盾。例如，如果我说我六岁了，而我的妹妹只有我的一半大，那么我们可以通过逻辑推理得出，我的妹妹今年应该三岁。这样的逻辑推理实际上涉及到的是一种极为高级的认知能力。

再来谈谈我们在传统的L2P（Learning to Predict）领域，即学习去预测要处理的这些问题类型，实际上有许多功能是难以想象的。如今，我们所拥有的大型模型已经能够完成这样复杂的任务，这在传统意义上是非常难得的。我们最近上线的初心大模型同样具备了类似的处理能力。如果大家对此感兴趣，可以去了解和探索我们这些大模型。

通过探讨，我们可以了解到，无论大模型如何变化，它们的核心往往都基于同一个基础架构——即transformer架构。然而，我们可能会注意到市面上存在许多不同的大型模型。这些模型之所以多样，是因为它们在几个关键方面进行了不同的配置和优化，主要是在结构、位置编码（position encoding）、激活函数（activation function）以及层归一化（Layer Normalization）方法等四个方面。我们现今能够接触到的这些主流大型模型，它们所采用的这些技术实际上并不新颖，这些技术基本上都是在2000年左右提出的，直到2020年左右才开始广泛应用，并且通过各种创新的组合拼凑，衍生出多样化的模型。

值得注意的是，并不是每一种技术组合都代表了最优解。有时候，如果我们将某个基础函数替换为另外一种，可能会带来性能上的提升。然而，当前大型模型面临的一个重大问题是训练成本极高，因此做一次实验的成本也随之增加。由于高昂的成本，许多可能的优化尚未被探索。我们通常只能根据最初的实验组合，一旦发现模型效果不错，就会发布出来，但仍然有许多值得优化和提升的地方。

在此，我还想提及中国学者在这一领域的贡献。旋转字编码（Rotary Position Encoding）便是由中国学者苏建林所创造，他是一位90后的年轻科研人员，这表明中国人在这一领域拥有很高的参与度。

至于大模型的不同架构，我们大致可以分为以下几种：第一种是Encode-Decode架构，典型的如BERT模型和T5模型；第二种是单向的语言模型架构，例如GPT和LAM这类模型；第三种则是Prefix LM模型，典型代表是我们所推崇的GLM类模型。目前，Encode-Decoder架构相对使用较少，因为达到同等效果时所需的参数量几乎是翻倍的。因此，现在主要是后两种架构被广泛使用。
我们在竞争的激烈市场中，目前GPT模型暂时处于领先地位，但谁将最终占得优势，真的很难预料。当我们考虑扩大模型规模时，其实与以往传统模型优化过程有所不同。例如，在优化BAT或者T5等早期模型时，我们通常会调整模型的具体参数。然而，对于大型模型，我们通常不会采取同样的微调策略。

在整理相关经验时，我总结出了四个核心点。首先，大型模型的参数数量是巨大的，以至于现在几十亿个参数已经是基本水平，而这样的参数规模对内存的要求很高，就拿我们使用的A100 80GB显存而言，往往都可以将其完全充满。

第二个点是数据需求量的增加。参数越多，理所当然需要更多的数据来进行训练。

第三是模型的收敛问题。由于参数众多，使得训练时间大幅延长，有时一次训练可能需要两三个月，而训练结束后的具体效果也是未知数。因此，投入到大型模型的风险非常高。

第四，模型中存在大量超参数。调整这些超参数之间的组合，比如学习率、批量大小等，都是非常耗时的工作。

鉴于这些原因，大型模型并不常见于传统意义上的微调。为此，出现了一些新的技术，主要包括Perfects tuning和Promote tuning。在这些方法中，我们不再使用具体的单词作为提示，而是对应于一组向量，直接调整这些向量即可。

另一个被采用的方法是适配器（Adapter）。这种方法其实是在Transformer结构中添加了附加模块，比如PK和PV。加入这些模块后，我们就只需学习这些小组件，这样整体模型的可学习参数数量就会显著减少。

第三种技术叫做Lara。它也是通过添加外挂部分来实现，但与适配器不同的是，如果适配器是通过串联方式加入外挂，Lara则是以并联方式加入外挂。在进行各种方法的实验时，我们尝试了这三种技术，并最终发现Lara的效果最为显著，它不仅易于上手，而且训练过程相对较快。

然而，使用Lara时有一个超参数需要特别关注，那就是rank的设置问题。基于我的经验，如果你的训练数据领域是垂直的，例如公司的专有数据，适当增大rank的值可能会更为合适。
在本次的讲座中，我们共同探讨了如何构建和训练大型运维模型。首先，我想强调的一点是，如果你所使用的训练数据相对通用，例如我们所开发的大型运维模型所包含的，很多是客观的普适性知识，并非仅限于我们公司内部的专有信息。在这种情况下，我们对数据质量的排名标准（rank）可以设定得相对较低。

接下来，我想与大家分享一些构建大型模型时的实用方法。事实上，要构建这样的模型，我们必须处理大量的数据。我们将这些数据分成了两大类。第一类是网络数据，即那些通过各种爬虫程序抓取的网页内容。这类数据在数量上很可观，但质量参差不齐，特别是在中文环境中，网络数据的质量实在不敢恭维，需要进行大量的清洗工作。就像OpenAI能够开发出优秀模型，很大程度上是因为得到了微软搜索引擎提供的海量网页数据。国内的百度、360搜狗等搜索引擎，对百川等机构同样能够开发出大型模型提供了支持，因为它们源自于大量的网页数据资源。

与此同时，还有第二种数据是专有数据，这类数据质量很高，比如公司内部的公文、技术文档等。专有数据的一个显著优点在于它们的质量非常高，但问题在于它们整理起来成本较高，并且数量有限，这在一定程度上会影响到大型模型的最终效果。这是众多从业者关注的一个问题，稍后我还会进一步详细探讨数据量与大型模型之间的关联。

在我们对模型所需要的数据进行分类时，我们可以识别出两种数据。一种是非结构化的纯文本数据，这类数据可以是简单的文字段落，适合训练像GLM、GBT这样的语言理解模型。然而，如果要训练对话型模型，我们就需要结构化的问答（QA）数据。幸好，我们的团队通过Lanchain技术部分解决了这个问题，使得我们在训练对话模型时可以使用非结构化的文本数据。目前，我们已经打通了这条技术路径，并正在进行相关项目的开发工作。一旦项目完成，我们将会与大家分选内容，并提供试用机会。

最后，我想展示一下我们在训练Bloom模型时语言覆盖的情况。可以看出，模型很大程度上覆盖了欧洲的语言，主要是印欧语系，例如英语、法语、西班牙语等为主要部分，而中文在其中所占的比例相对较少。此外，还包括了各种小语种和一些编程语言，如Java、PHP、C++等。通过这样的分析，我们可以更清晰地了解到多语种模型背后的数据组成。
在流行的模型分类中，我们关注的SOCO似乎并没被涵盖。这一点值得我们思考。原因何在？为何SOCO这种常见的语言模式未在分类中出现？PARM的大模型则有所不同，数据源的分类方式不是基于语言而是基于类别，涵盖了从艺术娱乐到游戏，再到新闻等不同领域。模型的这种全面性带给我们一种启示—我们所用于训练的数据应尽可能全面，并且包含尽可能多的不同语言。这样，我们训练出的通用型模型才能取得最优效果。然而，对于我们公司而言，如果只是针对一个垂直领域进行研究，语言的多样性问题就显得不那么重要了，因为我们主要关注中文。在这种情况下，我们需要做的是深入覆盖该领域的各个方面。如果我们训练的数据没有覆盖到特定的领域，则大模型在该领域的表现可能就会不尽人意。这告诉我们一个道理：模型所能达到的效果，很大程度上依赖于我们提供给它的数据类型。

我在这儿为大家总结了一些可以自行下载尝试的大型模型数据集。有兴趣的话可以下载下来自己操作看看。这些数据集都是公开可下载的，其中有不少是中文数据集。这些数据质量都相对较高。我特别推荐第四个数据集，无道的数据集，其质量较好。

此外，我想分享一个大模型领域我认为最重要的规律，即“密率定律”。这个定律描述了模型大小、数据量、训练强度增加时，模型性能会相应提高，但这种提升并非线性关系，而是密率关系。就像数字一样，当我拥有大量数据时，测试集的损失函数会迅速下降，损失函数越小意味着模型效果越好。起初，模型保持高速提升，但随着数据量继续增加，性价比可能降低，表现为密率关系。实际上，我觉得这不仅仅是大模型的特点，更像是一种社会自然规律。我们知道“二八法则”——大模型解决的问题中，大约有20%非常困难，而80%则相对简单。在训练的最开始，模型可以迅速解决那80%比较简单的问题，随后解决剩下20%的问题可能要付出更大的努力和代价。这就形成了一种密率的关联。
在此次讲座中，我们讨论了一个非常关键且有趣的概念：模型效能与相关因素之间的密切关系。具体来说，我们将聚焦于模型的计算量、计算强度、数据规模以及模型大小本身，它们之间构成了一种复杂的连接。

首先分享给大家的这张图，对于我个人而言，它极具启发意义。图表的纵轴代表了测试级的损失函数，即可以将其理解为指标值越低，模型的性能越优。横轴则展示了模型的参数量，亦即模型大小维度。这张图清晰地展示了一个现象：随着模型参数量的增加，损失函数指数通常会逐渐降低。以图中的线条为例，我们可以观察到，模型从较小的比如说10亿参数增长到较大的容量，其性能也会随之增强，损失函数会减小。

然而，一个有趣的发现是，当数据量变得庞大时，即使是相同参数量的模型，性能也会显著提升。举例而言，当我使用22亿条数据，与仅用21兆数据进行模型训练的效果相比较时，我们会发现，较大的数据量会让模型效用显著提升。不过，这样的关系终究是呈现出一种密率的特性。在实际业务中，常有同仁询问究竟需要多少数据才足够。对此，我能给出的建议通常是“越多越好”，因为事实上数据量与模型性能的关系确实是呈正比的。尽管如此，数据量增长到一定的程度后，获得的性能提升会开始减缓，性价比可能不再那么显著。但即使是如此，目前各业务部门能提供的数据量通常还远未触及到这一瓶颈。因此，我鼓励大家毫无保留地为我们提供数据。

进一步分析左图，我们可以看到，横坐标依然是代表测试级损失函数，而纵坐标则反映的是处理的token数量，这可以被看作是数据量的一个体现。数据量与模型性能同样存在着密切的联系。随着数据量的增加，模型的损失函数会逐渐降低，最终趋于平稳。然而，如图中的紫色线所示，当参数量较少时，损失函数降至一定程度就不再下降，这被称为收敛。由此可见，在一定范围内，选择较大模型是有益的。

最后，让我们来看看右图，它阐释了为什么模型不能无限制地越大越好。事实上，大模型除了会增加计算成本，还会带来过拟合等风险，因此我们需要在模型规模和数据量之间找到一个平衡点。综上所述，这两张图为我们提供了数据量、模型大小与模型性能之间的深刻洞见，对于我们的优化和决策大有帮助。
在此次讲座中，我们重点讨论了计算强度，即GPU的算力水平以及它所扮演的角色。我们观察到，对于较小的模型（以紫色标注），其损失函数的下降速度实际上是快于较大模型（以黄色标注）的。例如，当我们的计算资源仅限于特定水平时，可以发现选用小模型比起大模型更能有效地降低损失函数。尽管小模型的性能上限较低，但下降速度则相对更迅速。因此，在计算资源有限的情况下，选择小模型会更合理。

我们先前也遇到过一个相关的问题：使用价值130币的模型得到的效果，并不如一个只值6币的模型。这一发现可以归结于当前的算力无法达到运行大模型所要求的水平。事实上，多数情况下，当人们处理任务的时候，他们的算力往往达不到运行大模型所需的标准。因而，在选择模型时，我们应该量力而行。基于我们的经验，通常选择价值6到10币的模型已经足以满足正常的业务需求，选择更大的模型则没有必要，反而会造成资源的浪费——无论是机器、金钱还是电力。

接下来，我想讨论一个人们关心的话题：tokenization。在处理大型模型时，我们通常使用子词（subword）分词方法，而不是单词或单字符方法。对于英文而言，我们通常通过空格实现自然分割，而对于中文，我们可以采用结巴等分词手段。在大型模型中，更常见的是子词分词方法，如Byte Pair Encoding (BPE)、WordPiece和N-gram等。这些方法的核心原理是将经常共同出现的词或字符组合在一起视作一个词。例如，在中文中，“葡萄”、“枇杷”和“鸳鸯”这样的词汇一般不单独出现，所以它们通常被处理为一个子词。

谷歌开源的SentencePiece是一个分词器，如果我们想要实现自己的子词分词方法，可以使用这个工具来达成目标。实践证明，理论上WordPiece的效果比BPE好，但由于其计算量较大，在实际应用中，BPE被更广泛地使用，例如在GPT系列模型中就大量采用了BPE方法。
在我们进行自然语言处理时，Tokenization是常用的技术之一。我们会选择一种模型来处理文本，例如PE（Position Encoding）模型，该模型会将文本分解为更小的单位，我们称之为子词（subword）。在这一过程中，我们可以观察到一个有趣的现象。比如当使用XGLM模型时，我们会注意到在中文文本中的某个词有时候可能被分解成两个token，有时候则是单个token。这反映出子词分割并没有严格遵循"一个或两个token对应一个词"的固定规则，而是取决于具体的切割结果。这就是为何我们无法简单地将token和词之间建立一一对应的关系。

我们继续观察一些大型的语言模型及其词汇表的规模、性能等方面。可以看到这些模型的词汇量通常极大，对中文长文本处理能力强的模型，比如GBT和Lama，它们在处理长文本时具有明显的优势。这些模型的性能以及与性能相关的诸多因素，对于对此感兴趣的同学而言，可以进一步查阅相关的图表和论文来获取更深入的理解。

当我们训练这些大型模型时，我们通常会采用多GPU卡并行训练的策略。为什么需要这样做呢? 以我们公司使用的A100显卡为例，虽然它拥有80G的显存，足以支持装载6B参数的模型，但对于130B或者更大规模的模型，如讯飞的百B级别模型，单卡根本无法装下。因此，我们采取了多种并行策略来充分利用多个GPU卡。这些并行策略包括：

数据并行：这种策略主要用于训练阶段，简单来说，如果有100条数据需要处理，我们可以将其分配给四台机器，每台机器处理25条数据。处理完毕后，再将四台机器的训练结果合并，整体进行调整，这就是数据并行的典型应用。

模型并行：对于大型模型，尤其是单张卡装不下的情况，模型并行就显得尤为重要。此时，我们会将模型分解成若干部分，比如四部分，每个部分对应一张GPU卡。计算过程从第一张卡开始，完成前面层的计算后再传输到下一张卡继续计算，依次进行，这样就可以处理超出单卡显存限制的大型模型了。

以上是关于自然语言处理中tokenization的相关介绍，以及大型模型的训练策略和它们的特点。希望能对听众有所帮助。如有兴趣深入了解，欢迎进一步研究和探索。
在本次讲座中，我们讨论了多种高效的并行计算技术及其在深度学习模型训练过程当中的应用与意义。首先，介绍的是数据并行和模型并行的概念。假设我们的训练数据需要在四个GPU卡之间进行处理，训练开始时，数据会首先传递到第一个GPU，随后传到第二个，然后再依次传到第三个和第四个。当所有数据在第四张卡上处理完毕后，会进入反向传播阶段，这时，每张GPU卡会更新自己处理数据片段的参数。如此，四张卡协同工作，共同维护了整个模型。在模型训练的“变形”过程中，模型是被分布在四张GPU卡上，这样，当数据在这些卡之间流动时，每张卡都持有模型的完整数据。这就解释了数据并行与模型并行的某些相似之处。

进一步地，我们讨论了张量并行。众所周知，深度学习中的基础运算实质上都是矩阵操作。考虑到这一点，我们甚至可以对这些矩阵操作进行并行化处理。例如，若有两个矩阵需要相乘，我们可以让一个GPU卡负责乘以第一列和第二列，而另一个GPU卡则处理剩下的部分。这样的并行化能大大提升运算速度。然而，这样的张量并行不仅仅只存在于不同卡之间的并行，它也会在单个GPU的SM（Streaming Multiprocessors）内部进行。关于GPU的工作原理，我会在稍后的部分与大家进行分享。

此外，当我们在实施多头注意力（multi-head attention）计算时，每个"头"是完全独立的，因此我们可以使用不同的计算单元来处理不同的"头"，无需担忧某种固定的计算顺序。由此，我们的数据并行、模型并行以及张量并行实际上可以同时进行。通过这样的策略，我们可以最大化整体资源的利用率。

然而需要注意的是，这种高效的并行计算对通信带宽的要求极为严格。我们需要确保使用如NVLink等高速通信设备和优化数据中心的网络环境，以提供足够的通信带宽。否则，传输数据所耗费的时间可能过长，导致最终的计算效果并不理想。

最后，我们探讨了在完成模型训练之后，如何进行模型的压缩和加速操作。在深度学习领域中，整体上有一套被认为是加速的四件套，包括减脂（pruning）、低脂（slimming）、蒸馏（distillation）和量化（quantization）。其中，'减脂' 主要是通过移除一些权重较小的参数来实现模型的简化，而'低脂' 则是通过数学手段减少矩阵的规模。至于'蒸馏'，这是一个让小模型去学习大模型的过程。这三种方式在传统的自然语言处理（NLP）领域较为常见，但在大规模模型中更多采用的其实是模型量化这一技术。

通过这些技术的运用，我们能够使模型变得更为精简而高效，以适应不同的应用场景和性能要求。在高性能计算以及人工智能不断进步的今天，这些技术的实践和研究，无疑是优化计算资源，推动科技发展的关键步骤。
关于模型量化，其实这是一个非常简单的概念。例如，在以往，我构建的不定点树（应为浮点数结构），可能具有很大的数值范围，由于其占用的显存和比特率相对较高。接着，我会采取一种方式，强制使用一个int8类型的数据来表示它，即使用一个低精度的数据格式来表征一个高精度的数据。这个过程就是量化，即将一个浮点数（记作R）转换成一个整数（记作Q）。而反量化，正如其名，就是将整数再恢复为原先的浮点数。

在这一转换过程中，大家可能注意到了一个四舍五入的操作，因此这一步实际上会导致所谓的量化误差。比如，一个数值原本是1.25，经过量化再反量化后，返回的数据可能变成了1.20，即精度损失了一些。虽然对于推理来说，这样的精度损失并不特别重要，但对于训练过程而言，影响则较大。

针对量化技术，我们实际上有三种不同的方法。第一种，即Post-Training Dynamic Quantization（简称PDTQ），这种方法是直接对一个已经训练好的模型进行量化。这种方法的优势在于它非常简单，且运算量小。但其弊端在于，它只能量化模型参数本身，不能量化激活函数输出的结果。这是因为在模型未接收输入之前，我们无法知晓输入是什么，因此也就没办法对激活函数输出的数值进行有效且合理的量化，只能对模型权重进行量化。

第二种方法，会用到一些测试数据进行量化。其过程中，首先用FP32格式（全精度浮点数）跑一些测试数据，其好处就在于可以将激活函数输出的结果也量化。因此，从理论上讲，第二种方法的量化效果会更好，因为它能量化更多的内容。

第三种方式，则是Quantization-Aware Training（QAT）。这种方法是在训练模型的时候就直接进行量化，将量化的过程直接嵌入到模型学习中。实际上，这种方法是效果最佳的，但其成本也最为昂贵。

目前，我们正在应用第一种方法对模型进行量化。虽然我们也在探索第二种和第三种方法，但到目前为止还没有得到非常满意的结果。因此，我们目前依然采用第一种方法。这样一来，我们就可以提高GPU显存的利用率，并且整数运算的速度相比浮点数要快很多，各方面的推理速度也相应得到了提升。
在本次讲座中，我们首先了解到显存在处理整数运算时的速度远超于浮点数运算，这种优势使得各项计算推理的速度得到显著提升。接下来，我想和大家分享的是NVIDIA公司各个时期的产品架构，即我们熟知的GPU。

值得一提的是，在这里展示的架构并不包括我们日常用于游戏的GPU。我个人购买了一块A6000显卡，它在游戏性能上带来了非常流畅的体验，甚至可以说秒杀了RTX 4090等其他显卡，对于游戏性能来说，简直就是维度的粉碎性升级。

我们可以注意到NVIDIA公司给它的GPU架构命名时，倾向于选用历代著名科学家的姓名。例如，'赫伯'（Hopper）这个名字来源于美国女科学家，她在火箭和军事领域有着显著的成就。尽管在美国有一定的知名度，但在全球范围内，它可能并不如其他科学家那样出名。然而，GPU架构的命名表明了NVIDIA的野心：因为像牛顿、爱因斯坦、高斯这样的大名鼎鼎之人还未在产品中出现，这似乎是在暗示该公司对未来还有着无限的信心和巨大的升级潜力。这点与比亚迪将车系命名为‘秦’、‘探’、‘宋’类似，采用知名个体的命名方法。

穿越时代的产品中，帕斯卡（Pascal）架构值得一提。它带来了诸如NVLink这样的技术突破，使得带宽大幅度提升，较之前的PCIe有了长足的进步。此技术允许多张显卡进行联合训练，极大地提升了处理能力。实际上，从2016年开始，随深度学习的不断深入，大模型开始孵化，NVLink也不断得到优化。

在随后推出的伏打（Volta）架构中，V100显卡的使用也标志着技术上的一大进步：NVLink 2.0出现，速度进一步提升。此外，更重要的创新是Tensor Core的引入。Tensor Core专门用于矩阵乘法的计算，从而精准定位深度学习的需求。其矩阵乘法速度比前代技术快了16倍，这无疑是Tensor Core诞生的重要成就。而最新的A100显卡在结构优化方面取得了进一步的进展，尤其在处理稀疏矩阵方面效率提高，这在计算机视觉（CV）领域可能不那么明显，但在自然语言处理（NLP）领域，对于稀疏矩阵的优化则显得尤为重要。

综上所述，从显存处理速度的优势到NVIDIA显卡架构的命名和技术创新，这些全面而深入的信息展现了GPU技术在不断进化中的众多亮点。
关于一款名为Hover H100的高性能产品，我们了解到其在常规销售渠道中可能无法轻易购得。该产品经过更新迭代，除了提升了Tensor Core也升级了Nalink。另一个值得一提但在介绍图示中未明确的特性是，它对于Transformer架构有特别的性能优化。正因如此，Hover H100在处理Transformer架构时有着出色的处理速度，其设计初衷便是为运算大型模型而生，而对于日常娱乐如游戏则显冗余。

我们的公司有采用名为A100的模型。在A100的基础上，还演变出了其他变种，如轻量级的A10，适用于推理任务，还有A6000，它将NVLink的连接方式简化为了PCIE接口。这些版本相比之下属于更低端的选择。我公司主要还是使用如A100这类高端显卡，你们仅需要了解这些产品即可。

A100显卡的架构基础是由所谓的”SM”计算单元组成，一个A100含有108个SM单元。每个SM内部都包含了四个主要计算单元，其中最重要的是Tensor Core。此外，还有专门用于整数（Int）、浮点（FP）、以及32位和64位运算的计算单元。A100利用不同的计算单元来执行各种精度的运算任务。各个SM共享一段L1缓存，而每个SM小组则共享一个L0缓存。这种显存的架构使我们在开发清华大学的XRM2.0时，能够在运算速度上取得显著提升，这也是为什么它的推理速度如此之快。利用A100的硬件结构，尝试让计算尽量在单个运算单元内完成，减少跨SM操作，有效降低了数据传输时间，从而提高了整体的计算速度。

接下来介绍，A系列显卡现已支持BF16数据格式，然而在旧型号V100中尚不支持。因此，对于那些采用了BF16格式的大型模型，它们只能在A系列显卡上运行，如A10和A6000。如果要在V系列显卡上运行，则可能需要调整数据的精度。

A100的技术架构中的亮点之一是Tensor Core。Tensor Core主要针对矩阵运算的优化设计，它能在特定的矩阵计算任务上提供巨大的性能优势。
在矩阵运算方面，这些运算基本上都可以被分解成乘法和加法。矩阵运算通常以并行的方式进行，典型的运算步骤包括一行元素与一列元素相乘，再加上一个数值。这些计算步骤被转化为并行形式，从而提高了运算的效率。

矩阵乘法中的混合精度运算是指在运算过程中，相乘的两个矩阵采用较低的精度（如FP16），而相加操作使用较高的精度（如FP32），使得最终结果也拥有高精度。这种做法的优点是可以避免在模型训练时因精度不够而造成梯度下降过程中梯度消失的问题。举个例子，在微调模型时，数值较小的梯度若精度不足，可能会被四舍五入导致无法正常收敛。为解决这一问题，混合精度运算能够保留这些微小的梯度值。

有人可能会提出疑问:为什么不直接使用FP32，而要使用混合精度运算呢? 其实，直接使用FP32的话，会导致显存占用量翻倍，对显存造成较大消耗。因此，在资源有限的情况下，混合精度运算是一种权衡和折中的方法。

另外，在A100等支持张量核心(Tensor Core)的硬件平台上，针对矩阵运算的加速方法已经得到了优化。例如，如果原矩阵中存在许多数值接近于零的元素，那么可以先将这些数值置零，并将矩阵稀疏化处理。在这种情况下，计算时只需存储非零数值及其在原矩阵中的坐标位置，从而省略掉所有“零乘以数值”的无效运算，能够显著提升计算效率。特别是在处理自然语言处理（NLP）任务时，张量核心的这种优化极其有用，因为NLP中的矩阵往往是稀疏的，而这一点在计算机视觉（CV）领域不太常见，因为CV中的矩阵通常都是密集型的。
在探讨大数据和机器学习的应用领域中，我们发现在一些特定场景下数据稀疏性的问题格外突出，比如在推荐系统和搜索引擎中，若运用大型模型，其效果往往能够较快地得到显著提升。正因为在这些领域中，数据的矩阵非常稀疏，所以相应模型和算法的优化显得尤为关键。

今天，我想与大家分享的是关于NVLink和NVSwitch这两项先进技术。谈及这些技术之前，我们不得不提及GPU之间通信的历史。在没有NVLink之前，假如需要在两张GPU间进行通信，我们必须依赖PCIE作为传输介质。PCIE的带宽较低，有时甚至需要经过CPU的中转，导致整体通信效率低下。此时，就有了NVLink的出现。揭开NVLink的面纱，我们发现它并非英伟达原创，而是起源于IBM。事实上，早在英伟达与IBM合作的'蜜月期'，NVLink的概念就已由IBM首先提出。值得一提的是，如今提及NVLink，多数人会直接将其与英伟达联系起来，这也算是一个行业内的小趣闻。

技术的迭代从不停步，英伟达在2016年推出了基于帕斯卡架构的首代NVLink。这一技术能够实现四卡相连，每张卡可与其他四张卡建立连接。随后，随着技术的发展，NVSwitch问世了。NVSwitch可以类比于我们熟悉的网络路由器，一张GPU通过NVSwitch可以连接更多的通道。这些线路可能非常密集，具体的数量在此不便一一列举，有兴趣的朋友可以之后向我索取相关资料。通过NVSwitch的运用，卡与卡之间的通信不再局限于单一线路，大量线路的共同作用使得通信速度得以大幅提升。

说到英伟达的A100，其在NVSwitch的技术支持下，通信能力进一步增强。有趣的是，英伟达在公布其带宽时，采用的是双向计算方式，因此虽然单向只有1.2TB，但最终的数据会展示为2.4TB。而带宽之所以能够从75GB提升至150GB，也正是由于线路数量的增加。当我们迎来H100时，由于单线路速度的发展，带宽从2.4TB升至3.6TB。

因此，H100在训练如GBT这类超大型模型时，展现出更加明显的优势。这些优势主要体现在两方面：一方面，H100对诸如Transformer这样的架构进行了深入的优化；另一方面，在多机训练和并行通信能力方面，H100的性能尤为突出。这些先进技术的应用，无疑将推动机器学习和人工智能技术在各个领域的发展和应用。
关于这个特定地方——遗憾的是，如今在我们国内是无法购得的。我今天的分享就是围绕这一主题进行的。
