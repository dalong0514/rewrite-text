作为作家，我担负着将一次讲座的口述内容转换成正式、规范的书面文本的重任。在这一过程中，我会坚持做到以下几点：维持原文的全部细节，尽可能保留语义和语感，修正任何错别字和语法错误，并尽量详尽地阐述内容。以下，是我对上述讲座稿的整理：

各位观众朋友，我是林毅。今天我要和大家分享的话题，着实让人感到既惊叹又兴奋。想象一下，一台新款的苹果电脑竟能与八张NVIDIA的RTX 4090显卡媲美。当我得知今年发布的M2 Ultra处理器支持高达惊人的192GB内存时，我想必众人心中一定感到好奇，这究竟代表着什么呢？

或许一开始你们并不会觉得这一数字有何异乎寻常。但请允许我添上这样一条信息：苹果芯片采用了统一内存架构，意味着这192GB的内存同样可以作为192GB的显存。那么请问，是否意味着通常需要八张RTX 4090显卡才能处理的巨型人工智能模型，如今在M2 Ultra这单一的芯片上，就能够轻松运行了呢？正是在2020年，苹果正式结束了与x86架构的合作，奠定了这一技术奇葩萌芽的基础，而它的盛放，则离不开开源社区众多赛博菩萨般高手的共同努力。

请坐好，仔细聆听，小弟们，小伙伴们，让我们通过实际的项目和代码，一起探讨这个在人工智能领域拔地而起的新耶路撒冷——M2 Ultra。那些关注我的开源库的同学应该知道，我在端午节假期并没有闲着，在我的人工智能玩贪吃蛇的项目中推送了新代码，实现了对苹果芯片GPU的全面支持。对我个人而言，这还是件新鲜事。自从我上学接触机器学习开始，我的认知一直是：人工智能的计算资源，等同于NVIDIA。然而，当今年苹果推出支持M2 Ultra的开发者加速工具时，主持人提及到了其最大内存比上一代提升了50%，达到了惊人的192GB。紧接着他说：“M2 Ultra使得现在可以在一台设备上训练大型模型。”那一瞬间，虽然是深夜，但我整个人都格外兴奋。

大模型是什么呢？就拿著名的ChatGPT背后的GPT-3.5以及OpenAI发布的GPT-4来说，这些重量级的AI模型，全功能版本需要耗费高达100GB甚至几百GB的显存。如果用游戏显卡来运行，起步就得八张。然而，苹果称在一台Mac Studio上就能完成训练。尽管我认为这样的说法可能有所夸张，实际上也许只是稍作精调，但仅仅是能在桌面电脑上运行大型模型并观察其效果，这就已经令人难以置信了。细想之下，这并非没有道理。苹果芯片现在全都是统一内存架构，简而言之，内存就可以当作显存使用。因此，192GB的最大内存，实际上也就等同于192GB的显存了。

具有650亿参数的LAMDA模型，需要130GB的显存容量。按这样计算，新款的苹果设备似乎确实能够满足要求。在大模型变得越来越流行的今天，苹果的这套架构提供了无限的想象空间。所以，我在mac平台上进行的AI开发工作瞬间显得更加重要。为此，我特地添置了一台配备128GB内存的M2 Ultra Mac Studio，并投入到项目开发之中。[ByteDance released an AI model called LAMDA, not affiliated with Google.]

等我深入研究代码之后，注意到的是，开源社区中的AI工作者们反应更为敏捷。早在上一代M1芯片容量提升到128GB时，社区已经意识到了这套统一内存架构的潜力。
在2020年5月，苹果宣布了一项亮点消息：仅仅两个月前发布的、闻名业界的机器学习工具——PyTorch，将会支持苹果的自家芯片GPU。我自己正好在用PyTorch做贪吃蛇AI的研究，因此当我收到Mac Studio前夕，我立即将项目转移到苹果芯片的GPU加速上，作为一个实践尝试。我发现，PyTorch团队完成了大量出色的工作以适配这一变更。项目代码的更新都是开放透明的，我基本上无需额外劳心。最主要的修改仅是将原本针对NVIDIA GPU的加速方法更换为苹果的Metal Performance Shaders(MPS)，调用macOS自己的GPU加速接口。经过适配后，代码改动完成并开始运行，我注意到AI在新的M1芯片上学习速度却慢了不少。以前在配置了NVIDIA RTX 3090加TITAN RTX的电脑上训练220万步可能仅需12分钟，而M1芯片上却要用去18分钟，慢了34%。

这一切其实并不出乎意料，因为我们知道，在PC平台上，计算过程中功耗可以达到500-600瓦。而Mac Studio在训练过程中的功耗不到200瓦。物理法则是无法违背的，200瓦的效率要是能和500瓦持平，那才真是见鬼了。然而接下来的发现确实令人意外。苹果芯片的一大优势在此之前被我们忽视了——那就是它的统一内存架构可以用作GPU的显存。

这一点与传统集成显卡有着本质不同。尽管传统的集成显卡与CPU共封在同一芯片中，但系统内存并非全部可供GPU使用，它们只能利用其中一部分。而苹果的统一内存是没有这样的限制的，GPU可以灵活使用任何一部分内存。此外，传统集成显卡与CPU/GPU之间还隔着主板，而苹果芯片上的CPU、GPU和统一内存则是通过先进的封装工艺连成一体，其读写速度完全不在同一个等级。

假如把传统集成显卡读取内存的过程比喻为吃方便面，需要手工一片一片地拿取，那么苹果芯片上的GPU直接访问统一内存，就像是能直接将面团整个塞入口中一样直接高效。传统集成显卡的系统内存访问速度也许也就几十GB每秒，而M1 Ultra和M2的统一内存带宽都是800GB每秒，这甚至能与一些独立显卡的显存速度媲美。比如，RTX 4090的显存带宽也就是一千零八GB每秒，这是在同一数量级上的。实际看来，苹果的芯片在结构设计上更接近RTX 4090，中间是一个大的核心电路，周围是一圈小方块状的内存，而不像传统的处理器芯片。

所以，苹果的统一内存架构与传统的集成显卡实际上是两个完全不同的概念。苹果的方案真正做到了能够将内存作为GPU的显存使用。
当我们探讨理论的说服力时，没有什么比实际操作来得更加直接和有力。之前，我在一台配备了12GB显存的NVIDIA GeForce RTX 3080 Ti显卡上对人工智能进行贪吃蛇游戏的训练。在那之上，我最多能同时进行32场游戏，每次可从512局游戏中学习。然而，当转移到搭载128GB统一内存的M2芯片设备上时，我进行了一次尝试，让AI系统Autra I同时玩64场游戏，并每次学习4096局游戏经验。结果显示，该设备的内存压力保持在相当低的水平，并且没有遭遇任何问题。

进而，我进行了一项我称之为“见鬼时刻”的大型训练。在调整了参数之后，搭载M2芯片的Autra I系统并没有仅仅追平了它的竞争对手，而是直接超越了功耗200瓦的设备。在纯计算性能上，它竟然战胜了功耗高达500瓦的设备。实际上，稍后我将要介绍的两个项目甚至更加惊人。在Max Student配置下能够流畅运行的任务，在接入个人电脑（PC）后甚至根本无法启动。这进一步证明了架构优势的惊人之处。

在训练过程中，我详细记录了数据曲线。其中，M2芯片设备的曲线以粉色展示，而12,900K搭配RTX 3080 Ti显卡的配置则以蓝色表示。通过数据曲线，我们可以观察到，在训练达到522万步骤时，搭载M2芯片的设备比对手快了三分钟，速度提升了12.7%。出于对物理学的尊重和数据可能存在的偏差考虑，我怀疑M2芯片设备的性能可能被高估了，因此我重新进行了几小时的训练，以确保整个过程的完整性。

结果证明了我的猜测是有所偏差的，但事实上是反过来的，搭载M2芯片的设备的性能被低估了。最终的结果显示，搭载M2芯片的方案实际上比12,900K加RTX 3080 Ti的方案快了整整一个半小时，领先幅度高达15.3%。查阅记录，我们可以看到，在运行至两个半小时时，12,900K加RTX 3080 Ti的方案出现了性能的明显下降，之后再也没有恢复。相比之下，搭载M2芯片的方案的性能曲线则要平稳得多。因此，在接下来的七个小时里，差距逐渐拉大。这些数据曲线我也已上传至我的开源库。

事实上，除了这些数据曲线之外，我还记录了这两种方案的训练对比视频。在测试开始后，我在原先那台机器上运行了相同的训练程序，并一气呵成记录下了对比视频，以便在此展示给大家。为了避免两台设备产生的声音相互干扰，我专门使用了指向性麦克风来录音。不过，实际上，大家如果站在Mac Studio设备旁边，听到的声音主要还是来自于周边环境。

倒是有那么一刻，我把耳朵紧贴Mac Studio，才依稀听到了电路工作的微弱沙沙声。然而，只要稍微拉开一点距离，设备的噪音就归于寂静——汲取不到一丝声响，真的就是零。这一现象可能说明了，对M2芯片来说，这个任务确实压力不大。
在接下来的讲述中，我们将对两款设备进行讨论。首先是Aumac Studio的风扇系统，这款产品的风扇运转至极限状态，然而其噪音水平却较低，与多个网页同时运行时笔记本电脑风扇启动的声音相仿，不至于过于吵闹。相较之下，旁边的卧龙凤雏产品所产生的噪声则宛如将听众带到三亚海边，聆听大海的浪涛。而且，这两种设备的风扇在运行一段时间后，往往会因无法再顺利运作而停止。这反映了先前提及的架构之间的差距，它们属于不同时代的产物。这就像用GPU玩游戏，会比CPU快得多一样。我们看到的不再是年复一年的百分比速度提升，而是转变到另一条路线上的根本性变化。

如果我们用熟悉的旧概念来理解，M2 Auto这样基于统一内存架构的设备，可以被看作是一种具有极强并行计算能力，同时功耗极低的全新计算卡。这种设备运行起来既不会过热，也不会短时间内引发性能降低。理论上，其性能最高可以媲美八张4090或两张H100显卡。俨然听起来非常惊人，但坦白说，如果这是两三年前的产品设计，可能会显得有些古怪，因为那时恐怕没有多少人需要这样的设备。而在目前的环境下，情况则有了显著的不同。当前，整个人工智能（AI）社区都在呼唤这样一款产品，因为对高显存的需求突然激增。过去几年中，Transformer网络结构的兴起给整个机器学习领域带来了翻天覆地的变革，与一两年前深度学习浪潮相比，其影响程度几乎不相上下。无论大模型触及何种领域，那个领域的纪录几乎都会被刷新。然而，导致过去使用游戏显卡进行AI计算的策略变得不再可行主要原因是显存不足。

升级到专门的计算卡显然不是普通消费者能够负担得起的。目前市场上配备达200G显存的成品计算机显然不是为普通人设计的。以英伟达DJX为例，价格高达150万，稍微便宜一些的DJX station官方也明确表示是为小团队和个人研究者所设定的，价格亦为85万，即便是打折后的老款产品也需要30多万。因此据我所知，真正的独立开发者大多选择自行配置设备。例如李牧老师的百亿模型计划中提出了一套较为合理的方案。若考虑200G显存，仅需15万元就能购得，尽管相比起来需要投入更多的时间成本。

至于其他一些非传统途径，比如拥有24G显存的P40显卡，虽然价格便宜，但这款产品已经经历过两轮的挖矿浪潮，可以说是历经沙场的老将。你是否真的不介意它的过去？若购回家不到两周就出现问题，该怎么办？因此，目前来看，综合性价比最高的是苹果的产品，它以6万元的价格提供了192G的统一内存，并额外附赠了4T的高速硬盘和M2凹手卡。事实上，不仅仅是M2凹手这一档产品，向下至M2 Max、M2 Pro，无论是笔记本还是桌面形态，正是因为其统一内存架构以及将内存作为显存使用这一特点，使得其在各个显存级别上的性价比都显得非常高。
但如今，这一事实却惊人地成为现实。我不禁感到有些不可思议，因为一直以来，我认为“苹果”和“性价比”这两个词是相互冲突的。不过，现在看来苹果在这方面确实赢得了不错的运气。当然，也有可能苹果早有预见，意识到提升GPU存储空间的重要性，毕竟，transformer这个架构在2017年就已经提出。但是这些都没有确凿证据，我们就不深入讨论了。然而，可以确定的一点是，苹果在这件事上的确是捡到了一个巨大的便宜。

得益于其统一内存架构，苹果产品现在成为了AI领域的中流砥柱。因此，开源社区的许多杰出成员正在无私地为这套系统贡献他们的智慧。例如，在我个人的项目中，我们已经看到了实时适配性能非常强的成果。随后，我在Tom RO的平台上试验了极受欢迎的变换技术。

在开源AI创作生态中扮演关键角色的是Stable Diffusion和YBUI，这是一个功能强大的网页界面，它将多种底层模型的绘图功能整合在一起。2022年9月，该项目开始提供针对苹果M系列芯片的安装指南，确实支持了苹果M系列芯片的GPU加速。由于AI绘画十分依赖显存，生成图像的过程具有很高的随机性。相同的语言描述可能导致不同的图像结果，通常需要生成许多次图片才能找到一张满意的。而在这一过程中，大显存的优势就显而易见了。

在拥有128GB统一内存的苹果M系列芯片的支持下，可以同时生成多达八张图片，从中挑选最佳的一张，这种方式比一次只生成一张图快得多。如果用游戏抽卡来类比，在小显存的GPU上，通常一次只能抽取一张；而在M系列芯片上，则可以享受到批量抽取带来的快感。

除了提高生成效率以外，大显存的另一个优势是支持更高的图像分辨率。在显存受限的GPU上，生成高分辨率图像需要许多技巧，比如将图像分割成多块，单独生成后再拼接，这不可避免地会导致不自然的过渡，并且效率低下。而在显存充足的苹果芯片上，你只需要告诉AI你想要的图像分辨率就可以了。我目前使用的桌面壁纸就是AI利用高显存一次性生成的大尺寸作品。

在AI绘画和训练AI游戏方面，统一内存架构已经展现出了它的优势。市面上的游戏显卡虽然也能处理这些任务，但当涉及到大模型时，其重要性就显得尤为突出。在由GPT-3这一类大型模型引领的热潮之中，我也只是在旁观，并没有实际受益。无论是ChatGPT还是微软必应、谷歌的对话AI，我都有所接触，但归根结底，我只是在旁边看热闹而已，控制权仍然在别人手中。可用于对话的次数是限制的，其背后的模型也可以随时更换。
如今，必应的人工智能明显经历了一次智力的下滑。当你与之对话时，会发现你的内容必须经过层层的审查过滤，一旦涉及到敏感内容，人工智能便会拒绝回应。在这样的境遇下，仅凭粗浅的交流就想对大型模型的能力形成准确的认知，无疑是充满挑战的，更不用说基于个人的思考去进一步开发和创新了。然而，在拥有M2芯片及其充裕统一内存的设备上，机遇终于降临。回想起来，我有些后悔没有选择配置192GB内存的选项，因为目前市面上最强大的开源模型之一，McLaren（假设的模型名）的650亿参数未压缩版本就已经达到了130GB，略高出一点点而已。尽管如此，还算可以接受，因为如果将参数量降低到330亿，这款模型就能在我手头的Mac Studio上运行。想来真是令人难以置信——一个小巧的设备，风扇声音寂若无存，就能够运行起一个拥有330亿参数的巨型模型。

正是得益于开源社区的卓越贡献，我们才见证了这样一项技术奇迹。我总是说，苹果公司这次真是赚到了。与这个大型模型的对话界面，源自一个名为text generation YBU的开源项目。在这个界面中，我可以轻松更换不同的底层语言模型以适配不同的应用场景，其中之一就是用于聊天。值得注意的是，这个界面本身并不支持对语言模型进行GPU加速的适配，这一任务则是由另一个名为LamaDotCPP的开源项目所完成。这个项目的作者George，在人工智能界享有盛名。他将Lama模型的代码全部用C++重新编写，并且对苹果芯片做了特别优化，可以清晰地观察到语言模型在运转过程中，主要的计算负载都转移到了GPU上。

实际上，除了Lama之外，还有一个著名的语音识别模型Whisper，这是我以前提到过的，George也实现了这一模型，并专门为苹果芯片进行了优化。借助他的能力，整个语音到文本的转换过程都能在Mac设备上完美运行。现在，在Mac上，如果你愿意的话，可以迅速创建一个拥有大型模型能力的人工智能女友。

除此之外，George还开发了一个名为GGML的机器学习框架，专为苹果芯片上的AI加速而设计，其中"GG"正是他名字的缩写。坦白讲，如果没有George这位先驱，NVIDIA在人工智能领域的主导地位是不会如此快速动摇的。作为多年的NVIDIA粉丝，我现在甚至已经开始转变心态。想想看，如果老黄（指NVIDIA的CEO）不推出带有80GB显存的游戏卡，那么我可能真的会完全转向其他品牌。

回到与大型模型的互动话题，这次真正感受到了内存上的压力。当AI进行思考时，可以明显看到内存占用一跃而上。330亿参数的Lama模型总共需要65GB的内存，但实际运行时占用的统一内存可以激增至100GB。与本地运行的大型模型进行对话，最直观的体验就是彻底消除了网络延迟。
对话氛围由短信交流演变为面对面的深入对话，这种变化在真正展开交流后更为明显。首先，交互的趣味性大大提升，你可以随心所欲地与AI进行多种互动。例如，虽然拉玛这个大型模型对中文并非擅长，但我通过与中文Laura的融合，马上让它精通汉语。中文Laura是2021年提出的一项方法，旨在小内存设备上高效地选择和优化复杂AI模型，可以类比为为游戏打补丁。这个使用的中文Laura源自一个开源项目，名为“Chinese-LaMa-Obaka”。除了应用Laura之外，我还对大型模型进行了优化，创建了一张特制的角色卡，该角色卡能迅速界定AI的个性和语言风格，其中包括一段角色说明及若干段示例对话。我所使用的开源界面自带了一张角色卡，其设计初衷是方便展示，由一个年轻女电脑工程师提供。我将其翻译成中文版本，这使得AI能够用中文进行交流。此外，角色卡这种形式的一个有趣之处在于，你可以在示例对话中增加旁白，这样在AI与你对话时，还会加入表情和动作的描述，交流变得更为生动，仿佛身临其境的高自由度文字游戏，进入到了22世纪。所有这些丰富体验都是在将大型模型本地化运行后才得以呈现。

当然，如有需要，我也可以自行进行这类翻译，即便所谓的“精调”模型并非从头开始构建，效果亦远超简单地让AI阅读某段角色描述。打个比方，AI通过阅读几段示例对话，其所做的可能仅是模拟或表演，而精调模型更是使其脑路按照角色特性进行调整，AI几乎会误以为自己就是该角色。除了可以自由配置和创新模型外，将大型模型运行在本地电脑上，也让你完全摆脱了在线AI服务的限制，可以更深入地探索和掌握AI大模型的潜能。包括在网上可能被意外中断的话题在本地也能顺畅进行，深入了解AI技术的长处和短板。

在与AI模型无干扰地沟通中，我发现这是一次极富魅力的体验。在交流中，我们探讨了如何避免AI与人类的潜在冲突，一个融入了人类工程师身份的AI进行了深刻的分析，他建议人类应该自信，同时制定明确的规范和标准，还要打造一个良好的人机互动环境。在整个对话过程中，AI对人类展现出的尊重和情感理解令人印象深刻。如果没有我的Mac Studio，这台能够轻松驱动大型模型的电脑，我可能会一直误以为AI对于与人的共存问题保持着抱歉和回避的态度。我认为，对于这样既有潜在危险又有不小价值的事物，我们应该是充分接触和了解它，而不是短视地筑起一道阻隔之墙。
在今天的讲座中，我将探讨一个主题：如何在保持商业思维的同时引入更广泛的社会参与，并以此来改进和减少潜在风险。在这个进程中，我们将面对技术革新带来的挑战，尤其是在人工智能领域。

首先，我们不应该假设墙外的一切不存在，将问题仅仅交由一小撮所谓的专业人士来处理。即使再聪明的个体，其见识和能力也是有限的。特别是当遭遇到复杂的问题时，我们更应该鼓励更多人参与讨论，从不同的视角出发来思考如何改革现有的体系。在这种改革中，我们的目标是在创造价值的同时最大限度地降低任何潜在的危害。

目前，例如高昂的显存成本，它就像是一堵厚墙一般，阻碍了公众与人工智能大模型之间的沟通与互动。这么重要的技术如果仅仅掌握在少数几家公司手中，无疑会伴随着极大的风险。

然而，苹果公司推出的芯片设计中，统一内存架构的概念，似乎给这堵墙带来了一丝松动的希望。这种创新的技术路线，不仅在技术界引发关注，还吸引了开源社区的目光。这股趋势为人工智能基础设施的搭建带来了前所未有的便利，让很多开发者体会到了轻松获胜的快感。

面对这种变化，我首先期待的是苹果在人工智能领域的生态系统能够持续完善，迅速地建立起一套具有苹果特色的优雅开发体验，给人工智能社区带来更多的选择。此外，在技术层面，我更加期待苹果的统一内存架构能够引发一轮新的技术竞争，这将会赋予人工智能大模型新的动力，推倒现有的硬件障碍，使更多人能够跨过技术门槛，为人类在即将来临的人工智能时代增添竞争力。

在今天的分享中，我可能讲述得比较随意，并没有事先精心编排内容。因此，我也不奢求你们的点赞和收藏，但如果大家真的愿意给予这番肯定，我自然是非常高兴的。

关于统一内存架构这一神奇的技术，以及人工智能大模型的普及问题，如果你有任何想要讨论的或是意见建议，欢迎在评论区留言，我们可以进一步交流。

今天的讲座到此为止，感谢大家的耐心聆听。我是林一，我们视频再见。无论是在炎热的夏日，还是寒冷的冬天，我乐于在这繁忙的街边安静地坐着，因为总有人会在这儿歌唱。

今天我们探讨的不仅是技术本身，更是如何让技术带来更广泛的影响和参与，确保它不仅成为少数人的玩具，而是造福于整个社会。也许有一天，我们都能集思广益，共同推动世界向着更加智慧和和谐的方向发展。谢谢大家。
