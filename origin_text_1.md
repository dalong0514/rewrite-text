模型的微调实战和经验分享首先我们做大模型的话大模型的三要素其实就是算法数据还有算力然后我们今天的分享也会以这三个为切入点然后来跟大家汇报一下在这三个方面我们都做了些什么事然后以及将来我们会做些什么事情然后这也是如果说各位有兴趣自己做个大模型可能也会涉及到这三个点就是不可避免的这三个点有所涉及首先我们讲一下我们的初音大模型的一个就是一个整体的架构图首先我们从左边这条路可以看到我们有训练数据有结构化的文档问答队然后也有非结构化的文档结构化的文档就是我们常见的一些QA然后非结构化就是大段大段的领域知识的描述就比如说一些规章制度一些通知等等然后我们会构建一个领域知识库然后通过Lanchan加Pomod的工程然后再把这个模型呢经过量化最后呢进行推上线然后上线的时候呢我们加了一套自研的内容审核的系统然后呢可以保证就是我们的出来的内容呢是合规合法的然后呢我们除了Lanchan这条路以外呢我们还会对模型呢进行一个训练我们首先呢会把训练我们首先呢会把训练数据呢进行拆解然后呢有非结化文档数据是用来去训练基础模型就类似于GLM模型然后呢结构化问答堆呢我们是用来训练XGLM就是让它具有对话能力的一种模型然后呢模型训练方案呢我们其实都试了很多种有Lara自适应器还有强化学习然后还部分层微调其实最后我们就发现Lara其实是最好用的因为它的学习成本最低然后最好上手然后模型选取其实现在有很多的模型然后我们最终是选择了XJ2模型然后这个模型其实它并不是参数量越大越好因为这个我们后面会跟大家汇报一下就是在参数量的上面其实有的时候参数量太大反而会让效果变差这个也是我们踩的一个坑然后我们最后会把模型整体的进行量化模型的量化主要的原因就是毕竟我们是服务于线上我们这样的话呢可以最大程度的利用机器资源然后呢来进行这样我们的这个运营整个的运营成本呢会变得很低这就是我们初营大模型的一个整个的一个系统的一个架构然后呢首先呢跟大家分享一下就是我们为什么这一波的XGBT那么红火关键其实还是用提示工程在支撑的其实像之前有BERT T5还有BAT之类的各种大模型其实都是算大模型的它们参数量也并不低但是往往都只在NLP这个圈子里头起到了轰动其实外界人可能对它也不是很了解主要是因为那些模型它的学习使用成本特别高首先你得会编程然后你得会NLP会记忆学习然后才能用然后提示工程它其实相当于说把这个成本直接迅速地降下来了然后它是通过写提示词的方法然后来让模型完成各种任务其实我相信大家已经都做过类似的事情了它的学习成本非常低这就好比以前的Fantune的方法呢是我开一辆手动挡的车我还得对汽车的内部构造啊什么的比较了解而Promoter呢其实就是我换了特斯拉我其实就不用管太多事情了我一脚油门下去车就开得非常非常爽对吧然后大模型的Promote呢它其实它的优点其实就是刚才也说了非常的简单易上手但是呢它其实也有很多缺点首先它的缺点呢就是一个是上限有限就是你用它呢它的效果如果说你不训练的话呢它上限是非常有限的达到一个更好的效果其实它就难度就比较高了并且呢它会有就是模型适配的一个问题模型适配什么意思呢就是说你换一套模型你换一个模型你可能你的提示词呢你又得重新的区配这样的话呢其实它的通用性的相对来说其实比较弱的然后呢另外两个呢其实是从更宏观角度来写想就是说如果说比如说像创业公司如果只靠蓬勃技术其实投资人会嫌这个技术比较薄就是没有深度然后包括基础人其实是觉得它比较浅的就是它没有什么技术壁垒只是比较好上手这里跟大家举了比如说三个例子比如说我们第一个例子呢就是说比如说我们可以完成一个句子然后他就会把这个天空式后面呢给补全然后呢我会给他加个提示词就是描述天气情况然后呢他就会进一步的去理解我的语义然后呢把天气情况呢给描述出来然后呢然后呢case 3呢就是我把上海的晴天然后再进一步的描述出来然后case三就是我把上海的晴天然后再进一步的描述然后这种任务在大多数大模型上头其实都是可以完成的其实我们通过这三个case其实能看到一个今年整个大模型最大的一个变化就是说我们先不讨论它输出的对与错它首先是窃题就是它的答案呢不会跑题这个是它今年大模型的一个重大的突破就在这样以前经常会出现这种答非所问的情况而这里呢它其实你可以明显感觉到它至少对我们提问的问题它是能够进行一个正确的理解的然后呢我们的提示工程呢如果说用它做NLP任务的话呢其实就是常见的七大NLP的任务就是文本摘要抽取 问答 分类对话代码 生成 推理等等我们都可以通过提示工程的方式让大模型呢来进行一个回答然后呢我们也可以就是有些就是高级的一些提示词的一些玩法就首先比如说我跟他说巴黎是法国的首都有一些什么著名的景点然后纽约有什么景点那我问他北京市然后呢他就把北京市中国的首都然后呢也有什么景点给出出来他学习到了上面的这种知识以及他们之间的一些关系然后呢包括我们也可以做一些逻辑的问题比如煮一个鸡蛋两分钟煮五个鸡蛋需要多久然后呢还有包括他有自洽性就是他不太容易出现一些前后矛盾的一些事情就比如我六岁了我妹妹是比我只有一半大然后呢推理一下我妹妹只有一半大然后呢推理一下我妹妹今年有多大了这个呢其实是一种非常高级的一种一种性能这个是传统L2P领域其实都是不太敢想象的一些一些功能点现在我们的大模型呢其实都是可以做的了包括我们现在上线的初心大模型呢也是能够完成类似任务的然后大家呢感兴趣呢也可以进行一个就是把玩吧我们的大模型的内核呢其实无论是什么样的大模型其实它的最终呢其实都是一个这个整个transformer的一个架构然后呢但是呢它为什么还会出那么多模型呢就是因为它主要在四个地方有点有所不同就比如说结构 位置编码 机位函数还有Lay Normal的一些方法就是说我们现在能接触到的一些主流的大模型它其实都是在这四个地方进行各种的排列组合并且这些技术其实都不是一个特别新的技术基本上都是2000年左右就出现了然后2020年左右都出现了然后只是进行各种缝合然后搞出了各种各样的一种模型但是这里跟大家分享一下就是说其实它这样的组合它其实并不是代表这种组合比如说XJM6B它是这样的一种组合它并不代表这种组合比如说XJM6B它是这样的一种组合它并不代表这种组合一定是最优的有可能你把这个基础函数你换成了一个别的它可能会更优但是现在大模型它有个很大的问题就是说它的训练成本太高它的训练成本非常之高所以说导致做一次实验的成本太高了所以说大家都没有去做这个事就是说只是说我这样组合了以后发现效果不错那么我就把它给发布了但是其实还有很多就是值得去优化的一个点只要提一下这个就是这个旋转字编码其实是一个中国人做的然后苏建林然后是90后其实比较年轻的一个学者所以说中国人在这上面其实也是有很高的一个参与度的然后这个就是我们大模型的一个不同的架构比如说第一种是Encode-Decode架构典型的就是BAT模型还有T5模型然后还有语言code Decoder架构典型的就是BAT模型还有T5模型然后还有语言模型的架构然后就是JBT还有LAM这种羊驼类的模型然后还有这种Prefix LRM模型这个是典型的就是我们初引用的GLM类的模型这三个模型其实现在基本上就是第一个Encode Decoder现在用的相对来说比较少了因为它的达到同等效果它的参数量要翻倍所以说第一种用的比较少了现在主要是第二种和第三种再进行一个竞争虽然现在GPT暂时领先但是最后鹿死水手其实也不好说我们如果在调大模型的时候呢其实我们很少像就是传统调模型的一样包括我们以前调BAT或者T5之类的模型其实都会直接去调翻吞它的参数但是呢我们对大模型呢其实很少去微调的我总结了一下呢主要其实有四点第一点呢大模型的参数非常调的我总结了一下主要其实有四点第一点大模型参数非常多几十亿都算小了然后其实你的内存其实很难放下的像我们A100的80G基本都有的都能撑满然后第二是参数多就是说它需要你参数多的话你就得需要更大的数据然后第三个就是不容易收敛你的训练时间非常的长你参数多的话呢你就得需要更大的数据然后呢第三个呢就是不容易收敛你的训练时间非常的长可能训练一次都两三个月起最终是什么效果还不知道整体呢就是冒的风险太高了第四个呢就是它里面的超参数特别多如何去调整这种超参之间的组合包括我的学习因子啊各种超参组合Batch Size啊等等其实都是非常非常耗时间的所以说呢大模型呢很少像之前一样微调其实主要的原因呢也是在也是在这四点所以说呢我们大模型呢它既然不能直接去像传统那种玩法去微调呢所以说呢就搞出了几种就是一种新的技术一个第一个是perfects tuning还有promote tuning它这个呢就类似于软的提示词的方法就是提示词呢它不再是一个个的具体的单词了而是对应的一个个向量然后我们直接去调这个向量然后呢还有一个方法第二个方法呢就叫做适配器就是适配器呢其实这个方法就是在Transformer这个结构里面呢我们加了一个外挂就是这个PK和PV然后我们加了一个外挂然后呢将来给它加了一层然后我们只学这两个小外挂就可以了然后整体的可学习参数呢就会变得很少然后第三个呢就是LawLaw呢其实也是加个外挂就可以了然后整体的可学习参数呢就会变得很少然后第三个呢就是LaraLara呢其实也是加个外挂只不过适配器呢我们可以列成它是以串联的形式加外挂然后呢Lara呢它其实是以并联的形式来加外挂然后我们在做出营的时候呢其实我们三种方法呢其实都做了不同程度的尝试然后最后发现呢其实Lara呢效果是最好的她比较容易上手然后呢就是修炼的呢相对来说是比较快的但是呢Laura呢她有个超参可能需要大家要注意就是那个R就是她那个Rank到底怎么设这里呢我有点经验就是说如果说你训练的领域越垂直就是比如说我们公司自己私有的一些数据,那么你这个rank呢,你就要设大一点。如果说你训练的数据呢,比如说是相对来说通用的,就比如我们做的运维大模型,它其实很多知识呢,是客观的知识,并不是咱们公司内部的知识。那么呢,我们这个rank呢,其实可以设的相对来说小一点。就是这是我们总结出来的一些小方法跟大家在这分享一下然后我们下面讲一下我们如果说训练大模型的话其实我们需要对应大数据我们把数据其实分成了两类第一类其实就是网页的数据就是各种爬虫爬下来的就是为什么OpenAI能做出来呢也在于微软做并搜索的时候给它提供了大量的网页数据包括国内的百度 360SOGO就是那个百川能做出来其实也来得益于就是他们原始就有很多大量的网页数据然后呢网页数据的特点就是量大但是呢质量呢参差不齐特别是中文语境的这个网页数据呢其实非常非常垃圾就是不太好用得进行大量的清洗然后呢我们还有那个就是专有数据专有数据呢就是质量是非常高的就比如咱们公司内部的各种公文然后呢各种技术文档其实都属于专有数据专有数据的好处呢就是它质量非常高但是呢它的存在的问题呢就是整理的成本比较高然后呢并且呢它的数量就是不会太多这个数量呢也会影响到大模型的一个最终效果这也是很多大家关心的一个问题到时候我后面会跟大家分享一下就是那个数据量和大模型之间的一个关系然后我们模型呢需要的数据呢其实分成两种一种呢我们可以用一种叫做非结构化的纯文本数据就是一段段的文字就可以了它用来训练的呢是类似于GLMGBT这种具备语言理解能力但是不具备对话能力的机座模型然后呢如果说我们需要训练就是对话模型的话呢我们就得用一些结构化的一些QA的数据来进行做不过我们现在通过就是我们团队呢现在通过Lanchain的技术呢把这个问题呢进行了一个部分的解决就是我们训练对话模型的时候呢也可以用一些就是非结构化的文本数据在做然后呢我们现在已经把那个技术技术这条路径呢给打通了然后呢现在正在进行一个就是项目的开发然后呢到时候工程化的开发然后最后呢做完了以后呢也会分享给给大家大家也可以进行一个试用好然后呢我们可以看一下就是我们训练Bloom模型的时候它的一个就是语言的一个覆盖我们可以看到呢它其实大量的还是就是欧洲的一些语言为主就是印欧语系为主比如英语法语西班牙语这样的大头然后中文呢其实相对来说呢其实是其实是比较少的然后呢各种反正各种小语种它其实都是有一些的然后呢它里面涉及到了一些编程语言的一些数据然后呢有Java PHP C++等等那这里其实大家可以发现它并没有我们最关心的SOCO对吧就这个地方其实大家可以想一想就是为什么为什么它这个没有覆盖到SOCO这种比较常见的语言然后PARM的大模型它的数据来源它这里它按类别刚才是按语言来分然后它这里是按类别来分就是有艺术娱乐的有游戏的有新闻的等等各种各样的领域的模型也覆盖的比较全其实呢结合这两张图其实我们来看呢其实我们就可以想跟大家分享一个什么理念呢就是说我们的数据要尽可能的全然后呢涉及到的语言呢可能会尽可能的多这样的话呢你训练这种通用型的模型你的效果才会特别好如果说咱们公司自己做垂直领域的话呢其实咱们倒不太涉及语言的问题因为都是都是中文我们可能呢就是需要对这个领域覆盖的比较多就是说如果说我们训练的数据没有覆盖到这个领域那么呢大模型的可能会在这个领域上的效果呢其实就是会比较差的就是说想要什么样的效果取决于他未给他什么样的一个数据这里是我跟大家总结了一下我们市面上如果说我们想自己玩一些大模型的话我们可以下载一些数据我在这里跟大家总结了一些常见的一些数据的一个下载地址大家感兴趣的话可以把它下载下来然后自己玩一玩这些都是可以公开下载的然后这也是一些中文的一些数据然后大家感兴趣的话也可以把它下载下来玩一玩这里面的数据质量呢还算是相对来说比较高我比较推荐这个第四个就是无道这一堆数据这堆数据呢还是相对来说比较好的这里呢跟大家分享一个就是我个人认为整个大模型里面最重要的一个定律叫做密率它的意思是什么就是随着模型的大小还有数据级的大小还有包括训练的强度的增加然后模型的性能会跟着一起增长但是它的增长并不是一个线性的增长而它是成了一个就是密率的关系就比如我们看这个数字它其实是一个密率的关系也就是说你开始的时候比如说我的数据量很大那么呢我开始呢我的测试级的损失函数呢就会下降得非常快也就是说测试级损失函数呢越小那么说明模型效果越好然后开始我的效果会提升的非常快但是呢随着我数量进一步的无限的增大然后慢慢的呢可能它的性价比呢就不会很高了就是成一个就是密率的关系这个呢其实这个我觉得与其说是大模型的一个特点不如说是一个社会自然规律因为我们知道就是所谓的二八法则对吧比如说我们大模型解决的问题可能有20%的问题是比较难的然后呢80%问题是比较简单的那么首先呢我大模型在训练的时候呢我先可以快速的把那80%比较简单的问题我可以搞定然后呢慢慢的那20%的问题呢可能我需要花费更大的代价才能去解决这个呢就呈现了出了一种密率的一种一种一种概念然后呢包括咱们的计算量计算强度还包括数据还有包括我的模型大小其实呢都是成一个一个密率的一个关系好这张图呢其实跟大家就是分享一下这张图我个人觉得是非常非常有意思的有意思的一个图呢其实跟大家分享一下这张图我个人觉得是非常有意思的一个图它的纵轴是一个就是测试级的损失函数就是说纵轴可以理解成越小模型越好然后横轴呢我们可以理解成我们的就是模型的一个参数量模型的一个就是参数量就是模型的一个参数量模型的一个参数量就是模型的大小我们可以发现就是说随着模型的增大参数的增大那么它的这个就是比如我们看这个子丝小线它的损失函数会越来越小对吧就是是符合我们认知的我们可能拿就是10B的模型对吧就是是符合我们认知的我们可能拿就是10币的10币的模型效果可能就会好于5币的模型就慢慢的就减少但是我们会发现如果说我的数据量很大的话我的数据量很大的话那么对于同一个大小从对于同一个参数大小的模型来说如果我的数据量如果我的数据量非常之大的话那么我的效果也会很好就比如说这根黄色的线我是拿了22B的数据来做然后紫色这个线呢我只拿了21兆的数据来做那么呢我会发现就是数据量越大那么呢我的这个模型的效果呢就会越好但是呢它的整体关系呢就是都是成一个密率的一个关系就是说有就是有业务部门有时候经常问我到底要多少数据其实我只能给出一种回答就是多多益善就是因为确实它是成这种这种关系直播到更后头呢它的性价比呢可能就不会很高了但是呢其实我们目前就是各个业务部门能提供的数据呢其实远远都没有到它的瓶颈所以说大家呢就放心的给我们数据就可以了好 然后这个图呢我们可以看一下就是要有意思的图我们可以先看一下就是左边这张图它的横坐标其实也是一个测试级的损失函数然后呢,重坐标是测试级的损失函数横坐标是需要处理的token数可以理解成我们的数据量然后我们会发现随着数据量的增大数据量增大 模型的损失函数会慢慢的下降然后到一定程度以后就慢慢的走平了但是我们也会发现 比如说这条紫色的线它的参数量相对来说比较少 那么它损失函数到一定程度以后它就不会再降了 也就是我们所说的不收不收链了 也就是说我们在一定程度上我们它就不会再降了也就是我们所说的不收不收链了也就是说我们在一定程度上我们需要选择一个就是相对来说比较大的模型但是呢代模型是否是否越大越好呢其实也不是这样然后这张图呢就是右边这张图其实就说明了为什么模型不能越大越好然后呢这里的横坐标呢是我们的计算强度也就是说我们GPU我们算了多久我们的GPU有多强悍然后呢我们可以发现对于这种紫色的这种小模型来说小模型来说它的下降速度其实是要比黄色这条线要快的就比如说当我的算力只有在这个地方的时候那我们就会发现我的小模型其实我们的效果比我们的大模型还要好因为虽然说小模型它的上限比较低但是呢它的下降速度其实是比较快的所以说呢当我们的算力不够的时候当我们算力不够的时候其实我们选择一个小模型它的损失函数反而会下降的比较快这个我们之前也踩过一个坑就是我们用130币的模型做出来的效果就是不如6币的其实对应张图其实可以理解就是我们的算力其实根本就达不到这种程度其实大部分人在做的时候你的算力都本就达不到这种程度其实大部分人在做的时候你的算力都很难达到这种评级所以说我们选模型的时候就是一定要量力而行就是我根据我们的经验来说一般选择10币6币到10币左右的模型基本上都足以满足我们正常的一个就是业务需求再大没有必要了第一个是浪费机器浪费钱浪费电然后下一个就是我们有个理论指导你的结果其实未必有小模型相对小的模型会好然后这里跟大家讲一下就是大家比较关心的一个就是tokentoken呢其实呢在大模型里头呢其实我们可以叫做子词就是我们在做就是分词对于文本做分词的时候呢其实常见的有两种方法一种是单词分词法就比如说我们的英文我们就通过空格自然分割然后呢中文呢我们可以通过结巴这种分词的方法然后我们还有单字然后英文可能就是一个字母然后中文就是一个字但是这两种方法其实在大模型里头都不太用大模型里头用的比较多的是子词分词法比如说BPEWordPiece还有Ngram大量是用这三种方法GPT就大量的是在用BPE大量是用这三种方法GPT呢就大量的是在用BPEBPE呢它其实这三种方法其实它的基本原理都是非常像的就是说经常出现在一起的三词或者字当成一个词就比如说葡萄中文里头比较典型的就是葡萄枇杷鸳鸯这种字就是只要一出现肯定就成都出现很少会单独出现一个字那么这种字我们就把它当成一个子词来处理然后这就是各个大模型它所用的一个方法然后这个Send&Piece是谷歌开源出来的一个就是一个分词器就是说如果说我们想自己实现子子分词的话呢我们其实就用这个工具其实就可以解决了其实大部分情况下其实Word Piece呢它其实理论上效果要比BPE好但是呢它的计算量太大了所以说大量的其实都是在用BPE就比如说我们的GP系列都是用BPE来做的做这个token的所以说它因为它是分成了子词所以说我们会发现一个比较有意思现象就是说比如我们用XGLM的时候我们就发现它这个子词有的时候是两个token对应于一个中文对应于一个词有时候可能是一个token对应于一个�文对应于一个词有时候可能是一个token对应于一个词就是因为他在切子词的时候呢切出了不同的情况所以他并不是说有严格的一个token对应一个词或者两个token对应一个词他并没有一种严格的对应关系关键是看他的子词切成了什么样然后呢这个是就是我们现在常见的一些我们常见的一些大模型的一个词汇表然后以及它的一些大小和性能然后我们可以看一下它的词表长度其实大概都是非常非常非常非常之大的然后还有包括我们它的中文平均长度其实都大概都是都是非常非常非常非常之大的然后呢还有包括我们它的中文平均长度我们可以就发现就GBT还有Lama在处理这个长文本的时候确实占据了一个就是占据了一个很大的一个优势然后呢还有包括我的他们的一些性能啊等等等等一些关系然后这个呢感兴趣的同学呢可以到时候可以自己看一下这个图然后呢我们在做在训练模型的时候呢其实呢我们会经常会用到多卡进行的训练为什么呢就是因为我们像我们咱们公司的A100其实它的显存只有80G我们装6B的模型呢其实是没有问题的但是你如果要装130B啊或者讯飞的那种就是也是百B级别的那种模型你单卡是无论如何你推理你都是装不下的所以说我们就会出现了各种各样的一种并行策略然后并行策略呢就是说让多卡能种各样的一种并行策略然后并行策略呢就是说让多卡能够实现一种就是分布式的一个效果首先呢比如我们有一个叫做数据并行这个数据并行呢主要是在训练的时候呢用的比较多就是呢我比如我有100条数据然后我四个机子每个机子呢我就只放25条数据然后呢训练完了以后呢把这25条数据然后呢训练完了以后呢把这25条每台机器的训练结果呢再综合merge一下然后呢再整体来调参这就是典型的数据并行然后呢还有一个就是做模型并行模型并行呢可能我们如果说玩大模型的话呢会用的比较多就是说我的模型非常大可能一张卡我都装不下那么多然后呢我们就会把这个模型呢多就是说我的模型非常大可能一张卡我都装不下那么多然后我们就会把这个模型我们会把这个模型比如我们拆成四部分然后拆成四部分然后每个部分其实都对应的都对应的一张卡然后第一部分就是前面的层算完了以后传到下一张卡然后下一张卡再算然后再传到第三张卡再传到第四张卡然后最后算完了以后传到下一张卡然后下一张卡再算然后再传到第三张卡再传到第四张卡然后最后算完了以后呢然后反向传播的时候呢每张卡来更新自己的参数这样的话呢这四张卡共同支撑了支撑了一个模型也就是说这里的模型变形的时候一个模型是分在四张卡里头的那数据变形的时候呢每张卡都有一个完整的模型数据是分在四张卡里头的这个就是数据�行的时候呢每张卡都有一个完整的模型数据是分在四张卡里头的这个就是数据并行和模型并行的一个一样的一个地方我们还有一个更加比较细的就是张量并行张量并行就是我们知道其实我们的深度学习神经网络它其实它的底层运算其实全都是一个矩阵运算其实全都是一个矩阵运算所以说我们把矩阵运算呢其实我们也可以通过并行的方式来进行算比如说我两个矩阵相乘然后呢我一张卡负责乘第一列和第二列然后呢下一张卡呢乘另外的东西然后呢这样的话呢同时并行相乘这样的话能够提高速度但是呢这个有时候这个张量并行呢它并不是说在卡间并行而是在那个GPU的SM里面并行这个呢我等会儿后面会跟大家讲GPU原理的时候呢会跟大家分享然后呢我们还有一个张量并行呢就是我们在做多抽头attention的时候每个抽头它其实是完全独立的所以说我们不同的抽头之间完全也可以用不同的运算单元来进行计算就是它并没有存在一个先后顺序的东西的时候我们其实都是可以用张量并行的所以说我们的数据并行模型并行张量并行呢其实可以同时进行的所以说就出现了这张图我们可以上面讲的三种并行呢可以同时进行这样的话呢会最大程度的来把这个整体的资源利用率给调度起来但是这个地方呢一定要就是注意一点就是说这种并行它对于通信的要求非常高就是说我们的尽可能是比如说NVLink然后包括机房等等都尽量的通信的带宽相对来说比较大这样的话才能起到一个比较好的效果不然数据传来传去的过程耗费的时间呢可能会太长最终的效果呢其实反而不理想好然后呢我们在做完训练完了模型之后呢然后呢我们这边呢做了一些就是模型的压缩和加速的一些工作然后呢深度学习的领域呢其实它的四件套就是加速四件套减脂 低脂 蒸馏 量化然后其实前三个在大模型里头其实都不太适用减脂就是把一些权重比较小的weight给干掉然后低脂由于利用数学的手段让矩阵变小然后蒸馏其实就是用小模型去学大模型前面三种呢其实在传统的一些NLP领域能用的比较多但是呢在就是大模型里头呢其实我们用的最多呢其实是一个就是模型量化的一个一个工作然后模型量化呢其实非常非常简单比如说我以前的树比如我以前的伏点树可能是它的范围是很大的然后它可能占用的显存占用的比特币呢就会就会就会比较高然后呢我就强行的让一个int8类型的一个数据呢来来表示它就是说相对我用低精度的数据那个数据来表示高精度的数据这样呢这种过程呢就要做量化就是我把一个R一个这样的辅点数变成了Q一个这种整数然后呢反量化呢就是反的求回去然后就是把整数又恢复成原来的辅点数然后呢大家可以发现其实在这里呢有一个取整的一个操作所以说这一步呢其实会产生所谓的叫做量化误差就是它这个量化呢就是说我可能给它的数是1.25然后呢可能经过量化再反量化以后呢它给我返回的数据呢是1.20就是我就我就少了一点我就少了一个那个就是少了一个精度这点精度呢其实对于训练来说对于预测推理来说对于推理来说其实并不是很重要但是对于训练来说呢它是有比较大的影响的但是我们针对这种就是这种技术呢种就是这种技术呢量化这种技术呢我们其实有三种量化的量化的方式就是第一种呢就是PDTQ就直接对模型呢进行一个量化就是我们直接对我们一个训练好的模型呢进行一个量化它这种量化的方式呢它的好处就是特别简单特别简单然后它的运算量也很小但是它的怪处就是什么它只能量化群众它不能量化激活函数激活出来的东西为什么呢因为我们并不知道输入是什么我们并不知道真实的输入是什么所以说我们就没有办法去给激活函数出来的数值进行有效的合理的量化我们只能量化模型的权重然后方式二呢它会跑一点测试数据就是这个input fp32稍微跑点测试数据然后它的好处呢就是什么呢它的好处就是可以直接把激活函数出来的结果也给量化掉了所以说方式二呢它的量化效果呢其实理论上会更好就是量化的东西会更多然后方式三呢就QAT这种方式呢就是在训练模型的时候呢就直接进行量化就把量化的过程呢直接嵌入了模型的学习当中这种方法呢其实是最好的然后但是呢它的成本呢也是最高的然后呢我们现在呢做的工作呢我们现在把这个我们现在出营我们是用了一个方式一的这个模型呢在进行一个量化方式二和方式三呢我们也正在就是进行一些进行探索吧就是现在呢也没有得到一个特别好的结果所以说呢我们就在这跟大家说一下就是说我们现在可能用的还是方式一的方法这样的话呢我们就可以对我们的GPU的显存呢利用率呢会得到一个比较好的提高就是无论是显存还是计算整数的速度要比计算伏点数要快很多各方面的推理速度也会得到一个比较好的提高就是无论是显存还是计算整数的速度要比计算伏点数要快很多各方面的推理速度也会得到一个提高然后下面跟大家分享一下就是英伟达的一个就是历代的一个产品产品的一个架构就是我们所说的GPU然后它这张图它其实不包括我们就是玩游戏的那些GPU啊但是呢我拿我自己买了一块A6000我拿A6000玩游戏非常爽秒杀4090那些显卡就是说就是说就是说他们这一套显卡呢玩游戏就是玩游戏简直是降维打击啊其实大家可以看到它的架构大概是这样的其实我们可以发现它的中文名字其实都是一个就是历代的一些科学家这个赫博是美国的一个女科学家是搞火箭和军事的然后呢这个赫博其实不是很出名可能在美国出名吧然后前面的其实都是我们一些要见的一些科学家但是其实可以看到它还是很有野心的为啥因为像牛顿啊爱因斯坦啊高斯啊这种名字呢其实还还是没有出现的就说明还是他的他对自己香港很有信心还有很大的程度的提升还是很像那个咱们那个比亚迪的那个车的什么秦探宋那种系列他就是拿比较著名的科学家呢来命名的然后这个地方呢有几个比较著名的科学家呢来命名的然后这个地方呢有几个比较跨时代的一个产品一个是帕斯卡架构帕斯卡架构呢他其实呢就出现了一个所谓的NV-Link这样的NV-Link呢就是让这个就是带宽呢一下子飙升上去了比以前的PCIe要强很多这样的话呢就能够形成了一个所谓的就是说多卡训练多卡训练然后其实2016年这个时间点也是大模型开始孵化的时间点就是深度学习已经开始越做越深了NVLink一直在走然后在那个就是福特架构就是我们用的咱们公司也有的V100上头呢它其实也有一个比较大的一个突破是什么呢就是它出现了一个是NVLink的2.0就是速度更快了然后另外一个TensorCore出现TensorCore呢叫做它的好处是什么呢它是专门用来专门用来计算矩阵乘法的所以说呢就是说它这个已经是完全是为深度学校而生了专门来计算矩阵乘法的所以说呢就是说他这个已经是完全是为深度学校而生了就专门来计算矩阵乘法的所以说他的矩阵乘法的速度呢是传统的那个就是前面的16倍就是他的算矩阵的速度会更快这就是TenCircle的一个诞生然后在就是A100A100呢有一个比较有一个比较大的进步就是说它优化了一个结构吸收性就是说它算吸收矩阵的时候它的效果会很好就是吸收矩阵可能在CV里头不是很典型在NRP里头它那个矩阵其实都是非常吸收的然后它对吸收矩阵做了一个非常高的优化然后包括我们现在最近的最近刚出的一个大傻甲Hover H100之类的产品这个卡应该是现在正常渠道渠道是买不到的然后它包括TensorCore也升级了然后Nalink也升级了然后呢它其实还有一个特点这张图上没有写就是它是专门针对Transformer这个架构做了一个很强的性能优化也就是说它算Transformer的时候呢会非常的快所以说可以认为它这种架构呢是完全为了大模型而生的比如说它这个Transformer架构你去玩游戏也没意义用不上所以说它这个卡完全是为了大模型而生这是目前最强悍的就是H100然后在A100里AA系列头比如有咱们公司常见的A100然后其实还有我们要轻量化的就是A10可以用来做推理还包括A6000就是把PC把NVLink阉割成了PCIE然后它其实是一些也相对来说比较低端的一些版本因为咱们公司呢大量的主流显卡呢其实还是就是A包括平能这边提供的主流显卡呢还是A100所以说我在这里跟大家分享一下就是A100的一个就是技术架构那个显卡一个架构这个大家仅做了解就可以了就是它的基础计算单元呢是SM然后呢一个A100呢它其实是有108个有108个SM然后呢每个SM中间呢它其实是有四个计算单元的这个呢比较大的就是TensorCore然后呢还有专门算Int的专门算FP的专门算32进度的专门算64的将来就来把不同的不同的伏点数用不同的讨算64的将来就来把不同的不同的伏点数用不同的计算单元来进行计算然后整个SM共享一块L1缓存然后一个小块里头共享一个L0的缓存也就是说我们在做清华的XRM2.0的时候它那个推理速度为什么那么快呢它就是巧妙地利用了A100的这种就是显存的这种结构让那个就是尽可能呢让这个运算呢不要跨SM进行尽可能在一个运算单元里头完成它其实巧妙地利用了这种硬件结构所以导致呢它的计算量变大了但是计算速度实际上变快了就是因为传输的时间然后就变少了然后包括我们的A系列它其实开始出现了可以支持BF16的数据结构了这个数据结构其实在V100里头还不是很支持然后所以说如果说现在有些大模型如果说用了BF16这种数据结构你还真只能在A系列来进行跑A10 A6000都可以跑但是在V里头可能就得把精度给换一下这是我们的A100的一个技术架构然后跟大家分享我们的就是A100的一个技术架构然后跟大家分享就是我们的就是TensorCoreTensorCore它其实是什么意思呢就是我们在做那个就是矩阵运算的我们在做矩阵运算的时候它其实所有的运算都可以分解成乘法和加法它会同时并行的就是这个矩阵运算我们正常来说是一行乘一列然后再加一个数对吧一行乘一列然后再加一个数它会把这些计算呢全部做成了一个就是一个并行的形式并行的形式然后呢一口气的呢然后进行运算然后它这个地方呢它其实分数框呢其实还有个很大的优点我们可以发现这两个相乘的矩阵呢是lp16就是16倍的负点数,然后但是相加呢,这是fd32,然后结果呢,也有出现了fd32就是说它这个叫什么呢,它这个叫做混合精度运算,就是说我们在训练模型的时候往往有的时候因为我们在,特别是我们在微调模型的时候,它的那个就是梯度的数值都非常小如果说你的精度很低的话那梯度其实基本上可以理解成都被四尺五入掉了也就是所谓的训练半天不收敛它其实是有梯度的但是呢就是因为数值精度的问题导致梯度被四尺五入掉了所以说呢它这里就专门搞出了一种混合精度运算的方式然后这样的话梯度就不会损失掉那可能有人会问那我为啥不直接用FP32呢那也不会啊为什么要混合在一起呢我直接用FP32不好吗这主要是因为如果说我用FP32的话我的显存的占用量会翻倍所以说它其实对显存的消耗负担太重所以说一般也不会这样用所以说就会采取了一种折中的方法然后这个是就是刚才跟大家提的一个就是西枢矩阵的一个A100里面所以西枢矩阵的一个加速方法就比如说我原来有个矩阵对吧我一个这个绿色这个矩阵然后乘一个红色这个矩阵但是呢我发现这个绿色矩阵里头就颜色越浅的它可能数值呢就越均越均于零好那我先把它就是零化一下对吧就是很小的数值就越均于0好那我先把它就是0画一下对吧就是很小的数值我都让它变成0然后这个矩阵呢其实理论来说呢大量就变得非常稀疏了然后非常稀疏了以后呢我只存储两个东西一个是一个是这个矩阵非0的数值然后呢以及非0的数值对应于原矩阵非0的数值然后呢以及非0的数值对应于原矩阵的一个坐标位置一个坐标位置然后那些0的东西呢我就不存了然后呢真正矩阵相乘的时候呢就拿这一块跟这一块相乘这样的话呢我可以把很多以前0乘以什么的这种运算呢我其实我都可以我都可以省略掉了就是因为零乘多少都是零嘛乘起来也没有意义就是平白平空的是浪费计算时间这样的话呢我就可以把这些运算呢给全部给省略掉了这样的话呢它的速度呢就会得到了一个进一步的加速这块其实啊在这块主要是针对L2P做的做的很强的优化因为CV里头不太存在这种情况CV的矩阵往往都是非常成密的L2P的特点就是非常稀疏其实还有一个场景就是推荐搜索的那种场景用大模型的话它们其实效果也提升的比较快因为它们那种矩�非常的也是非常的稀疏的好下面呢跟大家分享的就是NVLink和NVSwitch就是在我们就是在我们那个就是没有NVLink之前就是比如我两张GPU两张卡我要通信我得通过PCIE来走PCIE呢本身就慢然后它有的时候呢还得通过GPU去中转一下那么就会更慢然后这样的话呢就出现了就是NVLinkNVLink这个技术其实啊就是我做这个PPT的时候其实我还专门查了一下它的那个来源它其实并不是英伟达搞的它其实是IBM搞的就是说英伟达和IBM呢之前呢有一段很长的明月期它其实最早是IBM搞的就是说英伟达和IBM之前有一段很长的蜜月期他其实最早是IBM搞的但是现在好像对外提都说提是英伟达搞的这是一个小八卦然后NVLink其实我们在P系列就是帕斯卡架构的时候2016年的时候第一代的NVLink其实就已经出现了它可以进行一个四卡的四卡的就是四卡的连接对吧每张卡它都可以连接连接四个然后呢比如说八张卡然后它就做出了这种比较经典的一个架构然后呢后面呢然后呢就出现了所谓的NVSwitchNVSwitch呢就非常像我们的就是那个路由器就是说我一张卡NVSwitchNVSwitch呢就非常像我们的就是那个路由器就是说我一张卡NVSwitch能支持的路数更多这里线太密了我也数不太清楚感兴趣的同学可以找我要资料我这有它的具体有多少根线然后呢通过这种NVSwitch的方法呢就卡与卡之间呢它就不只是一根线了它可能有好多线一起在连这样的话呢它的那个就是它的速度呢一下子就会就会飙升上去然后呢包括A100它这个NVSwitch的这个能力呢就会就会更强了其实我们就会发现它这个2.4TB每秒的这个提升其实呢并没有发生变化它这个英伟达算这个带宽的时候它算双向的它其实单向只有1.2它是把两头都算上去了然后就是2.4但是它只是路它最后为什么一个是75GB一个是150GB呢主要因为它的路数变多了就让路数翻倍了但是呢单根线呢其实并没有并没有变化在H100的时候呢单根线的速度呢发展比较从2.4然后提升到了就是一个3.6所以说H100就是训练GBT这种超大模型它的优势会更大主要集中在什么呢第一个是它针对Transformer这种架构进行了大量的优化然后另外一个它的这种多机训练的并行能力通信能力会更加的强悍所以说就是H100的很强悍的一个地方可惜现在我们国内是买不到的好上面就是我今天的分享的内容