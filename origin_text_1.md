不开玩笑，一台苹果电脑能顶八张四零九零。今年m二OO的最大内存飙到了足足一百九十二g不知道大家有没有纳闷？但如果我补一句，在苹果芯片的统一内存架构下，一百九十二g内存就是一百九十二g显存呢？这是不是就意味着八张四零九零才能装得下的AI大模型在m二o洲这一张芯片上就能跑下来呢？这朵技术奇葩发芽是二零年苹果正式告别叉八六，而真正开花结果则是靠着开源社区法力无边的各路赛博菩萨大朋友们，请坐好看的小老弟小伙伴，请坐，我是林毅。今天呢咱们就用实实在在的项目和代码聊聊AI圈子拔地而起的这座新耶路撒冷m二ultra，关注我开源库的同学应该知道我在端午假期没闲着。我之前那个AI玩贪吃蛇的项目呢推送了新代码，全面支持了苹果芯片的GPU加速。这会儿对我来说呢还挺新鲜的。从上学接触机器学习开始，我的认知里AI计算就是和英伟达这仨字画等号的。直到今年的苹果开发者加速m二o洲登场的时候呢，主持人讲到它的最大内存提高了百分之五十，达到了夸张的一百九十二g。然后紧接着他下一句话就是这样，m二ultra就做到了在一台设备上训练大模型。当时我真是大半夜，整个人都精神了，甚至后面v三pro这个大新闻出来的时候，我脑子里头还是这件事儿，什么是大模型呢？比如大名鼎鼎的check GPT背后的GPT三点五GBD四，还有麦它放出来的拉玛这些AI巨物的满血版，一张嘴就能吞下去，一百g甚至几百g显存，用游戏显卡跑，那都得八张起步。然后苹果说呢在一台max studio上就能训练。首先我觉得训练这个说法肯定是夸张了，顶多是fine to be精调一下，但能在自己桌面上的电脑里把大模型运行起来看看效果，这我都已经觉得很离谱了。可细想想啊，这块好像也不无道理。苹果芯片现在都是统一内存架构，简单点说呢就是内存能当显存用。那最大一百九十二g的内存就可以看成是一百九十二g的显存。满血版拉玛模型有六百五十亿参数，显存需求呢是一百三十g那还真就能装得下。在大模型越来越流行的。今天呢这套架构就非常有想象空间了。所以我的mac平台AI开发的性质一下子就上来了，搞了一台一百二十八g大内存的m二ultra max studio而钻到项目里写起来。代码之后呢，我发现开源社区的AI工作者们反应更快。早在前代m一内存上到一百二十八autrg的时候呢，就看到了这套统一架构的签力。二零年月月e幺说发布两月月之后拍toch很有名的一套机器学习基术工具，就宣布要适配苹果芯片的GPU。我那个玩贪吃蛇的AI呢，就是跑在拍toch上的。所以节前收到max dio之后呢，我就拿这个项目练手给它适配配苹果芯的的PPU加速果。我发现拍toch团队这一一多活活的还还挺挺漂亮。这项目的的代码改改记录就是都能看得到，我基本上没干啥。核心修改呢就一个地方就是把加速方法从英伟达的裤裆改成苹果的MPS调用mac OS自己的GPU加速接口，原过代码改好。跑起来之后呢，我发现AI在m二上学习速度明显变慢了。同样是训练二百二十万步，原来autri九幺二九零零k加三零八零钛要花十二分钟，而m二auture呢用了十八分钟，慢了百分之三十四。原来这个完全是意料之中。原来那套PC平台跑计算的时候，功率能干到五六百瓦。而max studio行这个训训程程序功功耗还不到两百瓦。物理学咱们还是得尊重的，两百瓦，打五百瓦，要是旗鼓相当的就见了鬼了。可是呢接下来还真就见了鬼了。到这儿苹果芯片的一个重要优势，其实让咱们给完全忽略了。就是前面说它的统一内存能当显存，用这事儿。苹果这套玩法呢和传统的集成显卡是俩概念，虽然都是CPUGPU封装在一块，但传统的集成显卡并不是所有的系统内存都可以用，只能用其中一部分，但苹果的统一内存是没这个限制的。GPU想用哪块内存就用哪哪块？这是其一。其二呢传统方案里的系统内存和芯片上的CPUGPU还隔了一个主板。而在苹果芯片上CPUGPU和统一内存而接统过集g显层连在一块读写速度根本不在一个量级上。如果把传统集成显卡读取内存类比上吃，干脆面的时候，用手一块一块儿拿。那苹苹果芯片上GPU访问统一内存就是直接对嘴闷爽就完事了。传统集成显卡访问系统内存的速度呢也就几十g每秒。而在m一ultra和m二的统一内存带宽都是八百autrg每秒，而就已经能跟正儿八经的独立显卡的显存速度。白摆手腕的像四零九零的显存带宽，也不过就是一千零八g每秒，这都是在一个数量级上的。实际上开了盖之后，苹果的auoffer芯片看着呢都明显更像四零九零，都是中间一个大的核心电路，周围一圈小方块内存，反而不像传统的处理器芯片。所以苹果的统一内存架构和传统的集成显卡纯属两码事儿，它是真能把内存当显存用。当然比这些理论更有说服力的呢，还得是实际跑跑看之前在十二g显存的三零八零钛上训练AI玩贪吃蛇呢，我最多只能让他同时玩三十二盘游戏，每次学习五百一十二部游戏经验。而在一百二十八g统一内存的m二上的，我试了下让autra i同时玩六十四盘游戏，每次学习四千零九十六部游戏经验，结果呢没有任何问题，内存压力保持在了比较低的水平。而接下来就是我称之为见鬼时刻拉大训练。参数之后，m二autra并没有追平对手，它直接反超了一个二百瓦的设备，在纯计算上真就把一个五百瓦的设备干翻了。实际上一会儿要讲的两个项目呢更离谱，在max student上能跑的活，它接PC甚至连跑都跑不起来。后面再说啊，架构优势就是这么狂。这个训练过程呢，我记录了下来，粉色曲线是m二o蓝色的是幺二九零零trk加三零八零钛。从曲线上可以看到训练到第五百二十二万步的时候呢，m二o tro的小幅领了了三分钟，快了百分之十二点七。所以对物理学和尊重呢，我担心这数有偏差，把m二o创高估了。所以干脆呢又跑了几个小时，把整个训练跑完了。结果事实证明了，这里面果然有偏差，不过搞反了m二o，反而是被低估了。最后实际上trm二o tra比幺二九零零k加三零八零钛的方案快了，整整一个半小时，领先幅度达到了百分之十五点三。回头翻记录可以看到了幺二九零零k加三零八零钛，这都卧龙凤雏啊，跑到两个半小时的时候，脚软了，性能出现了一个明显的小跳水，然后呢再也没回去。而m二二tro的性能曲线呢就平稳的多。所以在后面的七个小时差距就被拉的越来越大。这些曲线呢我也都传到了我的开源库里。实际上除了这些曲线呢，还录了一段这两套方案的训练对比MRO说，跑起来之后呢，我在原来那台机器上也运行了一样的训练程序，然后一镜到底录了一段可以放在这儿呢，大家听个命。为了避免这俩设备的声音互相干扰呢，我专门用了指向型麦克风。但其实大家在mac studio这边听到的声音呢，主要也还是来自旁边。那哥俩。我当时呢是把耳朵贴到mac studio上，才听到了一些电路运行的沙沙声这么长，距离下，它声音就是零啊，真的是零，可能也是这个任务对m二说确实压力不大。后面要讲的两个项目里呢，aumac studio的风扇是真拉满了，但那个声音也不算大，跟笔记本上网页开多了之后，那个风扇刚转起来的声音差不多，远远不像旁边那个卧龙凤雏啊，直接给你送到三亚海面听大海的声音，而且这俩玩意儿风扇转到这个份儿啊，没多一会儿呢也还是说不行就不行了。反过来看呢，这就是前面说的架构差距，这已经两个时代的东西了，就像GPU打游戏，就是比CPU快一样。这已经不是一年又一年百分之多少的提速了，这是直接换到另一条路线上去了。如果往熟悉的旧概念上套的话呢，统一内存架构上的m二auto可以看成是一张并行计算能力爆炸，同时功耗又极低的全新形态计算卡，这玩意儿呢不炒不热，显存灌钩，最多能顶八张四零九零两张h一百。可虽然听着挺厉害，但说实话呢如果放在两三年前，这个设计其实挺怪的，应该也没什么人会需要这玩意儿。可今天的情况就很不一样了。眼下整个AI社区都在呼唤一个这样的产品。因为AI计算的显存需求呢忽然就上来了。这两年发生了一个事儿，就是transformer网络结构崛起之后呢，AI大模型对整个机器学习领域的血洗这个程度已经快赶上一二年，那波的深度学习浪潮了。大模型走到哪个领域，每个领域的记录就被刷新。但相应的呢过去用游戏显卡跑AI计算的思路就走不下去了，因为显存不够用了。你说换计算卡，那还真不是一般人能玩得起的。现在显存能上到二百g的成品计算机都摆明了，不是给普通人用的，比如英伟达DJX一百五十万，再便宜点的还有DJX station官网上明确说了是给小团队和个人研究者用的多少钱呢？八十五万老款打完折也得三十来万。所以据我了解，真正的独立开发者大部分都是自己装。比如李牧老师一百亿模型计划里就给了一套比较合理的方案。二百g显存的话，十五万就能拿下，相相的呢也要付出更多的时间成本。至于说呢还有一些野路子，比如二十四g显存的p四零便宜是真便宜，八块一万块钱不到二百g显存就搞定了。但这卡呢可是经历过两轮挖矿锻炼潮的老运动员了，你真确定你不介意它的过去嘛，买回来两周就出问题了，咋办呢？所以说目前综合性价比最高的呢，反而成了苹果六万块一百九十二g统一内存，还赠送四t的高速硬盘和一张m二凹手，实际上不光是m二凹手这一档，它往下m二max m二pro不管是笔记本还是桌面形态，就因为它统一内存架构，这个内存当显存用的特点。你一算下来，在各个显存级别上，它性价比都是高的。但眼下呢这确实来，我我觉得得有点可可思议，苹果性价比一直觉觉得这俩词是犯冲的。但眼下呢它确实就是客观事实，就这事儿上，我觉得苹果运气也是真挺好。当然呢，你要是说苹果早早预见到了扩大GPU存储空间的重要性也不是没有可能。毕竟呢transformer这个结构一七年就提出来了，但这些也没什么实质性的根据，咱们就不讨论论过过过，有点是可以肯定定苹果在这事儿上确实捡了个大便宜。因为统一内存架构，现在真就是AI圈子全村的希望。所以此时此刻，开源社区的各路神仙真是在无偿的给这套东西贡献智慧。前面。在我自己这个项目上呢，咱们已经看到拍时式的适配了，底打的很。很接着呢我就又在tom RO说上上了试一下非常流行的的II化换。目前开源AI做化生态的枢纽呢是stable diffusion YBUI，这是一个功能强大的网页界面，它把各种底层模型绘画功能都给融到了一块二二年九月，这个项目开始提供针对苹果芯片的安装指南，正是支持苹果m系芯片的GPU加速。因为呢AI作画也是一个非常吃显存的东西。AI生成图片这事儿呢有很强的随机性。同样的语言描述可能会有各种各样的画面出来，经常要生成很多张图才能得到一张满意的。而在这个过程里呢，大显存的优势就表现出来了。在一百二十八g统一内存的MRO手上呢，我可以让它同时生成八张图，然后从里面挑，这又比分八次生成八，这张图上快的多类比上游戏抽卡的话呢，在小显存的游戏显卡上都是单抽到了m二这儿呢就可以体验失联的快乐。除了树图效率的提升，大显存的另一个优势，就是可以支持更高的图像分辨率。在显存有限的游戏显卡上想生成高分辨率的图像，需要很多技巧。比如把图像分割成好几块，分别生成好之后呢再拼起来，这就难免会有一些不自然的过渡效率呢也比较低。但在显存自由的苹果芯片上，你就告诉aua i你要多大的分辨率就完事儿了。我现在用的壁纸呢就是AI在大尺寸上一波画出来。在实实面面的的屏相多多的。不过AI作画还有前面训练AII游戏戏都还不算是统一内内架构的的光光刻过这些活呢，市面上的游戏显卡也都能对付一下。但到了大模型，这就真是非他不可在在差GGBD带来的这轮大模型热潮里呢，我是真只坚守猪跑了一口猪肉都没吃着过chai GPT和必应谷歌的对话AI呢我都用过，但这些呢都只是看看猪跑，不是真正的吃猪肉控制权，都在别人手上。对话次数是有限制的，背后的模型呢，也是人家想换就换。比如必应的AI呢，就明显经历了一轮智力衰退。再然后呢，你的对话内容也是要被层层审查过滤的，稍微敏感点AI就拒绝回复了。就这样一个状态下呢，我想准确建立一个对大模型能力的认知都非常困难。更别说结合我自己的想法进行进一步的开发了。不过呢在m二offer充裕的统一内存上机会终于来了。我现在其实有点后悔，没搞一百九十二g的版本。因为目前最强的开源模型，麦克拉拉玛它的六百五十亿参数满满未压缩版就是一百三十g出头就多了那么一点点，不过也还好，参数量要降一档到三百三十亿。这个目前最强的开源语言模型就能在我这台max studoter上跑起来了，就这么个小盒子，也听不着很少的风扇声，居然就把一个三百三十亿参数的大模型给跑起来了。而这个技术奇迹呢，也同样离不开开源社区的贡献。所以我总说苹果这回真是占了老大便宜了。我和这个大模型对话的界面呢来自开源项目。text generation YBU在这个界面里呢呢，我可以换不不同的底层语言模型，也可以选择不同的使用场景。其中一个就是聊天。这个界面本身呢不提供对语言模型的GPU加速适配。这个活是另一个开源项目，拉玛点CPP搞定的。就个项目的作者george现在在AI圈子里很有名。他把拉玛的模型代码整个用c加加重写了一遍，而且是特别针对苹果芯片优化的，可以看到语言模型跑起来之后呢，主要负载都成功转移到了GPU上。实际上呢不只是拉玛另一个很有名的语音识别模型。这en AI的whisper，我之前介绍过的呢，也让他复现了一遍，也是专门针对苹果芯片优化的，就靠着他整个语音的文字的流程都在麦克上跑通了。就现在在麦克上，你要是想的话，很快就能写个大模型的AI女友出来。然后到这儿呢，还没有他他还搞了一个叫做GGML的机器学习框架给苹果芯片呢做AI加速GG呢，就是他名字的缩写就讲道理。要是没有这老哥呢，英伟达扩大在AI圈子的制配地位，不会动摇的这么快。就我这么多年的老恩粉，现在都已经一条腿迈出去了。你想想老黄，他要是再不出个八十g显存的游戏卡呀，我真就随时可能把剩下几条腿都卖出去了。然后咱们再说回和大模型的兑换，这次内存压力是真上来了。AI思考的时候，明显能看到内存占用标上去了。然后三十亿参数的拉玛模型总共是六十五g但实际运行的时候吃掉的统一内存呢能冲到一百个g和本地运行的大模型对话。最直观的感受呢就是没有网络延迟了。对话的感觉从发短信变成了当面聊天，然后真正聊起来之后呢，体会到的这个区别就更大了。首先是可玩性增加太多了，你可以自由自在的对AI做很多事情，比如拉玛这个大模型本身是不擅长中文的，但我可以把它和中文laura融合，让它瞬间精通。汉语laura呢是二一年提出来的一个在小显存设备上快速精挑复杂AI模型的方法可以理解成是给游戏打补丁。我用了这个中文laura呢，也是来自一个开源项目，叫做chinese拉玛奥巴卡。除了加loora，我还给大模型，为了一张角色卡，角色卡呢可以快速定义AI的性格和讲话。习惯里面包括一段角色描述和几段视例对话。我用的这个开源界面呢，自带一张角色卡，是因为年轻的电脑工程师小姐姐打了方便给大家展示，我把它翻译成了一张中文的角色卡，这样AI呢就会使用中文来和我交流。另外角色卡这个玩法呢还有一个有意思的点，就是你可以在视力对话里加一些旁白这样AI和你对话的时候呢，就也会增加一些神态啊，动作方面的描述，交流起来呢就会生动很多，就像是一个自由度拉爆的高game二十二世纪的文字游戏。而这些呢都是在本地把大模型跑起来之后才有的体验。当然如果我想的话，我还可以自己来翻译。q也就是精条模型虽然不是从头去，但精条的效果也是远超过给AI看一段角色描述的。打个比方看几段视例。对话之后呢，AI干的事是表演是模仿，但精条呢就相当于是让人的脑回路都朝着角色的方向发展。AI自己都会以为自己就是那个角色，除了可以自由的配置和开发模型以外呢，把大模型跑在本地电脑上，还可以让你彻底摆脱那些在线AI服务的条条框框，真正深入的探索和掌握AI大模型的能力现状。了解在强在哪哪儿有问题，在网上会被莫名其妙掐断的话题在本地上呢都可以正常进行。实际上我觉得和AI模型这种不受干扰的交流是一种非常有意思的体验。在和AI交流的时候呢，我们聊到了怎么避免AI和人类发生冲突的问题，这个带入了人类工程师身份的AI认认真真的分析了很多。他说，人类要有自信，也要制定好规范和准则，还要建立良好的人际交互环境。而在整个交流的过程里呢，他对人类表现出来的情感也都是尊敬与感知。我。如果我没有这台能把大模型抛起的mac studio，我还会一直以为AI对这个话题的态度就是抱歉，我意思和你聊，想让一个有潜在威胁的东西无害化，甚至发挥出价值。我觉得呢就应该让人们充分的接触和了解它，而不是立起来一道墙。假装墙外的东西不存在，让一小群所谓的专业人士去处理商人的思维。再聪明也肯定是有局限性的，越是复杂的东西，就越应该让更多的人参与进来。从各种不同的视角来思考如何去改造它，让他在创造价值的同时，把危害降到最低。现在高昂的显存成本就是横在公众和AI大模型之间的一道墙。但这么重大的一项技术就掌握在那么几家公司的手里，风险是很大的。苹果芯片的统一内存架构呢，让这道墙出现了松动的希感。而相应的苹果呢也因为这条大胆革新的技术路线接到了开源社区。这股东风在AI基础设施搭建这件事啊，体验了一把躺赢的快感。接下来呢我首先期待苹果这边的AI生态能越来越完善，快速建立起一套苹果味的优雅开发体验，让AI社区呢又能多一种新选择。然后除此之外，在技术层面，我其实更期待苹果的统一内存架构，能带动一轮新的技术竞争。把AI大模型的硬件障碍给彻底打下来，让更多的人跨过这道门槛，让人类在即将到来的AI时代多几分胜算。这次呢闲聊的比较多，没怎么精心的编排内容。所以呢我也不好意思要赞了，但你要真来个点赞，收藏呢，那我肯定也是非常高兴的。然后关于统一内存这个神奇架构，关于AI大模型的普及，有啥想聊的呢？咱们也可以在评论区交流交流行，今天的视频就到这了。我是林一，咱们还是下个视频见。没关系，我愿安安静静的坐在马路旁，夏天也好，冬天也好，总是会有人在歌唱。
